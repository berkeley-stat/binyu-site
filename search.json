[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Course Description: Theory and practice of statistical prediction. Contemporary methods as extensions of classical methods. Topics: optimal prediction rules, the curse of dimensionality, empirical risk, linear regression and classification, basis expansions, regularization, splines, the bootstrap, model selection, classification and regression trees, boosting, support vector machines. Computational efficiency versus predictive performance. Emphasis on experience with real data and assessing statistical assumptions. This course uses Python as its primary computing language; details are determined by the instructor.\nThis is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear regression, model assessment, model selection, regularization methods (PCR, PLSR, ridge and lasso); logistic regression and discriminant analysis; cross-validation and the bootstrap; tree-based methods, random forests and boosting; support-vector machines. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).\nIn this course, students explore the predictive modeling lifecycle, including question formulation, data preprocessing, exploratory data analysis and visualization, model building, model assessment/validation, model selection, and decision-making. The course focuses on quantitative critical thinking and key principles needed to carry out this cycle: 1) Foundational principles for building predictive models; 2) Intuitive explanations of many commonly used predictive modeling techniques for both classification and regression problems; 3) Principles and steps for validating a predictive model; and 4) write and use computer code to perform the necessary foundational work to build and validate predictive models.\nPrerequisites: Mathematics 53 or equivalent; Mathematics 54, Electrical Engineering 16A, Statistics 89A, Mathematics 110 or equivalent linear algebra; Statistics 135, the combination of Data/Stat C140 and Data/Stat/Compsci C100, or equivalent; experience with some programming language. Recommended prerequisite: Mathematics 55 or equivalent exposure to counting arguments.\nCourse Website: [STAT 154]\n\n\n\nCourse Description: Applied statistics and machine learning, focusing on answering scientific questions using data, the data science life cycle, critical thinking, reasoning, methodology, and trustworthy and reproducible computational practice. Hands-on experience in open-ended data labs, using programming languages such as R and Python. Emphasis on understanding and examining the assumptions behind standard statistical models and methods and the match between the assumptions and the scientific question. Exploratory data analysis. Model formulation, fitting, model testing and validation, interpretation, and communication of results. Methods include linear regression and generalizations, decision trees, random forests, simulation, and randomization methods.\nPrerequisites: Linear algebra, calculus, upper division probability and statistics, and familiarity with high-level programming languages. Statistics 133, 134, and 135 recommended.\nCourse Website: [STAT 215A]"
  },
  {
    "objectID": "teaching.html#stat-154---modern-statistical-prediction-and-machine-learning",
    "href": "teaching.html#stat-154---modern-statistical-prediction-and-machine-learning",
    "title": "Teaching",
    "section": "",
    "text": "Course Description: Theory and practice of statistical prediction. Contemporary methods as extensions of classical methods. Topics: optimal prediction rules, the curse of dimensionality, empirical risk, linear regression and classification, basis expansions, regularization, splines, the bootstrap, model selection, classification and regression trees, boosting, support vector machines. Computational efficiency versus predictive performance. Emphasis on experience with real data and assessing statistical assumptions. This course uses Python as its primary computing language; details are determined by the instructor.\nThis is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear regression, model assessment, model selection, regularization methods (PCR, PLSR, ridge and lasso); logistic regression and discriminant analysis; cross-validation and the bootstrap; tree-based methods, random forests and boosting; support-vector machines. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).\nIn this course, students explore the predictive modeling lifecycle, including question formulation, data preprocessing, exploratory data analysis and visualization, model building, model assessment/validation, model selection, and decision-making. The course focuses on quantitative critical thinking and key principles needed to carry out this cycle: 1) Foundational principles for building predictive models; 2) Intuitive explanations of many commonly used predictive modeling techniques for both classification and regression problems; 3) Principles and steps for validating a predictive model; and 4) write and use computer code to perform the necessary foundational work to build and validate predictive models.\nPrerequisites: Mathematics 53 or equivalent; Mathematics 54, Electrical Engineering 16A, Statistics 89A, Mathematics 110 or equivalent linear algebra; Statistics 135, the combination of Data/Stat C140 and Data/Stat/Compsci C100, or equivalent; experience with some programming language. Recommended prerequisite: Mathematics 55 or equivalent exposure to counting arguments.\nCourse Website: [STAT 154]"
  },
  {
    "objectID": "teaching.html#stat-215a---statistical-models-theory-and-application",
    "href": "teaching.html#stat-215a---statistical-models-theory-and-application",
    "title": "Teaching",
    "section": "",
    "text": "Course Description: Applied statistics and machine learning, focusing on answering scientific questions using data, the data science life cycle, critical thinking, reasoning, methodology, and trustworthy and reproducible computational practice. Hands-on experience in open-ended data labs, using programming languages such as R and Python. Emphasis on understanding and examining the assumptions behind standard statistical models and methods and the match between the assumptions and the scientific question. Exploratory data analysis. Model formulation, fitting, model testing and validation, interpretation, and communication of results. Methods include linear regression and generalizations, decision trees, random forests, simulation, and randomization methods.\nPrerequisites: Linear algebra, calculus, upper division probability and statistics, and familiarity with high-level programming languages. Statistics 133, 134, and 135 recommended.\nCourse Website: [STAT 215A]"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Published by MIT Press, 2024\nVeridical Data Science presents the PCS (Predictability, Computability, and Stability) framework that prioritizes reproducibility, interpretability, and reliability in data analysis. The book covers essential topics including statistical modeling, machine learning, causal inference, and ethical considerations in data science. Through real-world case studies and practical examples, readers learn how to apply the PCS principles to ensure their analyses are not only accurate but also robust and interpretable.\n\nWeb (free online access)\nPrinted"
  },
  {
    "objectID": "books.html#veridical-data-science",
    "href": "books.html#veridical-data-science",
    "title": "Books",
    "section": "",
    "text": "Published by MIT Press, 2024\nVeridical Data Science presents the PCS (Predictability, Computability, and Stability) framework that prioritizes reproducibility, interpretability, and reliability in data analysis. The book covers essential topics including statistical modeling, machine learning, causal inference, and ethical considerations in data science. Through real-world case studies and practical examples, readers learn how to apply the PCS principles to ensure their analyses are not only accurate but also robust and interpretable.\n\nWeb (free online access)\nPrinted"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Chancellor‚Äôs Distinguished Professor of Statistics and Electrical Engineering and Computer Sciences by courtesy\nStatistics Department Chair, 2009 - 2012\nmail: 367 Evans Hall #3860 ‚Ä¢ Berkeley, CA 94720 phone: 510-642-2781 ‚Ä¢ fax: 510-642-7892 ‚Ä¢ binyu@berkeley.edu"
  },
  {
    "objectID": "index.html#recent-publications-talks",
    "href": "index.html#recent-publications-talks",
    "title": "Home",
    "section": "Recent Publications & Talks",
    "text": "Recent Publications & Talks\n\nNew paper: PCS workflow for veridical data science in the age of AI (Z. Rewolinski and B. Yu, 2025)\nVeridical data science and medical foundation models on arXiv by Alaa and Yu.\nVDS book review by Yuval and Yoav Benjamini in Harvard Data Science Review (HDSR)‚Äì ‚Äúpedagogical excellence, diverse examples, and projects make Veridical Data Science a suitable textbook for students of all levels, in addition to being a valuable resource for data scientists in general‚Äù (review summary in HDSR Editor-in-Chief‚Äôs note), June, 2024.\n‚ÄúVeridical data science towards trustworthy AI‚Äù (Talk video of COPSS DAAL (formerly Fisher Award and Lecture), scroll down), Aug.¬†JSM in Toronto, 2023.\nVeridical data science (PCS framework: v-flow code and documentation template), PNAS, 2020 (QnAs with Bin Yu)\nMy co-author Rebecca Barter and I are thrilled to announce the online release of our MIT Press book ‚ÄúVeridical Data Science: The Practice of Responsible Data Analysis and Decision Making‚Äù, an essential source for producing trustworthy data-driven results (Feb.¬†28, 2024).\n\nView all papers ‚Üí"
  },
  {
    "objectID": "index.html#recent-news",
    "href": "index.html#recent-news",
    "title": "Home",
    "section": "Recent News",
    "text": "Recent News\n\nVeridical Data Science in Biology on July 11, 2025 at UC Berkeley (Submission deadline July 4)\nRome Workshop on Veridical Data Science, June 20, 2025\nIMS Wald Lecture I and Lecture II, and COPSS Distinguished Award and Lecture (DAAL) delivered at Joint Statistical Meeting (JSM) in Toronto, August 2023\nInaugural Berkeley-Stanford Workshop on Veridical Data Science at UC Berkeley (May 31, 2024) (talk videos available)\nUC Berkeley team part of the new NSF AI Center for Cybersecurity ACTION led by UCSB.\nCDSS news: Statistics-Computer Science team reflects on tackling covid outbreaks, May, 2022\nHonorary Doctorate, University of Lausanne (UNIL) (Faculty of Business and Economics), June 4, 2021 (Interview of Bin Yu by journalist Nathalie Randin, with an introduction by Dean Jean-Philippe Bonardi of UNIL in French (English translation))\nCDSS news on our PCS framework: ‚ÄúA better framework for more robust, trustworthy data science‚Äù, Oct.¬†2020\nUC Berkeley to lead $10M NSF/Simons Foundation program to investigate theoretical underpinnings of deep learning, Aug.¬†25, 2020"
  },
  {
    "objectID": "papers/tree-based-methods.html",
    "href": "papers/tree-based-methods.html",
    "title": "Tree-based Methods",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/tree-based-methods.html#selected-recent-papers-on-tree-based-methods",
    "href": "papers/tree-based-methods.html#selected-recent-papers-on-tree-based-methods",
    "title": "Tree-based Methods",
    "section": "Selected Recent Papers on Tree-based Methods",
    "text": "Selected Recent Papers on Tree-based Methods\n\nY. S. Tan, O. Ronen, T. Saarinen, B. Yu (2024). The Computational Curse of Big Data for Bayesian Additive Regression Trees: a Hitting Time Analysis. https://arxiv.org/pdf/2406.19958.\nY. S. Tan, C. Singh, K. Nasseri, A. Agarwal, J. Duncan, O. Ronen, M. Epland, A. Kornblith, B. Yu (2022). Fast interpretable greedy-tree sums (FIGS). https://arxiv.org/abs/2201.11931 (imodels üîé: a python package for fitting interpretable models contains code for FIGS).\nM. Behr, Y. Wang, X. Li, B. Yu (2022). Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests. PNAS, https://arxiv.org/abs/2102.11800 (theory for a tractable version of iRF, PCS-related)\nA. Agarwal, Y. S. Tan, O. Ronen, C. Singh, B. Yu (2022). Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods. Proc. ICML https://arxiv.org/abs/2202.00858 (imodels üîé: a python package for fitting interpretable models contains code for hierarchical shrinkage (HS))\nY. Tan, A. Agarwal, and B. Yu (2021). A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds. Proc. AISTATS, https://arxiv.org/abs/2110.09626\nM. Behr, K. Kumbier, A. Cordova-Palomera, M. Aguirre, E. Ashley, A. Butte, R. Arnaout, J. B. Brown, J. Preist, B. Yu (2020). Learning epistatic polygenic phenotypes with Boolean interactions https://www.biorxiv.org/content/10.1101/2020.11.24.396846v1 (code) (PCS inference case study)\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. https://arxiv.org/abs/1810.0728 (an enhanced version of iRF, PCS related)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (code) (PCS related)"
  },
  {
    "objectID": "papers/deep-learning-and-machine-learning.html",
    "href": "papers/deep-learning-and-machine-learning.html",
    "title": "Deep Learning and Machine Learning",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/deep-learning-and-machine-learning.html#selected-recent-papers-in-deep-learning-and-machine-learning",
    "href": "papers/deep-learning-and-machine-learning.html#selected-recent-papers-in-deep-learning-and-machine-learning",
    "title": "Deep Learning and Machine Learning",
    "section": "Selected Recent Papers in Deep Learning and Machine Learning",
    "text": "Selected Recent Papers in Deep Learning and Machine Learning\n\nA. R. Hsu, Y. Cherapanamjeri, A. Y. Odisho, P. R. Carroll, B. Yu (2024). Mechanistic Interpretation through Contexual Decomposition in Transformers. https://arxiv.org/pdf/2407.00886.\nO. Ronen, A. I. Humayun, R. Balestriero, R. Baraniuk, B. Yu (2024). ScaLES: Scalable Latent Exploration Score for Pre-Trained Generative Networks. https://arxiv.org/pdf/2406.09657\nS. Hayou, N. Ghosh, B. Yu (2024). The Impact of Initialization on LoRA Fineuning Dynamics. https://arxiv.org/pdf/2406.08447\nB. Yu (2024). After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI (discussion of Donoho‚Äôs paper ‚ÄúData Science at the Singularity‚Äù) Harvard Data Science Review (HDSR).\nN. R. Mallinar, A. Zane, S. Frei, B. Yu (2024). Minimum-Norm Interpolation Under Covariate Shift. Proc. ICML. https://arxiv.org/pdf/2404.00522\nL. Sun, A. Agarwal, A. Kornblith, B. Yu, C. Xiong (2024). ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance. Proc. ICML. https://arxiv.org/abs/2402.13448\nS. Hayou, N. Ghosh, B. Yu (2024). LoRA+: Efficient Low Rank Adaptation of Large Models. Proc. ICML. https://arxiv.org/abs/2402.12354\nN. Ghosh, S. Frei, W. Ha, B. Yu (2023). The effect of SGD batch size on autoencoder learning: sparsity, sharpness and feature learning. https://arxiv.org/abs/2308.03215\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. ICLR 2023. https://arxiv.org/abs/2305.17588\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu and J. Gao (2023). Explaining black box text modules in natural language with language models.\nN. Ghosh, S. Mei, and B. Yu (2022). The three stages of dynamics in high-dimensional kernel methods. Proc. ICLR, 2022. https://arxiv.org/abs/2111.07167\nW. Ha, C. Singh, F. Lanusse, S. Upadhyayula, and B. Yu (2021). Adaptive Wavelet Distillation from Neural Networks through Interpretation. Proc. NeurIPS 2021. (code)\nL. Reiger, J. W. Murdoch, S. Singh, B. Yu (2020). Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge. ICML Proceedings. (code)\nC. Singh, W. Ha, F. Lanusse, V. Boehm , J. Liu, B. Yu (2020). Transformation Importance with Applications to Cosmology ICLR Workshop paper. (code)\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Fast Mixing of Metropolized Hamiltonian Monte Carlo: Benefits of Multi-Step Gradients, JMLR, https://arxiv.org/abs/1905.12247\nR. Dwivedi, Y. Chen, M. J. Wainwright and B. Yu (2019) Log-concave Sampling: Metropolis Hastings Algorithms are Fast JMLR. http://jmlr.org/papers/v20/19-306.html\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2018) Fast MCMC Algorithms on Polytopes. JMLR. http://jmlr.org/papers/v19/18-158.html\nY. Chen, R. Abbasi-Asl, A. Bloniarz, M. Oliver, B. Willmore, J. Gallant, and B. Yu (2018) The DeepTune framework for modeling and characterizing neurons in visual cortex area V4 https://www.biorxiv.org/content/10.1101/465534v1\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. https://arxiv.org/abs/1810.0728 (an enhanced version of iRF, PCS related)\nJ. Murdoch, P. Liu, and B. Yu (2018) Beyond word importance: contextual decomposition to extract interactions from LSTMs. Proc. ICLR 2018. https://arxiv.org/abs/1705.07356 (code)\nS. Kunzel, J. Sekhon, P. Bickel, and B. Yu* (2019) Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning, PNAS. 116 (10) 4156-4165. https://arxiv.org/abs/1706.03461 (code)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (code) (PCS related)\nS. Balakrishnan, M. Wainwright, B. Yu (2017) Statistical Guarantees for the EM algorithm: from population to sample-based analysis. Annals of Statistics, 45(1), 77 - 120.\nS. Wu and B. Yu (2018). Local identifiability of l1-minimization dictionary learning: a sufficient and almost necessary condition. JMLR. 18, 1 - 56.\nK. Rohe, T. Qin and B. Yu* (2016). Co-clustering directed graphs to discover asymmetries and directional communities. Proc. National Academy of Sciences (PNAS), 113(45), 12679 - 12684.\nSiqi Wu, Antony Joseph, Ann S. Hammonds, Susan E. Celniker, Bin Yu, and Erwin Frise (2016). Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks (with support information). PNAS, pp.¬†4290 - 4295. (code) (PCS related)\nA. Bloniarz, H. Liu, C. Zhang, J. Sekhon, and B. Yu* (2015). Lasso adjustments of treatment effect estimates in randomized experiments. PNAS. 113, 7383 - 7390."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Bin Yu is the Chancellor‚Äôs Distinguished Professor in the UC Berkeley Departments of statistics and EECS. She was Chair of the Department of Statistics at UC Berkeley from 2009 to 2012. She has been elected to the National Academy of Sciences and Academy of Arts and Sciences. She was President of the Institute of Mathematical Statistics (IMS), Guggenheim Fellow, Tukey Memorial Lecturer of the Bernoulli Society, Rietz Lecturer of IMS, and a COPSS E. L. Scott Prize winner. She is a member of the UC Berkeley Center for Computational Biology and serves as a scientific advisor at the Simons Institute for the Theory of Computing. She is a Chan-Zuckerberg Biohub Investigator and Weill Neurohub Investigator.\nShe obtained her BS Degree in Mathematics from Peking University, and MS and PhD Degrees in Statistics from UC Berkeley. She was Assistant Professor at UW-Madison, Visiting Assistant Professor at Yale University, Member of Technical Staff at Lucent Bell-Labs, and Miller Research Professor at Berkeley. She was a Visiting Faculty at MIT, ETH, Poincare Institute, Peking University, INRIA-Paris, Fields Institute at University of Toronto, Newton Institute at Cambridge University, and the Flatiron Institute in NYC. Recently, she was a 50% consultant researcher in the deep learning group of MSR at Redmond (2022-2023).\nRecently, she delivered the Wald Memorial Lectures of IMS and COPSS Distinguished Achievement Award and Lecture (DAAL) (formerly Fisher Award and Lecture) at JSM in 2023. She served on the Inaugural Scientific Committee of the UK Turing Institute for Data Science and AI. She holds an Honorary Doctorate from the University of Lausanne in Switzerland. She is serving on the Editorial Board of Proceedings of National Academy of Sciences (PNAS).\nShe has published more than 170 papers in premier venues on statistical machine learning including deep learning and the Predictability-Computability-Stability (PCS) framework and documentation for veridical (truthful) data science and these papers not only investigate a wide range of research topics from practice to algorithms and to theory, but also seek deep insights. The breadth and depth of her research experience enabled unique and novel solutions to interdisciplinary data problems in audio and image compression, network tomography, remote sensing, climate science, neuroscience, genomics, and precision medicine.\nProfessor Bin Yu was formally trained as a statistician, but her research interests and achievements extend beyond the realm of statistics. Together with her group, Bin Yu has leveraged new computational developments to solve important scientific problems by combining novel and often interpretable statistical machine learning approaches with the domain expertise of her many collaborators in neuroscience, genomics and precision medicine. She also develops relevant theory to understand random forests and deep learning for insight into and guidance for practice. Her work has been recognized by many awards. In particular, she was inducted to the National Academy of Sciences in 2014 and to the American Academy of Arts and Sciences in 2013. She delivered the IMS Wald Lectures and COPSS Distinguished Achievement and Award Lecture (DAAL, formerly Fisher Award and Lecture) in 2023.\nCurrently, she is championing ‚Äúin-context‚Äù research with experts in the subject knowledge and leading research in interpretable machine learning (e.g.¬†tree-based methods and deep learning) and causal inference to design algorithms such as iterative random forests (iRF), Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition (CD-T) and adaptive wavelet distillation (AWD) for interpreting deep neural networks and X-learner for heterogeneous treatment effect estimation in causal inference. Recently, she and her collaborators developed PCS-guided low-signal signed iterative random forests (lo-siRF) that recommends genetic targets for high-yield follow-up experiments with a case study showing epistasis regulation controls cardiac hypertrophy through gene-silencing experiment (4 out of 5 sets of experiments found causal genes or gene-gene interactions). She is also working on relevant deep learning theory and practical algorithms with her team (e.g.¬†the development of LoRA+ algorithm for fine-tuning large language models).\nPreviously, she pioneered Vapnik-Chervonenkis (VC) type theory needed for asymptotic analysis of time series and spatio-temporal processes, and made fundamental contributions to information theory and statistics through work on minimum description length (MDL) and entropy estimation, and through theory on sparse modeling, boosting and spectral clustering, EM algorithm, and MCMC convergence analysis. With her students and collaborators, she developed a highly cited spatially adaptive wavelet image denoising method and a low-complexity low-delay perceptually lossless audio coder that was incorporated in Bose wireless speakers, and developed a fast and well-validated Arctic cloud detection algorithm using NASA‚Äôs MISR data. With the Jack Gallant Lab and her students, she developed predictive models of fMRI brain activity in vision neuroscience that made ‚Äúmind-reading‚Äù possible (or reconstruction of movies using only fMRI signals).\nMoreover, she served on editorial boards including Annals of Statistics, Journal of American Statistical Association, and Journal of Machine Learning Research. Her leadership roles included co-chairing the National Scientific Committee of the Statistical and Applied Mathematical Sciences Institute (SAMSI), and serving on the scientific advisory committee of SAMSI and IPAM, and on the board of trustees of ICERM and the Board of Governors of IEEE-IT Society. She recently served on the scientific advisory committee for the IAS Special Year on optimization, statistics and theoretical machine learning, the Scientific Advisory Boards of Canadian Statistical Sciences Institute (CANSSI). She is serving on the advisory board of the AI Policy Hub at UC Berkeley, the Scientific Advisory Committee of the Department of Quantitative and Computational Biology at USC, and on the External Advisory Committee, Learning the Earth with Artificial Intelligence and Physics (LEAP), an NSF Science and Technology Center (STC), at Columbia University."
  },
  {
    "objectID": "about.html#deep-learning-and-machine-learning",
    "href": "about.html#deep-learning-and-machine-learning",
    "title": "About",
    "section": "Deep Learning and Machine Learning",
    "text": "Deep Learning and Machine Learning\nOur work in deep learning spans from theoretical understanding to practical applications. Key contributions include mechanistic interpretation through contextual decomposition in transformers, efficient fine-tuning methods like LoRA+, and frameworks for understanding neural network dynamics. We‚Äôve developed methods like Adaptive Wavelet Distillation (AWD) for interpreting deep neural networks, and theoretical work on the three stages of dynamics in high-dimensional kernel methods. Our applications range from clinical decision-making in healthcare to cosmological analysis.\nSelected recent papers: - LoRA+: Efficient Low Rank Adaptation of Large Models (ICML 2024) - Mechanistic Interpretation through Contextual Decomposition in Transformers (2024) - Diagnosing transformers: illuminating feature space for clinical decision-making (ICLR 2024) - Adaptive Wavelet Distillation from Neural Networks through Interpretation (NeurIPS 2021)"
  },
  {
    "objectID": "about.html#veridical-data-science-pcs",
    "href": "about.html#veridical-data-science-pcs",
    "title": "About",
    "section": "Veridical Data Science (PCS)",
    "text": "Veridical Data Science (PCS)\nThe PCS (Predictability, Computability, Stability) framework represents our approach to responsible, reliable, and transparent data analysis. This framework unifies and expands on best practices in machine learning and statistics, ensuring that data science findings are both scientifically sound and practically actionable. Our work includes applications to genomics, clinical research, and epidemiology, always emphasizing the importance of stability and reproducibility in scientific discovery.\nSelected recent papers: - Veridical data science (PNAS 2020) - foundational framework paper - Epistasis regulates genetic control of cardiac hypertrophy (2023) - PCS application to genomics - After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI (Harvard Data Science Review 2024) - Stable discovery of interpretable subgroups via calibration in causal studies (staDISC)"
  },
  {
    "objectID": "about.html#interpretable-machine-learning",
    "href": "about.html#interpretable-machine-learning",
    "title": "About",
    "section": "Interpretable Machine Learning",
    "text": "Interpretable Machine Learning\nWe develop methods to make machine learning models interpretable and trustworthy, particularly for high-stakes applications in healthcare and science. Our work spans contextual decomposition for understanding neural networks, feature importance frameworks, and methods for aligning model explanations with prior knowledge. We emphasize the principle that interpretations should be both faithful to the model and useful for human decision-making.\nSelected recent papers: - Mechanistic Interpretation through Contextual Decomposition in Transformers (2024) - MDI+: a flexible random forest-based feature importance framework (2023) - Tell your model where to attend: post-hoc attention steering for LLMs (ICLR 2024) - Definitions, methods, and applications in interpretable machine learning (PNAS 2019)"
  },
  {
    "objectID": "about.html#tree-based-methods",
    "href": "about.html#tree-based-methods",
    "title": "About",
    "section": "Tree-based Methods",
    "text": "Tree-based Methods\nWe develop both theoretical understanding and practical improvements for tree-based methods, including random forests and gradient boosting. Our contributions include the iterative Random Forests (iRF) algorithm for discovering high-order interactions, hierarchical shrinkage methods for improving accuracy and interpretability, and fast interpretable greedy-tree sums (FIGS). We also provide theoretical analysis of when and why tree-based methods succeed or fail.\nSelected recent papers: - Fast interpretable greedy-tree sums (FIGS) (2022) - Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods (ICML 2022) - Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests (PNAS 2022) - iterative Random Forests to discover predictive and stable high-order interactions (PNAS 2018)\n\nResearch is supported in part by grants from NSF, NIH, the Weill Neurohub, and the Simons Foundation.\n\nYu Group at Berkeley for current group members and current projects\nPapers\nCode\nStatistical Machine Learning at Berkeley"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "For the most up-to-date and complete list, see Google Scholar or arXiv.\n\n\nA. R. Hsu, Y. Cherapanamjeri, A. Y. Odisho, P. R. Carroll, B. Yu (2024). Mechanistic Interpretation through Contextual Decomposition in Transformers.\nY. S. Tan, O. Ronen, T. Saarinen, B. Yu (2024). The Computational Curse of Big Data for Bayesian Additive Regression Trees: a Hitting Time Analysis.\nO. Ronen, A. I. Humayun, R. Balestriero, R. Baraniuk, B. Yu (2024). ScaLES: Scalable Latent Exploration Score for Pre-Trained Generative Networks\nS. Hayou, N. Ghosh, B. Yu (2024). The Impact of Initialization on LoRA Fineuning Dynamics.\nB. Yu (2024). After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI (Harvard Data Science Review)\nS. Hayou, N. Ghosh, B. Yu (2024). LoRA+: Efficient Low Rank Adaptation of Large Models (Proc. ICML)\nC. F. Elliott, J. Duncan, T. M. Tang, M. Behr, K. Kumbier, B. Yu (2024). Designing a data science simulation with MERITS: a primer.\nY. Chen, C. Singh, X. Liu, S. Zuo, B. Yu, H. He, J. Gao (2024). Towards consistent natural-language explanations via explanation-consistent finetuning.\nN. R. Mallinar, A. Zane, S. Frei, B. Yu (2024). Minimum-Norm Interpolation Under Covariate Shift. (Proc. ICML) arXiv\nL. Sun, A. Agarwal, A. Kornblith, B. Yu, C. Xiong (2024). ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance. (Proc. ICML) arXiv\n\n\n\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu, J. Gao (2023). Explaining black box text modules in natural language with language models.\nQ. Zhang, C. Singh, L. Liu, X. Liu, B. Yu, J. Gao, T. Zhao (2023). Tell your model where to attend: post-hoc attention steering for LLMs.\nQ. Wang, T. M. Tang, N. Youlton, C. S. Weldy, A. M. Kenney, O. Ronen, J. W. Hughes, E. T. Chin, S. C. Sutton, A. Agarwal, X. Li, M. Behr, K. Kumbier, C. S. Moravec, W. H. W. Tang, K. B. Margulies, T. P. Cappola, A. J. Buitte, R. Arnaout, J. B. Brown, J. R. Priest, V. N. Parikh, B. Yu, E. Ashley (2023). Epistasis regulates genetic control of cardiac hypertrophy. medRxiv (Code) (PCS documentation)\nR. Cahill, Y. Wang, R. P. Xian, A. J. Lee, H. Zeng, B. Yu, B. Tasic, R. Abbasi-Asl (2023). Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology. bioRxiv (Code)\nE. Irajizad, A. Kenney, T. Tang, J. Vykoukal, R. Wu, E. Murage, J. B. Dennison, M. Sans, J. P. Long, M. Loftus, J. A. Chabot, M. D. Kluger, F. Kastrinos, L. Brais, A. Babic, K. Jajoo, L. S. Lee, T. E. Clancy, K. Ng, A. Bullock, J. M. Genkinger, A. Maitra, K. A. Do, B. Yu, B. W. Wolpin, S. Hanash, J. F. Fahrmann. (2023). A blood-based metabolomic signature predictive of risk for pancreatic cancer. Cell Reports Medicine 4(9): 101194. doi: 10.1016/j.xcrm.2023.101194. (PCS related) (Editorial)\nR. Dwivedi, C. Singh, B. Yu, M. Wainwright (2023). Revisiting miniumu description length complexity in overparametrized models. JMLR, 24(268): 1-59.\nK. Wu, Y. Chen, W. Ha, B. Yu (2023). Prominent roles of conditionally invariant components in domain adaptation: theory and algorithms. JMLR (accepted). https://arxiv.org/abs/2309.10301\nN. Ghosh, S. Frei, W. Ha, B. Yu (2023). The effect of SGD batch size on autoencoder learning: sparsity, sharpness and feature learning. https://arxiv.org/abs/2308.03215\nR. Netzorg, J. Li, B. Yu (2024). Improving prototypical part networks with reward reweighting, reselection, and retraining. Proc. ICML. https://arxiv.org/abs/2307.03887\nA. Agarwal, A. M. Kenny, Y. S. Tan, T. M. Tang, B. Yu (2023). MDI+: a flexible random forest-based feature importance framework. https://arxiv.org/abs/2307.01932 (PCS related)\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. Proc. ICLR. https://arxiv.org/abs/2305.17588\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu and J. Gao (2023). Explaining black box text modules in natural language with language models. https://arxiv.org/abs/2305.09863\n\n\n\nD. Shen, P. Ding, J. Sekhon, B. Yu (2022). Same root different leaves: time series and cross-sectional methods in panel data. Econometrica (accepted). https://arxiv.org/abs/2207.14481\nB. Park, X. Wu, B. Yu, A. Zhou (2022). Offline evaluation in RL: soft stability weighting to combine fitted Q-learning and model-based methods. NeurIPS 3rd Offline Reinforcement Learning Workshop.\nY. S. Tan, C. Singh, K. Nasseri, A. Agarwal, J. Duncan, O. Ronen, M. Epland, A. Kornblith, B. Yu (2022). Fast interpretable greedy-tree sums (FIGS). (imodels üîé contains code for FIGS)\nA. Agarwal, Y. S. Tan, O. Ronen, C. Singh, B. Yu (2022). Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods. Proc. ICML (imodels üîé contains code for hierarchical shrinkage (HS)).\nB. Yu and C. Singh (2022). Seven principles for rapid-response data science: lessons learned from covid-19 forecasting. Statistical Science, 36(2):266-269.\nN. Ghosh, S. Mei, and B. Yu (2022). The three stages of dynamics in high-dimensional kernel methods. Proc. ICLR, 2022.\n\n\n\nY. Tan, A. Agarwal, and B. Yu (2021). A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds. Proc. AISTATS.\nN. Altieri, B. Park, J. DeNero, A. Odisho, B. Yu. (2021). Improving natural language information extraction from cancer pathology reports using transfer learning and zero-shot string similarity. JAMIA Open. 2021 Sept.¬†30 4(3).\nC. Singh, W. Ha and B. Yu (2021). Interpreting and Improving Deep-Learning Models with Reality Checks. To appear in ‚ÄúxxAI - Beyond Explainable AI‚Äù (eds.¬†Holzinger et al.).\nW. Ha, C. Singh, F. Lanusse, S. Upadhyayula, and B. Yu (2021). Adaptive Wavelet Distillation from Neural Networks through Interpretation. Proc. NeurIPS 2021. (code)\nM. Behr, Y. Wang, X. Li, B. Yu (2022). Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests. PNAS. (theory for iRF, PCS-related)\nN. Altieri, B. Park, M. Olson, J. DeNero, A. Odisho, B. Yu. (2021). Supervised line attention for tumor attribute classification from pathology reports: Higher performance with less data. Journal of Biomedical Informatics. 122 (2021) 103872. arXiv version\n\n\n\nM. Behr, K. Kumbier, A. Cordova-Palomera, M. Aguirre, E. Ashley, A. Butte, R. Arnaout, J. B. Brown, J. Preist, B. Yu (2020). Learning epistatic polygenic phenotypes with Boolean interactions (code) (PCS inference case study)\nB. Norgeot, G. Quer, B. K. Beaulieu-Jones, A. Torkamani, R. Dias, M. Gianfrancesco, R. Arnaout, I. S. Kohane, S. Saria, E. Topol, Z. Obermeyer, B. Yu & A. Butte (2020). Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist, Nature Medicine, 26, 1320‚Äì1324.\nB. Yu (2020). Stability expanded, in reality. Harvard Data Science Review (PCS related)\nB. Yu and R. Barter (2020). Data science process: one culture. JASA. (PCS related)\nR. Dwivedi, Y. Tan, B. Park, M. Wei, K. Horgan, D. Madigan, B. Yu (2020). Stable discovery of interpretable subgroups via calibration in causal studies (staDISC). International Statistical Review (code) (PCS case study for causal inference)\nX. Li, T. M. Tang, X. Wang, J. A. Kocher, B. Yu (2020). A stability-driven protocol for drug response interpretable prediction (staDRIP). NeurISP workshop on ML4H (Machine learning for Health) Extended Abstract.\nA. Y. Odisho, B. Park, N. Altieri, J. DeNero, M. R Cooperberg, P. R .Carroll, B. Yu (2020). Natural language processing systems for pathology parsing in limited data environments with uncertainty estimation. JAMIA Open.\nL. Reiger, J. W. Murdoch, S. Singh, B. Yu (2020). Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge. ICML Proceedings. (code)\nC. Singh, W. Ha, F. Lanusse, V. Boehm, J. Liu, B. Yu (2020). Transformation Importance with Applications to Cosmology ICLR Workshop paper. (code)\nN. Altieri, R. Barter, J. Duncan, R. Dwivedi, K. Kumbier, X. Li, R. Netzorg, B. Park, C. Singh, Y. Tan, T.Tang, Y. Wang, C. Zhang, B. Yu. (2020) Curating a COVID-19 data repository and forecasting county-level death counts in the United States. Harvard Data Science Review (code) 7-day prediction results Short talk video\nB. Yu and K. Kumbier (2020) Veridical data science (PCS framework), PNAS. 117 (8), 3920-3929. QnAs with Bin Yu.\nR. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan and B. Yu (2020) Sharp Analysis of Expectation-Maximization for Weakly Identifiable Mixture Models AISTATS.\nR. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan and B. Yu (2020) Singularity, Misspecification and the Convergence Rate of EM Annals of Statistics.\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Fast Mixing of Metropolized Hamiltonian Monte Carlo: Benefits of Multi-Step Gradients, JMLR, arXiv\n\n\n\nR. Dwivedi, Y. Chen, M. J. Wainwright and B. Yu (2019) Log-concave Sampling: Metropolis Hastings Algorithms are Fast JMLR.\nD. Rothenh√§usler and B. Yu (2019). Incremental causal effects.\n\n\n\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2018) Fast MCMC Algorithms on Polytopes. JMLR . http://jmlr.org/papers/v19/18-158.html\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Vaidya Walk: A Sampling Algorithm Based on Volumetric-Logarithmic Barrier Allerton Conference 2017 https://ieeexplore.ieee.org/abstract/document/8262876/\nW. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu* (2019) Definitions, methods, and applications in interpretable machine learning. PNAS, 116 (44) 22071-22080.\nW. J. Murdoch, C. Sign, and B. Yu (2019). Hierarchical interpretations for neural network predictions. ICLR. (code)\nY. Wang, S. Wu and B. Yu (2020) Unique Sharp Local Minimum in l1-minimization Complete Dictionary Learning. JMLR. 21(63), pp.¬†1-52. Also at https://arxiv.org/abs/1902.08380\nY. Chen, R. Abbasi-Asl, A. Bloniarz, M. Oliver, B. Willmore, J. Gallant, and B. Yu (2018) The DeepTune framework for modeling and characterizing neurons in visual cortex area V4 https://www.biorxiv.org/content/10.1101/465534v1\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. https://arxiv.org/abs/1810.0728 (an enhanced version of iRF, PCS related)\nY. Chen C. Jin, and B. Yu (2018) Stability and Convergence Trade-off of Iterative Optimization Algorithms. https://arxiv.org/abs/1804.01619\nJ. Murdoch, P. Liu, and B. Yu (2018) Beyond word importance: contextual decomposition to extract interactions from LSTMs. Proc. ICLR 2018. https://arxiv.org/abs/1705.07356 (code)\nR. Diwivedi, Y. Chen, M. J. Wainwright, and B. Yu (2018) Log-concave sampling: Metropolis-Hastings algorithms are fast! https://arxiv.org/abs/1801.02309.\n\n\n\nY. Chen, R. Dwivedi, M. J. Wainwright, and B. Yu (2017) Fast MCMC sampling algorithms on polytopes https://arxiv.org/abs/1710.08165.\nB. Yu and K. Kumbier (2018) Artificial Intelligence and Statistics Frontiers of Information Technology and Electronic Engineering. 19(1), 6-9.\nR. Abbasi-Asl and B. Yu (2017) Structural Compression of Convolutional Neural Networks Based on Greedy Filter Pruning https://arxiv.org/abs/1705.07356\nR. Abbasi-Asl and B. Yu (2017) Interpreting Convolutional Neural Networks Through Compression. NIPS 2017. Symposium on Interpretable Machine Learning. (also https://arxiv.org/abs/1711.02329)\nS. Kunzel, J. Sekhon, P. Bickel, and B. Yu* (2019) Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning, PNAS. 116 (10) 4156-4165. https://arxiv.org/abs/1706.03461. (code)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (code) (PCS related)\nS. Balakrishnan, M. Wainwright, B. Yu (2017) Statistical Guarantees for the EM algorithm: from population to sample-based analysis. Annals of Statistics, 45(1), 77 - 120.\nR. Barter and B. Yu (2017) Superheat: An R package for creating beautiful and extendable heatmaps for visualizing complex data JCGS (revised). https://github.com/rlbarter/superheat\nH. Liu and B. Yu (2017) Comments on: High-dimensional simultaneous inference with the bootstrap by Dezeure et al Test. 26: 740-750.\nC. Carson et al (2016). UC Berkeley Data Science Planning Initiative Faculty Advisory Board (FAB) Report. FAB Report Executive Summary\nS. Wu and B. Yu (2018). Local identifiability of l1-minimization dictionary learning: a sufficient and almost necessary condition. JMLR. 18, 1 - 56.\n\n\n\nK. Rohe, T. Qin and B. Yu* (2016). Co-clustering directed graphs to discover asymmetries and directional communities. Proc. National Academy of Sciences (PNAS), 113(45), 12679 - 12684.\nR. E. Kass, B. S. Caffo, M. Davidian, X. Meng, B. Yu, Nancy Reid* (2016). Ten simple rules for effective statistical practice. PLoS Comput. Biol., 12(6): e1004961. doi:10.1371/journal.pcbi.1004961\nSiqi Wu, Antony Joseph, Ann S. Hammonds, Susan E. Celniker, Bin Yu, and Erwin Frise (2016). Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks (with support information). PNAS, pp.¬†4290 - 4295. (code) (PCS related)\nA. Bloniarz, C. Wu, B. Yu, A. Talwalkar (2016). Supervised neighborhoods for distributed nonparametric regression. Proc. of AISTATS, Barcelona, Spain.\n\n\n\nB. Yu (2015). Data wisdom for data science. Operational Database Management Systems (ODBMS.ORG).\nA. Bloniarz, H. Liu, C. Zhang, J. Sekhon, and B. Yu* (2015). Lasso adjustments of treatment effect estimates in randomized experiments. PNAS. 113, 7383 - 7390.\nP. Ma, M. W. Mahoney and B. Yu (2015). A Statistical Perspective on Algorithmic Leveraging. Journal of Machine Learning Research, 16, (2015), 861-911.\nT. Moon, Y. Wang, Y. Liu, and B. Yu (2015). Evaluation of a MISR-based high-resolution aerosol retrieval method using AERONET DRAGON campaign data. IEEE Transactions on Geoscience and Remote Sensing, 53, 4328-4339.\n\n\n\nB. Yu (2014). Let us own data science. video IMS Bulletin Institute of Mathematical Statistics (IMS) Presidental Address, ASC-IMS Joint Conference, Sydney, July, 2014.\nG. Schiebinger, M. J. Wainwright and B. Yu (2014). The geometry of kernelized spectral clustering. Annals of Statistics, 43, 819-846.\nL. Miratrix, J. Jia, B. Yu, B. Gawalt, L. El Ghaoui, L. Barnesmoore, S. Clavier (2014). Concise comparative summaries (CCS) of large text corpora with a human experiment. Ann. Applied Statist., 8, 499-529.\nY. Benjamini and B. Yu (2014). The shuttle estimator for explainable variance in fMRI experiments. Annals of Applied Statistics, 7, 2007-2033.\nD. Bean, P. Bickel, N. El Karoui and B. Yu (2014). Optimal M-estimation in high-dimensional regression. Proceedings of National Academy of Sciences, 110, 1456314568.\nN. El Karoui, D. Bean, P. Bickel, C. Lim, and B. Yu (2014). On robust regression with high-dimensional predictors. Proceedings of National Academy of Sciences, 110, 1455714562.\nP. Ma, M. W. Mahoney, B. Yu (2014). A Statistical Perspective on Algorithmic Leveraging. Proc. of International Conference on Machine Learning (ICML) (This conference paper contains some of preliminary results of the journal submission Ma et al.¬†(2015))\nA. Bloniarz, A. Talwalkar, J. Terhorst, M. Jordan, D. Patterson, B. Yu and Y. Song (2014). Changepoint Analysis for Efficient Variant Calling. Proc. of RECOMB 2014 (to appear).\nTao Shi (2013), A conversation with Professor Bin Yu International Chinese Statistical Association (ICSA) Bulletin, Vol 25, Issue 2, pp 85-98. (Selected Parts in Statblogs)\nA. Joseph and B. Yu (2016). The impact of regularization on spectral clustering. Annals of Statistics. 4, 1765 - 1791.\nC. Lim and B. Yu (2016). Estimation Stability with Cross Validation (ESCV) Journal of Computational and Graphical Statistics. 25, 464 - 492. (First paper towards PCS)\n\n\n\nA. S. Hammonds, C. A. Bristow, W. W. Fisher, R. Weiszmann, S. Wu, V. Hartenstein, M. Kellis, B. Yu, E. Frise, and S. E. Celniker (2013). Spatial expression of transcription factors in Drosophila embryonic organ development. Genome Biology, 14(12), R140.\nH. Liu and B. Yu (2013). Asymptotic properties of Lasso+mLS and Lasso+Ridge in sparse high-dimensional linear regression. Electron. J. Statist., 7, 312-3169.\nJ. Mairal and B. Yu (2013). Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows. Journal of Machine Learning Research, 14, 2449-2485.\nY. Wang, X. Jiang, B. Yu, M. Jiang (2013). A Hierarchical Bayesian Approach for Aerosol Retrieval Using MISR Data. J. American Statistical Association, 108, 483-493.\nY. He, J. Jia and B. Yu (2013). Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs. Annals of Statistics, 41(4), 1742-1779.\nB. Yu (2013). Stability. Bernoulli, 19 (4), 1484-1500. (Invited paper for the Special Issue commemorating the 300th anniversary of the publication of Jakob Bernoullis Ars Conjectandi in 1712) (Begining of PCS)\nJ. Mairal and B. Yu (2013). Discussion on Grouping Strategies and Thresholding for High Dimensional Linear Models Journal of Statistical Planning and Inference, 143, 1451-1453.\nC. Uhler, G. Raskutti, and P. Buhlmann and B. Yu (2013). Geometry of faithfulness assumption in causal inference. Annals of Statistics, 41, 436-463.\nL. Miratrix, J. Sehkon, and B. Yu (2013). Adjusting Treatment Effect Estimates by Post-Stratification in Randomized Experiments. Journal of Royal Statistical Society, Series B, 75 (part 2), 369-396.\nJ. Jia, K. Rohe and B. Yu (2013) The Lasso under Poisson-like Heteroscadecity. Statistica Sinica, 23, 99-118.\nS. Negahban, P. Ravikumar, M. Wainwrigt, and B. Yu (2012) A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. Statistical Science, 27, 538-557.\nG. Raskutti, M. Wainwrigt, and B. Yu (2012) Minimax-optimal rates for sparse additive models over kernel classes via convex programming. J. Machine Learning Research, 13, 389-427.\nJ. Mairal and B. Yu (2012). Complexity analysis of the Lasso regularization path. Proc. of International Conference on Machine Learning (ICML).\n\n\n\nYanfeng Gu, Shizhe Wang, Tao Shi, Yinghui Lu, Eugene E. Clothiaux, and Bin Yu (2012). Multiple-kernel learning-based unmixing algorithm for estimation of cloud fractions with MODIS and CLOUDSAT data. Proc. of IEEE International Geoscience and Remote Sensing Symposium (IGRSS).\n\n\n\nS. Nishimoto, A. T. Vu, T. Naselaris, Y. Benjamini, B. Yu, J. L. Gallant (2011). Reconstructing visual experiences from brain activity evoked by natural movies. Current Biology, 21(19), 1641-1646. related videos\nP. Ravikumar, M. Wainwright, G. Raskutti, B. Yu (2011). High-dimensional covariance estimation by minimizing l1-penalized log-determinant divergence. Electronic Journal of Statistics, 5, 935-980.\nG. Raskutti, M. Wainwright, B. Yu (2011). Minimax rates of estimation for high-dimensional linear regression over lq-balls. IEEE Trans. Inform. Th., 57(10), 6976-6994.\nK. Rohe, S. Chatterjee, and B. Yu (2011). Spectral clustering and the high-dimensional Stochastic Block Model. Annals of Statistics, 39 (4), 1878-1915\nV. Q. Vu, P. Ravikumar, T. Naselaris, K. N. Kay, J. L. Gallant, B. Yu* (2011). Encoding and decoding V1 fMRI responses to natural images with sparse nonparametric models. Annals of Applied Statistics, 5, 1150-1182. (*First senior author as last author in biology tradition)\nS. N. Pakzad, G. Rocha, and B. Yu (2011). Distributed modal identification by regularized auto regressive models. International Journal of Systems Science, 42, 1473-1489.\nJ. Yousafzai, P. Sollich, Z. Cvetkovic, and B. Yu (2011). Combined Features and Kernel Design for Robust Phoneme Classification Using Support Vector Machines. IEEE Trans. Audio, Speech and Language Processing (to appear). 64.\nX. Dai, J. Jia, B. Yu, El Ghaoui (2011) SBA-term: Sparse Bilingual Association for terms. Proc. International Conference on Semantic Computing.\nB. Yu (2011). Asymptotics and Coding Theory: One of the n - 1 Dimensions of Terry. In Selected Works of Terry Speed (ed.¬†S. Duoit), pp.¬†33-36, Springer.\n\n\n\nB. Yu (2010). Remembering Leo. Annals of Applied Statistics, 4(4), 1657-1659.\nJ. Jia, Y. Benjamini, C. Lim, G. Raskutti, B. Yu (2010). Comment on ‚ÄúEnvelope models for parsimonious and efficient multivariate linear regression‚Äù by R. D. Cook, B. Li, and F. Chiaromonte. Statistica Sinica, 20, 961-967.\nG. Raskutti, M. Wainwrigt, and B. Yu (2010) Restricted Eigenvalue Properties for Correlated Gaussian Designs. Journal of Machine Learning Research, 11, 2241-2259.\nJ. Jia and B. Yu (2010). On model selection consistency of elastic net when p &gt;&gt;n. Statistica Sinica, 10, 595-611.\nP. Buhlmann and B. Yu (2010). Boosting. Wiley Interdisciplinary Reviews: Computational Statistics, 2, 69-74.\nL. Huang, J. Jia, B. Yu, B. Chun, P. Maniatis, M. Naik (2010). Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression. Proc. NIPS 2010.\nY. Han, F. Wu, J. Jia, Y. Zhuang and B. Yu (2010). Multi-task Sparse Discriminant Analysis (MtSDA) with Overlapping Categories. Proc. of The 24th AAAI Conference on Artificial Intelligence, July 11-15, Atlanta, GA.\nB. Gawalt, J. Jia, L. Miratrix, L. El Ghaoui, B. Yu, and S. Clavier (2010). Discovering Word Associations in News Media via Feature Selection and Sparse Classification. Proc. 11th ACM SIGMM International Confernece on Multimedia Information Retrieval (MIR).\n\n\n\nE. Anderes, B. Yu, V. Jovanovic, C. Moroney, M. Garay, A. Braverman, E. Clothiaux (2009) Maximum Likelihood Estimation of Cloud Height from Multi-Angle Satellite Imagery. Annals of Applied Statistics, 3, 902-921\nT. Shi, M. Belkin, and B. Yu, (2009) Data Spectroscopy: Eigenspace of Convolution Operator and Clustering Annals of Statistics, 37 (6B), 3960-3984.\nVincent Q. Vu, Bin Yu, Robert E. Kass (2009) Information In The Non-Stationary Case Neural computation 21, 688-703.\nN. Meinshausen and B. Yu (2009). Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics 37, 246-270.\nP. Zhao, G. Rocha, and B. Yu (2009). The composite absolute penalties family for grouped and hierarchical variable selection Annals of Statistics 37, 3468-3497. (An earlier version ‚Äòappeared as Grouped and hierarchical model selection through composite absolute penalties‚Äô by P. Zhao, G. Rocha and B. Yu, Department of Statistics, UC Berkeley, Tech. Rep 703.)\nS. Negahban, P. Ravikumar, M. Wainwright, and B. Yu (2009). A unified framework for high-dimensional analysis of \\(M\\)-estimators with decomposable regularizers Proc. NIPS, 2009. (This conference paper contains preliminary results of the journal submission Negahban et al.¬†2012).\nG. Raskutti, M. Wainwright, B. Yu (2009) High-dimensional regression under lq-ball sparsity: Optimal rates of convergence. Proc. of Allerton Conference on Communication, Control, and Computing. (This conference paper contains some of preliminary results of the journal submission Ravikumar et al.¬†2011).\nG. Raskutti, M. Wainwrigt, and B. Yu (2009) Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness. Proc. NIPS, 2009. (This conference paper contains some of preliminary results of the journal submission Ravikumar et al.¬†2011).\n\n\n\nT. Shi, B. Yu, E. Clothiaux, and A. Braverman (2008). Daytime Arctic Cloud Detection based on Multi-angle Satellite Data with Case Studies. Journal of American Statistical Association. 103( 482), 584-593.\nPeter Buhlmann and Bin Yu (2008) Invited discussion on ‚ÄúEvidence contrary to the statistical view of boosting (D. Mease and A. Wyner)‚Äù. (paper with discussion) Journal of Machine Learning Research 9, 187-194.\nP. Ravikumer, V. Vu, B. Yu, T. Naselaris, K. Kay, J. Gallant (2008). Nonparametric sparse hiearchical models describe V1 fMRI responses to natural images In Adavances in Neural Information Processing Systems (NIPS) 21, (2008). (This conference paper contains some preliminary results of journal paper Vu et al.¬†(2011) on encoding models, but also contains an encoding model that is not in Vu et al.¬†(2011). It does not contain decoding results.)\nP. Ravikumar, G. Raskutti, M. Wainwright, B. Yu (2008) Model selection in Gaussian graphical models: high-dimensional consistency of l1-regularized MLE. In Adavances in Neural Information Processing Systems (NIPS) 21, (2008).\nT. Shi, M. Belkin, and B. Yu (2008). Data spectroscopy: learning mixture models using eigenspaces of convolution operators. Proc. of ICML 2008.\nM. Ager, Z. Cvetkovic, P. Pollich, and B. Yu (2008). Towards Robust Phoneme Classification Augmentation of PLP Models with Acoustic Waveforms. Proceedings of EUSIPCO.\nJ. Yousafzai, Z. Cvetkoviƒá, P. Pollich, and B. Yu (2008). Combined PLP-Acoustic Waveform Classification for Robust Phoneme Recognition using Support Vector Machines. Proceedings of EUSIPCO.\n\n\n\nN. Meinshausen, G. Rocha, and B. Yu (2007). A tale of three cousins: Lasso, L2Boosting, and Danzig Annals of Statistics (invited discussion on Candes and Tao‚Äôs Danzig Selector paper)\nV. Vu, B. Yu, and R. Kass (2007). Coverage Adjusted Entropy Estimation. Statistics and Medicine, 26(21), 4039-4060.\nB. Yu (2007). Embracing Statistical Challenges in the Information Technology Age Technometrics (special issue on statistics and information technologies). vol.¬†49 (3), 237-248.\nX. Jiang, Y. Liu, B. Yu and M. Jiang (2007). Comparison of MISR aerosol optical thickness with AERONET measurements in Beijing metropolitan area. Remote Sensing of Environment (Special Issue on Multi-angle Imaging SpectroRadiometer), vol.¬†107, pp.¬†45-53.\nT. Shi, E. E. Clothiaux, B. Yu, A. J. Braverman, and G. N. Groff (2007). Detection of Daytime Arctic Clouds using MISR and MODIS Data. Remote Sensing of Environment (Special Issue on Multi-angle Imaging SpectroRadiometer), vol.¬†107, pp.¬†172-184.\n\n\n\nPeng Zhao and Bin Yu (2006). On Model Selection Consistency of Lasso. J. Machine Learning Research, 7 (nov), 2541-2567.\nB. Yu (2006). Comments on: Monitoring networked applications with incremental quantile estimation by Chambers et al. Statist. Sci., 21, 483-485.\nB. Yu (2006). Comments on: Regularization in Statistics, by P. J. Bickel and B. Li. Test, vol.¬†15 (2), pages 314-316.\nP. Buhlmann and B. Yu (2006). Sparse Boosing Journal of Machine Learning Research ( 7 (June), 1001-1024). This is a shortened and more focused version of Buhlmann and Yu ‚ÄúBoosting, Model Selection, Lasso and Nonnegative Garotte‚Äù given below.\nJ. Gao, H. Suzuki, and B. Yu (2006). Approximation Lasso Methods for Language Modeling. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp.¬†225-232, Sydney.\n\n\n\nT. Shi and B. Yu (2005). Binning in Gaussian Kernel Regularization. Statistica Sinica (special issue on machine learning), 16, 541-567.\nG. Liang, N. Taft, and B. Yu (2005). A fast lightweight approach to origin-destination IP traffic estimation using partial measurements. Tech Report 687, Statistics Department, UCB (accepted for Special Issue of IEEE-IT and ACM Networks on data networks, Jan.¬†2006)\nTong Zhang and B. Yu (2005). Boosting with early stopping: convergence and consistency. The Annals of Statistics. Vol. 33, 1538-1579.\nCastro, M. Coates, G. Liang, R. Nowak, and B. Yu (2005) Network tomography: recent developments. Statistical Science, 19, 499-517.\nC. D. Giurcaneanu and B. Yu (2005). Efficient algorithms for discrete universal denoising for channels with memeory. Proceedings of International Symposium on Information Theory, Australia. (Also as Tech. Report 686, Statistics Department, UCB (Proc. ISIT, Sept.¬†2005))\n\n\n\nP. Zhao and B. Yu (2004). Stagewise Lasso (old title: Boosted Lasso) Journal of Machine Learning Research, 8, 2701-2726. (An earlier version appeared as Tech. Report #678, Statistics Department, UC Berkeley (December, 2004; revised in April, 2005)\nD. J. Diner et al (2004). PARAGON: A Systematic, Integrated Approach to Aerosol Observation and Modeling. American Meterological Society, Oct., 1491-1501.\nP. Buhlmann and B. Yu (2004). Discussion on three boosting papers by Jiang, Lugosi and Vayatis, and Zhang Annals of Statistics. 32 (1): 96-101.\nR. Jorsten and B. Yu (2004). Compressing genomic and proteomic array images for statistical analyses. Invited chapter in a book on Genomic signal processing and statistics, edited by E. R. Dougherty, I. Shmulevich, J. Chen, and Z. J. Wang, pp.¬†341 - 366.\nG. Liang, B. Yu, and N. Taft (2004). Maximum entropy models: convergence rates and application in dynamic system monitoring. International Symposium on Information Theory, Chicago.\n\n\n\nR. Castro, M. Coates, G. Liang, R. Nowak, and B. Yu (2003). Internet Tomography: Recent Developments Statistical Science. Vol. 19(3), 499-517.\nG. Liang and B. Yu (2003). Maximum Pseudo Likelihood Estimation in Network Tomography. IEEE Trans. on Signal Processing (Special Issue on Data Networks). 51(8), 2043-2053\nRebecka Jornsten and Bin Yu (2003). Simultaneous Gene Clustering and Subset Selection for Classification via MDL. Bioinformatics. 19(9): 1100-1109.\nPeter Buhlmann and Bin Yu (2003). Boosting with the L2 Loss: Regression and Classification. J. Amer. Statist. Assoc. 98, 324-340.\nR. Jornsten, W. Wang, B. Yu, and K. Ramchandran (2003). Microarray image compression: SLOCO and the effects of information loss. Signal Processing Journal (Special Issue on Genomic Signal Processing). 83, 859-869.\nG. Liang and B. Yu (2003). Pseudo Likelihood Estimation in Network Tomography. Proceedings of of Infocom, San Francisco.\n\n\n\nPeter Buhlmann and Bin Yu (2002). Analyzing Bagging. Annals of Statistics vol.¬†30, 927-961.\nR. Jornsten, M. Hansen, and B. Yu (2002). Adaptive Minimum Description Length (MDL) criteria with applications to microarray data. In Advances in Minimum Description Length: Theory and Applications, edited by P. Grunwald, I.J. Myung and M.A.¬†Pitt. The MIT Press, pp.¬†295-321.\nMark Hansen and Bin Yu (2002). Minimum Description Length Model Selection Criteria for Generalized Linear Models.{}, IMS Lecture Notes ‚Äì Monograph Series, Vol. 40.\nRebecka Jornsten, and Bin Yu (2002). Multiterminal Estimation: Extensions and a Geometric interpretation. Proceedings of International Symposium on Information Theory (ISIT), June, 2002.\nGerald Schuller, Bin Yu, Dawei Huang, and Bern Edler (2002). Perceptual Audio Coding using Pre- and Poster- Filters and Lossless Compression. IEEE Trans. Speech and Audio Processing. Vol. 10 (6), 379-390\nMark Coates, Alfred Hero, Robert Nowak, and Bin Yu (2002). Internet Tomography. Signal Processing Magazine. vol.¬†19, No.¬†3 (May issue), 47-65.\n\n\n\nM. Hansen and B. Yu (2001). Model selection and the principle of Minimum Description Length. Journal of American Statistical Association. 96, 746-774.\n\n\n\nJin Cao, Drew Davis, Scott Vander Wiel and Bin Yu (2000). [PDF| Time-varying network tomography: router link data. J. Amer. Statist. Assoc. vol.¬†95, 1063-1075.\nPeter Buhlmann and Bin Yu (2000). Discussion. Additive logistic regression: a statistical view of boosting, by Friedman, J., Hastie, T. and Tibshirani, R. Annals of Statistics. Vol. 28, 377-386\nMark Hansen and Bin Yu (2000). Wavelet thresholding via MDL for natural images. IEEE Trans. Inform. Theory (Special Issue on Information Theoretic Imaging). vol.¬†46, 1778-1788.\nJorma Rissanen and Bin Yu (2000). Coding and compression: a happy union of theory and practice. J. Amer. Statist. Assoc. (Year 2000 Commemorative Vignette on Engineering and Physical Sciences). vol.¬†95, 986-988.\nLei Li and Bin Yu (2000). Iterated logarithm expansions of the pathwise code lengths for exponential families. IEEE Trans. Inform. Theory. vol.¬†46, 2683-2689.\nG. Chang, B. Yu and M. Vetterli (2000). Adaptive wavelet thresholding for image denoising and compression. IEEE Trans. Image Processing, vol.¬†9, 1532-1546.\nG. Chang, B. Yu and M. Vetterli (2000). Spatially adaptive wavelet thresholding based on context modeling for image denoising. IEEE Trans. Image Processing, vol.¬†9, 1522-1531.\nG. Chang, B. Yu and M. Vetterli (2000). Wavelet thresholding for multiple noisy image copies. IEEE Trans. Image Processing, vol.¬†9, 1631-1635.\n\n\n\nY. Yoo, A. Ortega, and B. Yu (1999). Image subband coding using context-based classification and adaptive quantization. IEEE Trans. Image Processing, vol.¬†8, 1702-1215.\nB. Yu, M. Ostland, P. Gong and R. Pu (1999). Penalized discriminant analysis of in situ hyperspectral data for conifer species recognition. IEEE Trans. Geoscience and Remote Sensing, in press.\n\n\n\nA. Barron, J. Rissanen, and B. Yu (1998). The Minimum Description Length principle in coding and modeling. (Special Commemorative Issue: Information Theory: 1948-1998) IEEE. Trans. Inform. Th., 44, 2743-2760. Reprinted in Information 50 Years of Discovery, Theory: S. Verdu and S. McLaughlin (eds), IEEE Press , 1999.\nB. Yu and P. Mykland (1998). Looking at Markov samplers through cusum path plots: a simple diagnostic idea. Statistics and Computing , 8, 275-286.\nP. Gong, R. Pu and B. Yu (1998) Conifer species recognition: effects of data transformation and band width (in Chinese) Journal of Remote Sensing, 2(3), 211-217.\nG. Chang, B. Yu and M. Vetterli (1998). Spatially adaptive wavelet thresholding for image denoising. Proceedings of IEEE International Conference on Image Processing, October, Chicago.\nS. G. Chang, B. Yu, and M. Vetterli (1998). Image denoising via lossy compression and wavelet thresholding. Proceedings of International Conference on Image Processing. Santa Barbara, California, vol.¬†1, pp.¬†604-607.\nM. Ostland and B. Yu (1997). Exploring quasi Monte Carlo for marginal density approximation. Statistics and Computing, 7, 217-228.\nP. Gong, R. Pu, and B. Yu (1997). Conifer species recognition with in Situ hyperspectral data. Remote Sensing of Environment, 62, 189-200.\nB. Yu and T. P. Speed (1997). Information and the clone mapping of chromosomes. Ann. Statist. 25, 169-185.\n\n\n\nD. Nelson, T. Speed, and B. Yu (1997). The limits of random fingerprinting. Genomics, 40, 1-12.\nB. Yu (1997). Assouad, Fano, and Le Cam. Festschrift for Lucien Le Cam . D. Pollard, E. Torgersen, and G. Yang (eds), pp.¬†423-435, Springer-Verlag.\n\n\n\nB. Yu (1996). Lower bounds on expected redundancy for nonparametric classes. IEEE Trans. on Information Theory, 42, 272-275.\nY. Yoo, A. Ortega, and B. Yu (1996). Adaptive quantization of image subbands with efficient overhead rate selection. In Proceedings of IEEE International Conference on Image Processing, Lausanne, Switzerland.\nB. Yu (1996). A Statistical analysis of adaptive scalar quantization based on quantized past data. In Proceedings of International Symposium on Information Theory and its Applications (ISITA96), Victoria, Canada.\n\n\n\nB. Yu (1995). Comment: Extracting more diagnostic information from a single run using cusum path plot. Statist. Sci., 10, 54-58.\nJ. Rissanen and B. Yu (1995). MDL learning. In Learning and Geometry: Computational Approaches, Progress in Computer Science and Applied Logic, 14, David Kueker and Carl Smith (eds), Birkh√§user, Boston, pp.¬†3-19.\nP. Mykland, L. Tierney, and B. Yu (1995). Regeneration in Markov Chain samplers. J. Amer. Statist. Assoc., 90, 233-241.\n\n\n\nB. Yu (1994). Rates of convergence for empirical processes of stationary mixing sequences. Ann. Probab. 22, 94-116.\nM. Arcones and B. Yu (1994). Central limit theorems for empirical and U-processes of stationary mixing sequences. J. Theor. Probab. 7, 47-71.\nB. Yu (1994). Lower bound on the expected redundancy for classes of continuous Markov sources. In Statistical Decision Theory and Related Topics V, S. S. Gupta and J. O. Berger (eds), 453-466.\nM. Arcones and B. Yu (1994). Limit theorems for empirical processes under dependence. In Proceedings in Chaos expansions, multiple Wiener integrals and their applications. 205-221.\nA. R. Barron, Y. Yang and B. Yu (1994). Asymptotically optimal function estimation by minimum complexity criteria. In Proceedings of 1994 International Symposium on Information Theory, pp.¬†38, Trondheim, Norway.\n\n\n\nB. Yu and T. Speed (1993). A rate of convergence result for a universal D-semifaithful code. IEEE Trans. on Information Theory 39, 8813-820.\nB. Yu (1993). Density estimation in the L‚àû norm for dependent data with applications to the Gibbs sampler. Ann. Statist. 21, 711-735.\nT. Speed and B. Yu (1993). Model selection and prediction: normal regression. J. Inst. Statist. Math. 45, 35-54.\n\n\n\nJ. Rissanen, T. Speed and B. Yu (1992). Density estimation by stochastic complexity. IEEE Trans. on Information Theory, 38, 315-323.\nB. Yu and T. Speed (1992) Data compression and histograms. Probability Theory and Related Fields, 92, 195-229."
  },
  {
    "objectID": "papers.html#section",
    "href": "papers.html#section",
    "title": "Papers",
    "section": "",
    "text": "A. R. Hsu, Y. Cherapanamjeri, A. Y. Odisho, P. R. Carroll, B. Yu (2024). Mechanistic Interpretation through Contextual Decomposition in Transformers.\nY. S. Tan, O. Ronen, T. Saarinen, B. Yu (2024). The Computational Curse of Big Data for Bayesian Additive Regression Trees: a Hitting Time Analysis.\nO. Ronen, A. I. Humayun, R. Balestriero, R. Baraniuk, B. Yu (2024). ScaLES: Scalable Latent Exploration Score for Pre-Trained Generative Networks\nS. Hayou, N. Ghosh, B. Yu (2024). The Impact of Initialization on LoRA Fineuning Dynamics.\nB. Yu (2024). After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI (Harvard Data Science Review)\nS. Hayou, N. Ghosh, B. Yu (2024). LoRA+: Efficient Low Rank Adaptation of Large Models (Proc. ICML)\nC. F. Elliott, J. Duncan, T. M. Tang, M. Behr, K. Kumbier, B. Yu (2024). Designing a data science simulation with MERITS: a primer.\nY. Chen, C. Singh, X. Liu, S. Zuo, B. Yu, H. He, J. Gao (2024). Towards consistent natural-language explanations via explanation-consistent finetuning.\nN. R. Mallinar, A. Zane, S. Frei, B. Yu (2024). Minimum-Norm Interpolation Under Covariate Shift. (Proc. ICML) arXiv\nL. Sun, A. Agarwal, A. Kornblith, B. Yu, C. Xiong (2024). ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance. (Proc. ICML) arXiv"
  },
  {
    "objectID": "papers.html#section-1",
    "href": "papers.html#section-1",
    "title": "Papers",
    "section": "",
    "text": "C. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu, J. Gao (2023). Explaining black box text modules in natural language with language models.\nQ. Zhang, C. Singh, L. Liu, X. Liu, B. Yu, J. Gao, T. Zhao (2023). Tell your model where to attend: post-hoc attention steering for LLMs.\nQ. Wang, T. M. Tang, N. Youlton, C. S. Weldy, A. M. Kenney, O. Ronen, J. W. Hughes, E. T. Chin, S. C. Sutton, A. Agarwal, X. Li, M. Behr, K. Kumbier, C. S. Moravec, W. H. W. Tang, K. B. Margulies, T. P. Cappola, A. J. Buitte, R. Arnaout, J. B. Brown, J. R. Priest, V. N. Parikh, B. Yu, E. Ashley (2023). Epistasis regulates genetic control of cardiac hypertrophy. medRxiv (Code) (PCS documentation)\nR. Cahill, Y. Wang, R. P. Xian, A. J. Lee, H. Zeng, B. Yu, B. Tasic, R. Abbasi-Asl (2023). Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology. bioRxiv (Code)\nE. Irajizad, A. Kenney, T. Tang, J. Vykoukal, R. Wu, E. Murage, J. B. Dennison, M. Sans, J. P. Long, M. Loftus, J. A. Chabot, M. D. Kluger, F. Kastrinos, L. Brais, A. Babic, K. Jajoo, L. S. Lee, T. E. Clancy, K. Ng, A. Bullock, J. M. Genkinger, A. Maitra, K. A. Do, B. Yu, B. W. Wolpin, S. Hanash, J. F. Fahrmann. (2023). A blood-based metabolomic signature predictive of risk for pancreatic cancer. Cell Reports Medicine 4(9): 101194. doi: 10.1016/j.xcrm.2023.101194. (PCS related) (Editorial)\nR. Dwivedi, C. Singh, B. Yu, M. Wainwright (2023). Revisiting miniumu description length complexity in overparametrized models. JMLR, 24(268): 1-59.\nK. Wu, Y. Chen, W. Ha, B. Yu (2023). Prominent roles of conditionally invariant components in domain adaptation: theory and algorithms. JMLR (accepted). https://arxiv.org/abs/2309.10301\nN. Ghosh, S. Frei, W. Ha, B. Yu (2023). The effect of SGD batch size on autoencoder learning: sparsity, sharpness and feature learning. https://arxiv.org/abs/2308.03215\nR. Netzorg, J. Li, B. Yu (2024). Improving prototypical part networks with reward reweighting, reselection, and retraining. Proc. ICML. https://arxiv.org/abs/2307.03887\nA. Agarwal, A. M. Kenny, Y. S. Tan, T. M. Tang, B. Yu (2023). MDI+: a flexible random forest-based feature importance framework. https://arxiv.org/abs/2307.01932 (PCS related)\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. Proc. ICLR. https://arxiv.org/abs/2305.17588\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu and J. Gao (2023). Explaining black box text modules in natural language with language models. https://arxiv.org/abs/2305.09863"
  },
  {
    "objectID": "papers.html#section-2",
    "href": "papers.html#section-2",
    "title": "Papers",
    "section": "",
    "text": "D. Shen, P. Ding, J. Sekhon, B. Yu (2022). Same root different leaves: time series and cross-sectional methods in panel data. Econometrica (accepted). https://arxiv.org/abs/2207.14481\nB. Park, X. Wu, B. Yu, A. Zhou (2022). Offline evaluation in RL: soft stability weighting to combine fitted Q-learning and model-based methods. NeurIPS 3rd Offline Reinforcement Learning Workshop.\nY. S. Tan, C. Singh, K. Nasseri, A. Agarwal, J. Duncan, O. Ronen, M. Epland, A. Kornblith, B. Yu (2022). Fast interpretable greedy-tree sums (FIGS). (imodels üîé contains code for FIGS)\nA. Agarwal, Y. S. Tan, O. Ronen, C. Singh, B. Yu (2022). Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods. Proc. ICML (imodels üîé contains code for hierarchical shrinkage (HS)).\nB. Yu and C. Singh (2022). Seven principles for rapid-response data science: lessons learned from covid-19 forecasting. Statistical Science, 36(2):266-269.\nN. Ghosh, S. Mei, and B. Yu (2022). The three stages of dynamics in high-dimensional kernel methods. Proc. ICLR, 2022."
  },
  {
    "objectID": "papers.html#section-3",
    "href": "papers.html#section-3",
    "title": "Papers",
    "section": "",
    "text": "Y. Tan, A. Agarwal, and B. Yu (2021). A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds. Proc. AISTATS.\nN. Altieri, B. Park, J. DeNero, A. Odisho, B. Yu. (2021). Improving natural language information extraction from cancer pathology reports using transfer learning and zero-shot string similarity. JAMIA Open. 2021 Sept.¬†30 4(3).\nC. Singh, W. Ha and B. Yu (2021). Interpreting and Improving Deep-Learning Models with Reality Checks. To appear in ‚ÄúxxAI - Beyond Explainable AI‚Äù (eds.¬†Holzinger et al.).\nW. Ha, C. Singh, F. Lanusse, S. Upadhyayula, and B. Yu (2021). Adaptive Wavelet Distillation from Neural Networks through Interpretation. Proc. NeurIPS 2021. (code)\nM. Behr, Y. Wang, X. Li, B. Yu (2022). Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests. PNAS. (theory for iRF, PCS-related)\nN. Altieri, B. Park, M. Olson, J. DeNero, A. Odisho, B. Yu. (2021). Supervised line attention for tumor attribute classification from pathology reports: Higher performance with less data. Journal of Biomedical Informatics. 122 (2021) 103872. arXiv version"
  },
  {
    "objectID": "papers.html#section-4",
    "href": "papers.html#section-4",
    "title": "Papers",
    "section": "",
    "text": "M. Behr, K. Kumbier, A. Cordova-Palomera, M. Aguirre, E. Ashley, A. Butte, R. Arnaout, J. B. Brown, J. Preist, B. Yu (2020). Learning epistatic polygenic phenotypes with Boolean interactions (code) (PCS inference case study)\nB. Norgeot, G. Quer, B. K. Beaulieu-Jones, A. Torkamani, R. Dias, M. Gianfrancesco, R. Arnaout, I. S. Kohane, S. Saria, E. Topol, Z. Obermeyer, B. Yu & A. Butte (2020). Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist, Nature Medicine, 26, 1320‚Äì1324.\nB. Yu (2020). Stability expanded, in reality. Harvard Data Science Review (PCS related)\nB. Yu and R. Barter (2020). Data science process: one culture. JASA. (PCS related)\nR. Dwivedi, Y. Tan, B. Park, M. Wei, K. Horgan, D. Madigan, B. Yu (2020). Stable discovery of interpretable subgroups via calibration in causal studies (staDISC). International Statistical Review (code) (PCS case study for causal inference)\nX. Li, T. M. Tang, X. Wang, J. A. Kocher, B. Yu (2020). A stability-driven protocol for drug response interpretable prediction (staDRIP). NeurISP workshop on ML4H (Machine learning for Health) Extended Abstract.\nA. Y. Odisho, B. Park, N. Altieri, J. DeNero, M. R Cooperberg, P. R .Carroll, B. Yu (2020). Natural language processing systems for pathology parsing in limited data environments with uncertainty estimation. JAMIA Open.\nL. Reiger, J. W. Murdoch, S. Singh, B. Yu (2020). Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge. ICML Proceedings. (code)\nC. Singh, W. Ha, F. Lanusse, V. Boehm, J. Liu, B. Yu (2020). Transformation Importance with Applications to Cosmology ICLR Workshop paper. (code)\nN. Altieri, R. Barter, J. Duncan, R. Dwivedi, K. Kumbier, X. Li, R. Netzorg, B. Park, C. Singh, Y. Tan, T.Tang, Y. Wang, C. Zhang, B. Yu. (2020) Curating a COVID-19 data repository and forecasting county-level death counts in the United States. Harvard Data Science Review (code) 7-day prediction results Short talk video\nB. Yu and K. Kumbier (2020) Veridical data science (PCS framework), PNAS. 117 (8), 3920-3929. QnAs with Bin Yu.\nR. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan and B. Yu (2020) Sharp Analysis of Expectation-Maximization for Weakly Identifiable Mixture Models AISTATS.\nR. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan and B. Yu (2020) Singularity, Misspecification and the Convergence Rate of EM Annals of Statistics.\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Fast Mixing of Metropolized Hamiltonian Monte Carlo: Benefits of Multi-Step Gradients, JMLR, arXiv"
  },
  {
    "objectID": "papers.html#section-5",
    "href": "papers.html#section-5",
    "title": "Papers",
    "section": "",
    "text": "R. Dwivedi, Y. Chen, M. J. Wainwright and B. Yu (2019) Log-concave Sampling: Metropolis Hastings Algorithms are Fast JMLR.\nD. Rothenh√§usler and B. Yu (2019). Incremental causal effects."
  },
  {
    "objectID": "papers.html#section-6",
    "href": "papers.html#section-6",
    "title": "Papers",
    "section": "",
    "text": "Y. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2018) Fast MCMC Algorithms on Polytopes. JMLR . http://jmlr.org/papers/v19/18-158.html\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Vaidya Walk: A Sampling Algorithm Based on Volumetric-Logarithmic Barrier Allerton Conference 2017 https://ieeexplore.ieee.org/abstract/document/8262876/\nW. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu* (2019) Definitions, methods, and applications in interpretable machine learning. PNAS, 116 (44) 22071-22080.\nW. J. Murdoch, C. Sign, and B. Yu (2019). Hierarchical interpretations for neural network predictions. ICLR. (code)\nY. Wang, S. Wu and B. Yu (2020) Unique Sharp Local Minimum in l1-minimization Complete Dictionary Learning. JMLR. 21(63), pp.¬†1-52. Also at https://arxiv.org/abs/1902.08380\nY. Chen, R. Abbasi-Asl, A. Bloniarz, M. Oliver, B. Willmore, J. Gallant, and B. Yu (2018) The DeepTune framework for modeling and characterizing neurons in visual cortex area V4 https://www.biorxiv.org/content/10.1101/465534v1\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. https://arxiv.org/abs/1810.0728 (an enhanced version of iRF, PCS related)\nY. Chen C. Jin, and B. Yu (2018) Stability and Convergence Trade-off of Iterative Optimization Algorithms. https://arxiv.org/abs/1804.01619\nJ. Murdoch, P. Liu, and B. Yu (2018) Beyond word importance: contextual decomposition to extract interactions from LSTMs. Proc. ICLR 2018. https://arxiv.org/abs/1705.07356 (code)\nR. Diwivedi, Y. Chen, M. J. Wainwright, and B. Yu (2018) Log-concave sampling: Metropolis-Hastings algorithms are fast! https://arxiv.org/abs/1801.02309."
  },
  {
    "objectID": "papers.html#section-7",
    "href": "papers.html#section-7",
    "title": "Papers",
    "section": "",
    "text": "Y. Chen, R. Dwivedi, M. J. Wainwright, and B. Yu (2017) Fast MCMC sampling algorithms on polytopes https://arxiv.org/abs/1710.08165.\nB. Yu and K. Kumbier (2018) Artificial Intelligence and Statistics Frontiers of Information Technology and Electronic Engineering. 19(1), 6-9.\nR. Abbasi-Asl and B. Yu (2017) Structural Compression of Convolutional Neural Networks Based on Greedy Filter Pruning https://arxiv.org/abs/1705.07356\nR. Abbasi-Asl and B. Yu (2017) Interpreting Convolutional Neural Networks Through Compression. NIPS 2017. Symposium on Interpretable Machine Learning. (also https://arxiv.org/abs/1711.02329)\nS. Kunzel, J. Sekhon, P. Bickel, and B. Yu* (2019) Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning, PNAS. 116 (10) 4156-4165. https://arxiv.org/abs/1706.03461. (code)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (code) (PCS related)\nS. Balakrishnan, M. Wainwright, B. Yu (2017) Statistical Guarantees for the EM algorithm: from population to sample-based analysis. Annals of Statistics, 45(1), 77 - 120.\nR. Barter and B. Yu (2017) Superheat: An R package for creating beautiful and extendable heatmaps for visualizing complex data JCGS (revised). https://github.com/rlbarter/superheat\nH. Liu and B. Yu (2017) Comments on: High-dimensional simultaneous inference with the bootstrap by Dezeure et al Test. 26: 740-750.\nC. Carson et al (2016). UC Berkeley Data Science Planning Initiative Faculty Advisory Board (FAB) Report. FAB Report Executive Summary\nS. Wu and B. Yu (2018). Local identifiability of l1-minimization dictionary learning: a sufficient and almost necessary condition. JMLR. 18, 1 - 56."
  },
  {
    "objectID": "papers.html#section-8",
    "href": "papers.html#section-8",
    "title": "Papers",
    "section": "",
    "text": "K. Rohe, T. Qin and B. Yu* (2016). Co-clustering directed graphs to discover asymmetries and directional communities. Proc. National Academy of Sciences (PNAS), 113(45), 12679 - 12684.\nR. E. Kass, B. S. Caffo, M. Davidian, X. Meng, B. Yu, Nancy Reid* (2016). Ten simple rules for effective statistical practice. PLoS Comput. Biol., 12(6): e1004961. doi:10.1371/journal.pcbi.1004961\nSiqi Wu, Antony Joseph, Ann S. Hammonds, Susan E. Celniker, Bin Yu, and Erwin Frise (2016). Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks (with support information). PNAS, pp.¬†4290 - 4295. (code) (PCS related)\nA. Bloniarz, C. Wu, B. Yu, A. Talwalkar (2016). Supervised neighborhoods for distributed nonparametric regression. Proc. of AISTATS, Barcelona, Spain."
  },
  {
    "objectID": "papers.html#section-9",
    "href": "papers.html#section-9",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (2015). Data wisdom for data science. Operational Database Management Systems (ODBMS.ORG).\nA. Bloniarz, H. Liu, C. Zhang, J. Sekhon, and B. Yu* (2015). Lasso adjustments of treatment effect estimates in randomized experiments. PNAS. 113, 7383 - 7390.\nP. Ma, M. W. Mahoney and B. Yu (2015). A Statistical Perspective on Algorithmic Leveraging. Journal of Machine Learning Research, 16, (2015), 861-911.\nT. Moon, Y. Wang, Y. Liu, and B. Yu (2015). Evaluation of a MISR-based high-resolution aerosol retrieval method using AERONET DRAGON campaign data. IEEE Transactions on Geoscience and Remote Sensing, 53, 4328-4339."
  },
  {
    "objectID": "papers.html#section-10",
    "href": "papers.html#section-10",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (2014). Let us own data science. video IMS Bulletin Institute of Mathematical Statistics (IMS) Presidental Address, ASC-IMS Joint Conference, Sydney, July, 2014.\nG. Schiebinger, M. J. Wainwright and B. Yu (2014). The geometry of kernelized spectral clustering. Annals of Statistics, 43, 819-846.\nL. Miratrix, J. Jia, B. Yu, B. Gawalt, L. El Ghaoui, L. Barnesmoore, S. Clavier (2014). Concise comparative summaries (CCS) of large text corpora with a human experiment. Ann. Applied Statist., 8, 499-529.\nY. Benjamini and B. Yu (2014). The shuttle estimator for explainable variance in fMRI experiments. Annals of Applied Statistics, 7, 2007-2033.\nD. Bean, P. Bickel, N. El Karoui and B. Yu (2014). Optimal M-estimation in high-dimensional regression. Proceedings of National Academy of Sciences, 110, 1456314568.\nN. El Karoui, D. Bean, P. Bickel, C. Lim, and B. Yu (2014). On robust regression with high-dimensional predictors. Proceedings of National Academy of Sciences, 110, 1455714562.\nP. Ma, M. W. Mahoney, B. Yu (2014). A Statistical Perspective on Algorithmic Leveraging. Proc. of International Conference on Machine Learning (ICML) (This conference paper contains some of preliminary results of the journal submission Ma et al.¬†(2015))\nA. Bloniarz, A. Talwalkar, J. Terhorst, M. Jordan, D. Patterson, B. Yu and Y. Song (2014). Changepoint Analysis for Efficient Variant Calling. Proc. of RECOMB 2014 (to appear).\nTao Shi (2013), A conversation with Professor Bin Yu International Chinese Statistical Association (ICSA) Bulletin, Vol 25, Issue 2, pp 85-98. (Selected Parts in Statblogs)\nA. Joseph and B. Yu (2016). The impact of regularization on spectral clustering. Annals of Statistics. 4, 1765 - 1791.\nC. Lim and B. Yu (2016). Estimation Stability with Cross Validation (ESCV) Journal of Computational and Graphical Statistics. 25, 464 - 492. (First paper towards PCS)"
  },
  {
    "objectID": "papers.html#section-11",
    "href": "papers.html#section-11",
    "title": "Papers",
    "section": "",
    "text": "A. S. Hammonds, C. A. Bristow, W. W. Fisher, R. Weiszmann, S. Wu, V. Hartenstein, M. Kellis, B. Yu, E. Frise, and S. E. Celniker (2013). Spatial expression of transcription factors in Drosophila embryonic organ development. Genome Biology, 14(12), R140.\nH. Liu and B. Yu (2013). Asymptotic properties of Lasso+mLS and Lasso+Ridge in sparse high-dimensional linear regression. Electron. J. Statist., 7, 312-3169.\nJ. Mairal and B. Yu (2013). Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows. Journal of Machine Learning Research, 14, 2449-2485.\nY. Wang, X. Jiang, B. Yu, M. Jiang (2013). A Hierarchical Bayesian Approach for Aerosol Retrieval Using MISR Data. J. American Statistical Association, 108, 483-493.\nY. He, J. Jia and B. Yu (2013). Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs. Annals of Statistics, 41(4), 1742-1779.\nB. Yu (2013). Stability. Bernoulli, 19 (4), 1484-1500. (Invited paper for the Special Issue commemorating the 300th anniversary of the publication of Jakob Bernoullis Ars Conjectandi in 1712) (Begining of PCS)\nJ. Mairal and B. Yu (2013). Discussion on Grouping Strategies and Thresholding for High Dimensional Linear Models Journal of Statistical Planning and Inference, 143, 1451-1453.\nC. Uhler, G. Raskutti, and P. Buhlmann and B. Yu (2013). Geometry of faithfulness assumption in causal inference. Annals of Statistics, 41, 436-463.\nL. Miratrix, J. Sehkon, and B. Yu (2013). Adjusting Treatment Effect Estimates by Post-Stratification in Randomized Experiments. Journal of Royal Statistical Society, Series B, 75 (part 2), 369-396.\nJ. Jia, K. Rohe and B. Yu (2013) The Lasso under Poisson-like Heteroscadecity. Statistica Sinica, 23, 99-118.\nS. Negahban, P. Ravikumar, M. Wainwrigt, and B. Yu (2012) A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. Statistical Science, 27, 538-557.\nG. Raskutti, M. Wainwrigt, and B. Yu (2012) Minimax-optimal rates for sparse additive models over kernel classes via convex programming. J. Machine Learning Research, 13, 389-427.\nJ. Mairal and B. Yu (2012). Complexity analysis of the Lasso regularization path. Proc. of International Conference on Machine Learning (ICML)."
  },
  {
    "objectID": "papers.html#section-12",
    "href": "papers.html#section-12",
    "title": "Papers",
    "section": "",
    "text": "Yanfeng Gu, Shizhe Wang, Tao Shi, Yinghui Lu, Eugene E. Clothiaux, and Bin Yu (2012). Multiple-kernel learning-based unmixing algorithm for estimation of cloud fractions with MODIS and CLOUDSAT data. Proc. of IEEE International Geoscience and Remote Sensing Symposium (IGRSS)."
  },
  {
    "objectID": "papers.html#section-13",
    "href": "papers.html#section-13",
    "title": "Papers",
    "section": "",
    "text": "S. Nishimoto, A. T. Vu, T. Naselaris, Y. Benjamini, B. Yu, J. L. Gallant (2011). Reconstructing visual experiences from brain activity evoked by natural movies. Current Biology, 21(19), 1641-1646. related videos\nP. Ravikumar, M. Wainwright, G. Raskutti, B. Yu (2011). High-dimensional covariance estimation by minimizing l1-penalized log-determinant divergence. Electronic Journal of Statistics, 5, 935-980.\nG. Raskutti, M. Wainwright, B. Yu (2011). Minimax rates of estimation for high-dimensional linear regression over lq-balls. IEEE Trans. Inform. Th., 57(10), 6976-6994.\nK. Rohe, S. Chatterjee, and B. Yu (2011). Spectral clustering and the high-dimensional Stochastic Block Model. Annals of Statistics, 39 (4), 1878-1915\nV. Q. Vu, P. Ravikumar, T. Naselaris, K. N. Kay, J. L. Gallant, B. Yu* (2011). Encoding and decoding V1 fMRI responses to natural images with sparse nonparametric models. Annals of Applied Statistics, 5, 1150-1182. (*First senior author as last author in biology tradition)\nS. N. Pakzad, G. Rocha, and B. Yu (2011). Distributed modal identification by regularized auto regressive models. International Journal of Systems Science, 42, 1473-1489.\nJ. Yousafzai, P. Sollich, Z. Cvetkovic, and B. Yu (2011). Combined Features and Kernel Design for Robust Phoneme Classification Using Support Vector Machines. IEEE Trans. Audio, Speech and Language Processing (to appear). 64.\nX. Dai, J. Jia, B. Yu, El Ghaoui (2011) SBA-term: Sparse Bilingual Association for terms. Proc. International Conference on Semantic Computing.\nB. Yu (2011). Asymptotics and Coding Theory: One of the n - 1 Dimensions of Terry. In Selected Works of Terry Speed (ed.¬†S. Duoit), pp.¬†33-36, Springer."
  },
  {
    "objectID": "papers.html#section-14",
    "href": "papers.html#section-14",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (2010). Remembering Leo. Annals of Applied Statistics, 4(4), 1657-1659.\nJ. Jia, Y. Benjamini, C. Lim, G. Raskutti, B. Yu (2010). Comment on ‚ÄúEnvelope models for parsimonious and efficient multivariate linear regression‚Äù by R. D. Cook, B. Li, and F. Chiaromonte. Statistica Sinica, 20, 961-967.\nG. Raskutti, M. Wainwrigt, and B. Yu (2010) Restricted Eigenvalue Properties for Correlated Gaussian Designs. Journal of Machine Learning Research, 11, 2241-2259.\nJ. Jia and B. Yu (2010). On model selection consistency of elastic net when p &gt;&gt;n. Statistica Sinica, 10, 595-611.\nP. Buhlmann and B. Yu (2010). Boosting. Wiley Interdisciplinary Reviews: Computational Statistics, 2, 69-74.\nL. Huang, J. Jia, B. Yu, B. Chun, P. Maniatis, M. Naik (2010). Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression. Proc. NIPS 2010.\nY. Han, F. Wu, J. Jia, Y. Zhuang and B. Yu (2010). Multi-task Sparse Discriminant Analysis (MtSDA) with Overlapping Categories. Proc. of The 24th AAAI Conference on Artificial Intelligence, July 11-15, Atlanta, GA.\nB. Gawalt, J. Jia, L. Miratrix, L. El Ghaoui, B. Yu, and S. Clavier (2010). Discovering Word Associations in News Media via Feature Selection and Sparse Classification. Proc. 11th ACM SIGMM International Confernece on Multimedia Information Retrieval (MIR)."
  },
  {
    "objectID": "papers.html#section-15",
    "href": "papers.html#section-15",
    "title": "Papers",
    "section": "",
    "text": "E. Anderes, B. Yu, V. Jovanovic, C. Moroney, M. Garay, A. Braverman, E. Clothiaux (2009) Maximum Likelihood Estimation of Cloud Height from Multi-Angle Satellite Imagery. Annals of Applied Statistics, 3, 902-921\nT. Shi, M. Belkin, and B. Yu, (2009) Data Spectroscopy: Eigenspace of Convolution Operator and Clustering Annals of Statistics, 37 (6B), 3960-3984.\nVincent Q. Vu, Bin Yu, Robert E. Kass (2009) Information In The Non-Stationary Case Neural computation 21, 688-703.\nN. Meinshausen and B. Yu (2009). Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics 37, 246-270.\nP. Zhao, G. Rocha, and B. Yu (2009). The composite absolute penalties family for grouped and hierarchical variable selection Annals of Statistics 37, 3468-3497. (An earlier version ‚Äòappeared as Grouped and hierarchical model selection through composite absolute penalties‚Äô by P. Zhao, G. Rocha and B. Yu, Department of Statistics, UC Berkeley, Tech. Rep 703.)\nS. Negahban, P. Ravikumar, M. Wainwright, and B. Yu (2009). A unified framework for high-dimensional analysis of \\(M\\)-estimators with decomposable regularizers Proc. NIPS, 2009. (This conference paper contains preliminary results of the journal submission Negahban et al.¬†2012).\nG. Raskutti, M. Wainwright, B. Yu (2009) High-dimensional regression under lq-ball sparsity: Optimal rates of convergence. Proc. of Allerton Conference on Communication, Control, and Computing. (This conference paper contains some of preliminary results of the journal submission Ravikumar et al.¬†2011).\nG. Raskutti, M. Wainwrigt, and B. Yu (2009) Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness. Proc. NIPS, 2009. (This conference paper contains some of preliminary results of the journal submission Ravikumar et al.¬†2011)."
  },
  {
    "objectID": "papers.html#section-16",
    "href": "papers.html#section-16",
    "title": "Papers",
    "section": "",
    "text": "T. Shi, B. Yu, E. Clothiaux, and A. Braverman (2008). Daytime Arctic Cloud Detection based on Multi-angle Satellite Data with Case Studies. Journal of American Statistical Association. 103( 482), 584-593.\nPeter Buhlmann and Bin Yu (2008) Invited discussion on ‚ÄúEvidence contrary to the statistical view of boosting (D. Mease and A. Wyner)‚Äù. (paper with discussion) Journal of Machine Learning Research 9, 187-194.\nP. Ravikumer, V. Vu, B. Yu, T. Naselaris, K. Kay, J. Gallant (2008). Nonparametric sparse hiearchical models describe V1 fMRI responses to natural images In Adavances in Neural Information Processing Systems (NIPS) 21, (2008). (This conference paper contains some preliminary results of journal paper Vu et al.¬†(2011) on encoding models, but also contains an encoding model that is not in Vu et al.¬†(2011). It does not contain decoding results.)\nP. Ravikumar, G. Raskutti, M. Wainwright, B. Yu (2008) Model selection in Gaussian graphical models: high-dimensional consistency of l1-regularized MLE. In Adavances in Neural Information Processing Systems (NIPS) 21, (2008).\nT. Shi, M. Belkin, and B. Yu (2008). Data spectroscopy: learning mixture models using eigenspaces of convolution operators. Proc. of ICML 2008.\nM. Ager, Z. Cvetkovic, P. Pollich, and B. Yu (2008). Towards Robust Phoneme Classification Augmentation of PLP Models with Acoustic Waveforms. Proceedings of EUSIPCO.\nJ. Yousafzai, Z. Cvetkoviƒá, P. Pollich, and B. Yu (2008). Combined PLP-Acoustic Waveform Classification for Robust Phoneme Recognition using Support Vector Machines. Proceedings of EUSIPCO."
  },
  {
    "objectID": "papers.html#section-17",
    "href": "papers.html#section-17",
    "title": "Papers",
    "section": "",
    "text": "N. Meinshausen, G. Rocha, and B. Yu (2007). A tale of three cousins: Lasso, L2Boosting, and Danzig Annals of Statistics (invited discussion on Candes and Tao‚Äôs Danzig Selector paper)\nV. Vu, B. Yu, and R. Kass (2007). Coverage Adjusted Entropy Estimation. Statistics and Medicine, 26(21), 4039-4060.\nB. Yu (2007). Embracing Statistical Challenges in the Information Technology Age Technometrics (special issue on statistics and information technologies). vol.¬†49 (3), 237-248.\nX. Jiang, Y. Liu, B. Yu and M. Jiang (2007). Comparison of MISR aerosol optical thickness with AERONET measurements in Beijing metropolitan area. Remote Sensing of Environment (Special Issue on Multi-angle Imaging SpectroRadiometer), vol.¬†107, pp.¬†45-53.\nT. Shi, E. E. Clothiaux, B. Yu, A. J. Braverman, and G. N. Groff (2007). Detection of Daytime Arctic Clouds using MISR and MODIS Data. Remote Sensing of Environment (Special Issue on Multi-angle Imaging SpectroRadiometer), vol.¬†107, pp.¬†172-184."
  },
  {
    "objectID": "papers.html#section-18",
    "href": "papers.html#section-18",
    "title": "Papers",
    "section": "",
    "text": "Peng Zhao and Bin Yu (2006). On Model Selection Consistency of Lasso. J. Machine Learning Research, 7 (nov), 2541-2567.\nB. Yu (2006). Comments on: Monitoring networked applications with incremental quantile estimation by Chambers et al. Statist. Sci., 21, 483-485.\nB. Yu (2006). Comments on: Regularization in Statistics, by P. J. Bickel and B. Li. Test, vol.¬†15 (2), pages 314-316.\nP. Buhlmann and B. Yu (2006). Sparse Boosing Journal of Machine Learning Research ( 7 (June), 1001-1024). This is a shortened and more focused version of Buhlmann and Yu ‚ÄúBoosting, Model Selection, Lasso and Nonnegative Garotte‚Äù given below.\nJ. Gao, H. Suzuki, and B. Yu (2006). Approximation Lasso Methods for Language Modeling. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp.¬†225-232, Sydney."
  },
  {
    "objectID": "papers.html#section-19",
    "href": "papers.html#section-19",
    "title": "Papers",
    "section": "",
    "text": "T. Shi and B. Yu (2005). Binning in Gaussian Kernel Regularization. Statistica Sinica (special issue on machine learning), 16, 541-567.\nG. Liang, N. Taft, and B. Yu (2005). A fast lightweight approach to origin-destination IP traffic estimation using partial measurements. Tech Report 687, Statistics Department, UCB (accepted for Special Issue of IEEE-IT and ACM Networks on data networks, Jan.¬†2006)\nTong Zhang and B. Yu (2005). Boosting with early stopping: convergence and consistency. The Annals of Statistics. Vol. 33, 1538-1579.\nCastro, M. Coates, G. Liang, R. Nowak, and B. Yu (2005) Network tomography: recent developments. Statistical Science, 19, 499-517.\nC. D. Giurcaneanu and B. Yu (2005). Efficient algorithms for discrete universal denoising for channels with memeory. Proceedings of International Symposium on Information Theory, Australia. (Also as Tech. Report 686, Statistics Department, UCB (Proc. ISIT, Sept.¬†2005))"
  },
  {
    "objectID": "papers.html#section-20",
    "href": "papers.html#section-20",
    "title": "Papers",
    "section": "",
    "text": "P. Zhao and B. Yu (2004). Stagewise Lasso (old title: Boosted Lasso) Journal of Machine Learning Research, 8, 2701-2726. (An earlier version appeared as Tech. Report #678, Statistics Department, UC Berkeley (December, 2004; revised in April, 2005)\nD. J. Diner et al (2004). PARAGON: A Systematic, Integrated Approach to Aerosol Observation and Modeling. American Meterological Society, Oct., 1491-1501.\nP. Buhlmann and B. Yu (2004). Discussion on three boosting papers by Jiang, Lugosi and Vayatis, and Zhang Annals of Statistics. 32 (1): 96-101.\nR. Jorsten and B. Yu (2004). Compressing genomic and proteomic array images for statistical analyses. Invited chapter in a book on Genomic signal processing and statistics, edited by E. R. Dougherty, I. Shmulevich, J. Chen, and Z. J. Wang, pp.¬†341 - 366.\nG. Liang, B. Yu, and N. Taft (2004). Maximum entropy models: convergence rates and application in dynamic system monitoring. International Symposium on Information Theory, Chicago."
  },
  {
    "objectID": "papers.html#section-21",
    "href": "papers.html#section-21",
    "title": "Papers",
    "section": "",
    "text": "R. Castro, M. Coates, G. Liang, R. Nowak, and B. Yu (2003). Internet Tomography: Recent Developments Statistical Science. Vol. 19(3), 499-517.\nG. Liang and B. Yu (2003). Maximum Pseudo Likelihood Estimation in Network Tomography. IEEE Trans. on Signal Processing (Special Issue on Data Networks). 51(8), 2043-2053\nRebecka Jornsten and Bin Yu (2003). Simultaneous Gene Clustering and Subset Selection for Classification via MDL. Bioinformatics. 19(9): 1100-1109.\nPeter Buhlmann and Bin Yu (2003). Boosting with the L2 Loss: Regression and Classification. J. Amer. Statist. Assoc. 98, 324-340.\nR. Jornsten, W. Wang, B. Yu, and K. Ramchandran (2003). Microarray image compression: SLOCO and the effects of information loss. Signal Processing Journal (Special Issue on Genomic Signal Processing). 83, 859-869.\nG. Liang and B. Yu (2003). Pseudo Likelihood Estimation in Network Tomography. Proceedings of of Infocom, San Francisco."
  },
  {
    "objectID": "papers.html#section-22",
    "href": "papers.html#section-22",
    "title": "Papers",
    "section": "",
    "text": "Peter Buhlmann and Bin Yu (2002). Analyzing Bagging. Annals of Statistics vol.¬†30, 927-961.\nR. Jornsten, M. Hansen, and B. Yu (2002). Adaptive Minimum Description Length (MDL) criteria with applications to microarray data. In Advances in Minimum Description Length: Theory and Applications, edited by P. Grunwald, I.J. Myung and M.A.¬†Pitt. The MIT Press, pp.¬†295-321.\nMark Hansen and Bin Yu (2002). Minimum Description Length Model Selection Criteria for Generalized Linear Models.{}, IMS Lecture Notes ‚Äì Monograph Series, Vol. 40.\nRebecka Jornsten, and Bin Yu (2002). Multiterminal Estimation: Extensions and a Geometric interpretation. Proceedings of International Symposium on Information Theory (ISIT), June, 2002.\nGerald Schuller, Bin Yu, Dawei Huang, and Bern Edler (2002). Perceptual Audio Coding using Pre- and Poster- Filters and Lossless Compression. IEEE Trans. Speech and Audio Processing. Vol. 10 (6), 379-390\nMark Coates, Alfred Hero, Robert Nowak, and Bin Yu (2002). Internet Tomography. Signal Processing Magazine. vol.¬†19, No.¬†3 (May issue), 47-65."
  },
  {
    "objectID": "papers.html#section-23",
    "href": "papers.html#section-23",
    "title": "Papers",
    "section": "",
    "text": "M. Hansen and B. Yu (2001). Model selection and the principle of Minimum Description Length. Journal of American Statistical Association. 96, 746-774."
  },
  {
    "objectID": "papers.html#section-24",
    "href": "papers.html#section-24",
    "title": "Papers",
    "section": "",
    "text": "Jin Cao, Drew Davis, Scott Vander Wiel and Bin Yu (2000). [PDF| Time-varying network tomography: router link data. J. Amer. Statist. Assoc. vol.¬†95, 1063-1075.\nPeter Buhlmann and Bin Yu (2000). Discussion. Additive logistic regression: a statistical view of boosting, by Friedman, J., Hastie, T. and Tibshirani, R. Annals of Statistics. Vol. 28, 377-386\nMark Hansen and Bin Yu (2000). Wavelet thresholding via MDL for natural images. IEEE Trans. Inform. Theory (Special Issue on Information Theoretic Imaging). vol.¬†46, 1778-1788.\nJorma Rissanen and Bin Yu (2000). Coding and compression: a happy union of theory and practice. J. Amer. Statist. Assoc. (Year 2000 Commemorative Vignette on Engineering and Physical Sciences). vol.¬†95, 986-988.\nLei Li and Bin Yu (2000). Iterated logarithm expansions of the pathwise code lengths for exponential families. IEEE Trans. Inform. Theory. vol.¬†46, 2683-2689.\nG. Chang, B. Yu and M. Vetterli (2000). Adaptive wavelet thresholding for image denoising and compression. IEEE Trans. Image Processing, vol.¬†9, 1532-1546.\nG. Chang, B. Yu and M. Vetterli (2000). Spatially adaptive wavelet thresholding based on context modeling for image denoising. IEEE Trans. Image Processing, vol.¬†9, 1522-1531.\nG. Chang, B. Yu and M. Vetterli (2000). Wavelet thresholding for multiple noisy image copies. IEEE Trans. Image Processing, vol.¬†9, 1631-1635."
  },
  {
    "objectID": "papers.html#section-25",
    "href": "papers.html#section-25",
    "title": "Papers",
    "section": "",
    "text": "Y. Yoo, A. Ortega, and B. Yu (1999). Image subband coding using context-based classification and adaptive quantization. IEEE Trans. Image Processing, vol.¬†8, 1702-1215.\nB. Yu, M. Ostland, P. Gong and R. Pu (1999). Penalized discriminant analysis of in situ hyperspectral data for conifer species recognition. IEEE Trans. Geoscience and Remote Sensing, in press."
  },
  {
    "objectID": "papers.html#section-26",
    "href": "papers.html#section-26",
    "title": "Papers",
    "section": "",
    "text": "A. Barron, J. Rissanen, and B. Yu (1998). The Minimum Description Length principle in coding and modeling. (Special Commemorative Issue: Information Theory: 1948-1998) IEEE. Trans. Inform. Th., 44, 2743-2760. Reprinted in Information 50 Years of Discovery, Theory: S. Verdu and S. McLaughlin (eds), IEEE Press , 1999.\nB. Yu and P. Mykland (1998). Looking at Markov samplers through cusum path plots: a simple diagnostic idea. Statistics and Computing , 8, 275-286.\nP. Gong, R. Pu and B. Yu (1998) Conifer species recognition: effects of data transformation and band width (in Chinese) Journal of Remote Sensing, 2(3), 211-217.\nG. Chang, B. Yu and M. Vetterli (1998). Spatially adaptive wavelet thresholding for image denoising. Proceedings of IEEE International Conference on Image Processing, October, Chicago.\nS. G. Chang, B. Yu, and M. Vetterli (1998). Image denoising via lossy compression and wavelet thresholding. Proceedings of International Conference on Image Processing. Santa Barbara, California, vol.¬†1, pp.¬†604-607.\nM. Ostland and B. Yu (1997). Exploring quasi Monte Carlo for marginal density approximation. Statistics and Computing, 7, 217-228.\nP. Gong, R. Pu, and B. Yu (1997). Conifer species recognition with in Situ hyperspectral data. Remote Sensing of Environment, 62, 189-200.\nB. Yu and T. P. Speed (1997). Information and the clone mapping of chromosomes. Ann. Statist. 25, 169-185."
  },
  {
    "objectID": "papers.html#section-27",
    "href": "papers.html#section-27",
    "title": "Papers",
    "section": "",
    "text": "D. Nelson, T. Speed, and B. Yu (1997). The limits of random fingerprinting. Genomics, 40, 1-12.\nB. Yu (1997). Assouad, Fano, and Le Cam. Festschrift for Lucien Le Cam . D. Pollard, E. Torgersen, and G. Yang (eds), pp.¬†423-435, Springer-Verlag."
  },
  {
    "objectID": "papers.html#section-28",
    "href": "papers.html#section-28",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (1996). Lower bounds on expected redundancy for nonparametric classes. IEEE Trans. on Information Theory, 42, 272-275.\nY. Yoo, A. Ortega, and B. Yu (1996). Adaptive quantization of image subbands with efficient overhead rate selection. In Proceedings of IEEE International Conference on Image Processing, Lausanne, Switzerland.\nB. Yu (1996). A Statistical analysis of adaptive scalar quantization based on quantized past data. In Proceedings of International Symposium on Information Theory and its Applications (ISITA96), Victoria, Canada."
  },
  {
    "objectID": "papers.html#section-29",
    "href": "papers.html#section-29",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (1995). Comment: Extracting more diagnostic information from a single run using cusum path plot. Statist. Sci., 10, 54-58.\nJ. Rissanen and B. Yu (1995). MDL learning. In Learning and Geometry: Computational Approaches, Progress in Computer Science and Applied Logic, 14, David Kueker and Carl Smith (eds), Birkh√§user, Boston, pp.¬†3-19.\nP. Mykland, L. Tierney, and B. Yu (1995). Regeneration in Markov Chain samplers. J. Amer. Statist. Assoc., 90, 233-241."
  },
  {
    "objectID": "papers.html#section-30",
    "href": "papers.html#section-30",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (1994). Rates of convergence for empirical processes of stationary mixing sequences. Ann. Probab. 22, 94-116.\nM. Arcones and B. Yu (1994). Central limit theorems for empirical and U-processes of stationary mixing sequences. J. Theor. Probab. 7, 47-71.\nB. Yu (1994). Lower bound on the expected redundancy for classes of continuous Markov sources. In Statistical Decision Theory and Related Topics V, S. S. Gupta and J. O. Berger (eds), 453-466.\nM. Arcones and B. Yu (1994). Limit theorems for empirical processes under dependence. In Proceedings in Chaos expansions, multiple Wiener integrals and their applications. 205-221.\nA. R. Barron, Y. Yang and B. Yu (1994). Asymptotically optimal function estimation by minimum complexity criteria. In Proceedings of 1994 International Symposium on Information Theory, pp.¬†38, Trondheim, Norway."
  },
  {
    "objectID": "papers.html#section-31",
    "href": "papers.html#section-31",
    "title": "Papers",
    "section": "",
    "text": "B. Yu and T. Speed (1993). A rate of convergence result for a universal D-semifaithful code. IEEE Trans. on Information Theory 39, 8813-820.\nB. Yu (1993). Density estimation in the L‚àû norm for dependent data with applications to the Gibbs sampler. Ann. Statist. 21, 711-735.\nT. Speed and B. Yu (1993). Model selection and prediction: normal regression. J. Inst. Statist. Math. 45, 35-54."
  },
  {
    "objectID": "papers.html#section-32",
    "href": "papers.html#section-32",
    "title": "Papers",
    "section": "",
    "text": "J. Rissanen, T. Speed and B. Yu (1992). Density estimation by stochastic complexity. IEEE Trans. on Information Theory, 38, 315-323.\nB. Yu and T. Speed (1992) Data compression and histograms. Probability Theory and Related Fields, 92, 195-229."
  },
  {
    "objectID": "papers/veridical-data-science-pcs.html",
    "href": "papers/veridical-data-science-pcs.html",
    "title": "Veridical Data Science (PCS)",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/veridical-data-science-pcs.html#selected-recent-papers-on-veridical-data-science-pcs",
    "href": "papers/veridical-data-science-pcs.html#selected-recent-papers-on-veridical-data-science-pcs",
    "title": "Veridical Data Science (PCS)",
    "section": "Selected Recent Papers on Veridical Data Science (PCS)",
    "text": "Selected Recent Papers on Veridical Data Science (PCS)\n\nB. Yu (2024). After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI (discussion of Donoho‚Äôs paper ‚ÄúData Science at the Singularity‚Äù) Harvard Data Science Review (HDSR).\nB. Yu (2023). What is uncertainty in today‚Äôs practice of data science? J. Econometrics. 237, 105519.\nQ. Wang, T. M. Tang, N. Youlton, C. S. Weldy, A. M. Kenney, O. Ronen, J. W. Hughes, E. T. Chin, S. C. Sutton. A. Agarwal, X. Li, M. Behr, K. Kumbier, C. S. Moravec, W. H. W. Tang, K. B. Margulies, T. P. Cappola, A. J. Buitte, R. Arnaout, J. B. Brown, J. R. Priest, V. N. Parikh, B. Yu, E. Ashley (2023). Epistasis regulates genetic control of cardiac hypertrophy. https://www.medrxiv.org/content/10.1101/2023.11.06.23297858v1 (Code) (PCS documentation)\nR. Cahill, Y. Wang, R. P. Xian, A. J. Lee, H. Zeng, B. Yu, B. Tasic, R. Abbasi-Asl (2023). Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology. https://www.biorxiv.org/content/10.1101/2023.03.10.531984v2 (Code)\nA. Agarwal, A. M. Kenny, Y. S. Tan, T. M. Tang, B. Yu (2023). MDI+: a flexible random forest-based feature importance framework. https://arxiv.org/abs/2307.01932 (PCS related)\nM. Behr, K. Kumbier, A. Cordova-Palomera, M. Aguirre, E. Ashley, A. Butte, R. Arnaout, J. B. Brown, J. Preist, B. Yu (2020). Learning epistatic polygenic phenotypes with Boolean interactions https://www.biorxiv.org/content/10.1101/2020.11.24.396846v1 (code) (PCS inference case study)\nB. Norgeot, G. Quer, B. K. Beaulieu-Jones, A. Torkamani, R. Dias, M. Gianfrancesco, R. Arnaout, I. S. Kohane, S. Saria, E. Topol, Z. Obermeyer, B. Yu & A. Butte (2020). Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist, Nature Medicine, 26, 1320‚Äì1324.\nB. Yu (2020). Stability expanded, in reality. Harvard Data Science Review (HDSR). (PCS related)\nB. Yu and R. Barter (2020). Data science process: one culture. JASA. (PCS related)\nR. Dwivedi, Y. Tan, B. Park, M. Wei, K. Horgan, D. Madigan, B. Yu (2020). Stable discovery of interpretable subgroups via calibration in causal studies (staDISC). International Statistical Review and also at arxiv.org/abs/2008.10109 (code) (PCS case study for causal inference)\nX. Li, T. M. Tang, X. Wang, J. A. Kocher, B. Yu (2020). A stability-driven protocol for drug response interpretable prediction (staDRIP). NeurISP workshop on ML4H (Machine learning for Health) Extended Abstract. drive link\nB. Yu and K. Kumbier (2020) Veridical data science (PCS framework), PNAS. 117 (8), 3920-3929. QnAs with Bin Yu.\nY. Chen, R. Abbasi-Asl, A. Bloniarz, M. Oliver, B. Willmore, J. Gallant, and B. Yu (2018) The DeepTune framework for modeling and characterizing neurons in visual cortex area V4 https://www.biorxiv.org/content/10.1101/465534v1\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. https://arxiv.org/abs/1810.0728 (an enhanced version of iRF, PCS related)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (code) (PCS related)\nSiqi Wu, Antony Joseph, Ann S. Hammonds, Susan E. Celniker, Bin Yu, and Erwin Frise (2016). Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks (with support information). PNAS, pp.¬†4290 - 4295. (code) (PCS related)"
  },
  {
    "objectID": "papers/interpretable-machine-learning.html",
    "href": "papers/interpretable-machine-learning.html",
    "title": "Interpretable Machine Learning",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/interpretable-machine-learning.html#selected-recent-papers-on-interpretable-machine-learning",
    "href": "papers/interpretable-machine-learning.html#selected-recent-papers-on-interpretable-machine-learning",
    "title": "Interpretable Machine Learning",
    "section": "Selected Recent Papers on Interpretable Machine Learning",
    "text": "Selected Recent Papers on Interpretable Machine Learning\n\nA. R. Hsu, Y. Cherapanamjeri, A. Y. Odisho, P. R. Carroll, B. Yu (2024). Mechanistic Interpretation through Contexual Decomposition in Transformers. https://arxiv.org/pdf/2407.00886.\nY. Chen, C. Singh, X. Liu, S. Zuo, B. Yu, H. He, J. Gao (2024). Towards consistent natural-language explanations via explanation-consistent finetuning. https://arxiv.org/abs/2401.13986\nQ. Zhang, C. Singh, L. Liu, X. Liu, B. Yu, J. Gao, T. Zhao (2023). Tell your model where to attend: post-hoc attention steering for LLMs. ICLR 2024. https://arxiv.org/abs/2311.02262\nA. Agarwal, A. M. Kenny, Y. S. Tan, T. M. Tang, B. Yu (2023). MDI+: a flexible random forest-based feature importance framework. https://arxiv.org/abs/2307.01932 (PCS related)\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. ICLR (2024) https://arxiv.org/abs/2305.17588\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu and J. Gao (2023). Explaining black box text modules in natural language with language models.\nC. Singh, W. Ha and B. Yu (2021). Interpreting and Improving Deep-Learning Models with Reality Checks. https://arxiv.org/abs/2108.06847 to appear in the book entitled ‚ÄúxxAI - Beyond Explainable AI‚Äù (eds.¬†Andreas Holzinger, Randy Goebel, Ruth Fong, Taesup Moon, Klaus-Robert M√ºller, and Wojciech Samek).\nW. Ha, C. Singh, F. Lanusse, S. Upadhyayula, and B. Yu (2021). Adaptive Wavelet Distillation from Neural Networks through Interpretation. Proc. NeurIPS 2021. (code)\nL. Reiger, J. W. Murdoch, S. Singh, B. Yu (2020). Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge. ICML Proceedings. (code)\nC. Singh, W. Ha, F. Lanusse, V. Boehm , J. Liu, B. Yu (2020). Transformation Importance with Applications to Cosmology ICLR Workshop paper. (code)\nW. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu* (2019) Definitions, methods, and applications in interpretable machine learning. PNAS, 116 (44) 22071-22080.\nW. J. Murdoch, C. Sign, and B. Yu (2019). Hierarchical interpretations for neural network predictions. ICLR. (code)\nJ. Murdoch, P. Liu, and B. Yu (2018) Beyond word importance: contextual decomposition to extract interactions from LSTMs. Proc. ICLR 2018. https://arxiv.org/abs/1705.07356 (code)"
  },
  {
    "objectID": "papers/interdisciplinary-research-in-biomedicine.html",
    "href": "papers/interdisciplinary-research-in-biomedicine.html",
    "title": "Interdisciplinary Research in Biomedicine",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/interdisciplinary-research-in-biomedicine.html#selected-recent-papers-in-biomedicine",
    "href": "papers/interdisciplinary-research-in-biomedicine.html#selected-recent-papers-in-biomedicine",
    "title": "Interdisciplinary Research in Biomedicine",
    "section": "Selected Recent Papers in Biomedicine",
    "text": "Selected Recent Papers in Biomedicine\n\nQ. Wang,* T. M. Tang, N. Youlton, C. S. Weldy, A. M. Kenney, O. Ronen, J. W. Hughes, E. T. Chin, S. C. Sutton. A. Agarwal, X. Li, M. Behr, K. Kumbier, C. S. Moravec, W. H. W. Tang, K. B. Margulies, T. P. Cappola, A. J. Buitte, R. Arnaout, J. B. Brown, J. R. Priest, V. N. Parikh, B. Yu, E. Ashley* (2023). Epistasis regulates genetic control of cardiac hypertrophy. https://www.medrxiv.org/content/10.1101/2023.11.06.23297858v1 (Code) (PCS documentation)\nR. Cahill, Y. Wang, R. P. Xian, A. J. Lee, H. Zeng, B. Yu, B. Tasic, R. Abbasi-Asl (2023). Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology. https://www.biorxiv.org/content/10.1101/2023.03.10.531984v2 (Code)\nE. Irajizad, A. Kenney, T. Tang, J. Vykoukal, R. Wu, E. Murage, J. B. Dennison, M. Sans, J. P. Long, M. Loftus, J. A. Chabot, M. D. Kluger, F. Kastrinos, L. Brais, A. Babic, K. Jajoo, L. S. Lee, T. E. Clancy, K. Ng, A. Bullock, J. M. Genkinger, A. Maitra, K. A. Do, B. Yu, B. W. Wolpin, S. Hanash, J. F. Fahrmann. (2023). A blood-based metabolomic signature predictive of risk for pancreatic cancer. Cell Reports Medicine 4(9): 101194. doi: 10.1016/j.xcrm.2023.101194. (PCS related) (Editorial in Cell Reports Medicine by L. Oldfield and E. Costello on Erajizad et al.¬†(2023))\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. https://arxiv.org/abs/2305.17588\nN. Altieri, B. Park, J. DeNero, A. Odisho, B. Yu. (2021). Improving natural language information extraction from cancer pathology reports using transfer learning and zero-shot string similarity. JAMIA Open. 2021 Sept.¬†30 4(3).\nB. Norgeot, G. Quer, B. K. Beaulieu-Jones, A. Torkamani, R. Dias, M. Gianfrancesco, R. Arnaout, I. S. Kohane, S. Saria, E. Topol, Z. Obermeyer, B. Yu & A. Butte (2020). Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist, Nature Medicine, 26, 1320‚Äì1324."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "Software\nAlso see our group‚Äôs public repos on GitHub.\n\nVeridicalFlow: Library for building stable, trustworthy data-science pipelines, PCS framework, paper, docs\nsimChef: R package to facilitate PCS simulation studies, data science workflows\nimodels: Python package for interpretable machine learning models, FIGS, hierarchical shrinkage, MDI+, FIGS paper, hierarchical shrinkage paper, MDI+ paper\niRF: Iterative Random Forests, stable high-order interactions, PCS-guided, paper\nAdaptive wavelets: Adaptive interpretable wavelets and wavelet distillation, domain-agnostic, paper\nCOVID-19 severity prediction: Extensive COVID-19 data and forecasting for counties and hospitals, pandemic severity index, paper, website\nEpistasis cardiac hypertrophy: Code for epistasis in genetic control of cardiac hypertrophy, PCS-guided, preprint, PCS docs\nMolecular partner prediction: Predicting successful CME events using clathrin markers, cell biology\nsMPS2: Simplified MyProstateScore2.0 for high-grade prostate cancer detection, clinical decision rules\nMS-analysis-NMF: Non-negative matrix factorization for deconvolving complex mass spectra, simulation study\nVDS book supplementary: Supplementary materials for Veridical Data Science book, educational resources\nimodels-experiments: Experimental interpretable models to accompany imodels package, rule-based models\nYu Group website: Source code for the Yu Group website, web development"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "Abhineet Agarwal (Ph.D.¬†Student, Statistics)\nSoufiane Hayou (Post-doc)\nJakob Heiss (Post-doc)\nAliyah Hsu (Ph.D.¬†Student, Statistics)\nYaxuan Huang (Ph.D.¬†Student, Statistics)\nNathan Benjamin McNaughton (Undergraduate Student)\nRobin Netzorg (Ph.D.¬†Student, Statistics)\nAnthony Ozerov (Ph.D.¬†Student, Statistics)\nZachary Rewolinski (Ph.D.¬†Student, Statistics)\nOmer Ronen (Ph.D.¬†Student, Statistics)\nJingfeng Wu (Post-doc)\nChengzhong Ye (Ph.D.¬†Student, Statistics)\nZeyu Yun (Ph.D.¬†Student, Statistics)\nAustin Zane (Ph.D.¬†Student, Statistics)"
  },
  {
    "objectID": "people.html#current-members-role-program",
    "href": "people.html#current-members-role-program",
    "title": "People",
    "section": "",
    "text": "Abhineet Agarwal (Ph.D.¬†Student, Statistics)\nSoufiane Hayou (Post-doc)\nJakob Heiss (Post-doc)\nAliyah Hsu (Ph.D.¬†Student, Statistics)\nYaxuan Huang (Ph.D.¬†Student, Statistics)\nNathan Benjamin McNaughton (Undergraduate Student)\nRobin Netzorg (Ph.D.¬†Student, Statistics)\nAnthony Ozerov (Ph.D.¬†Student, Statistics)\nZachary Rewolinski (Ph.D.¬†Student, Statistics)\nOmer Ronen (Ph.D.¬†Student, Statistics)\nJingfeng Wu (Post-doc)\nChengzhong Ye (Ph.D.¬†Student, Statistics)\nZeyu Yun (Ph.D.¬†Student, Statistics)\nAustin Zane (Ph.D.¬†Student, Statistics)"
  },
  {
    "objectID": "people.html#former-ph.d.-students-graduation-year",
    "href": "people.html#former-ph.d.-students-graduation-year",
    "title": "People",
    "section": "Former Ph.D.¬†Students (Graduation Year)",
    "text": "Former Ph.D.¬†Students (Graduation Year)\n\nCorine Elliot (2024) - Statistical Scientist, Berry Consultants\nJames Duncan (2024) - Climate ML Engineering, Allen Institute for AI (AI2)\nTiffany Tang (2023) - Clare Boothe Luce Assistant Professor, University of Notre Dame\nYeshwanth Cherapanamjeri (2023) - Postdoctoral Researcher, MIT\nDennis Shen (2023) - Assistant Professor, USC\nSpencer Frei (2023) - Assistant Professor, UC Davis\nRebecca L. Barter (2022) - Research Assistant Professor, University of Utah\nLuiz Chamon (2022) - Research Group Leader, University of Stuttgart\nAngela Zhou (2022) - Assistant Professor, University of Southern California\nBriton Park (2022) - Citadel Securities\nChandan Singh (2022) - Microsoft Research\nKeyan Nasseri (2022) - Google\nWooseok Ha (2022) - AWS\nYan Shuo Tan (2022) - Assistant Profressor, Department of Statistics and Data Science, National University of Singapore\nXiao Li (2021) - Voleon\nRaaz Dwivedi (2021) - Postdoctoral scholar, MIT / Harvard\nYu Wang (2020) - Two Sigma\nMerle Behr (2020) - Scientific Expert, Bayer AG\nNicholas Altieri (2020) - Genentech\nYuansi Chen (2019) - Assistant Professor, Department of Statistical Sciences, Duke University\nKarl Kumbier (2019) - Postdoctoral Researcher, UC San Francisco\nJamie Murdoch (2019) - Co-founder / President, clientelligent.ai\nSoren Kunzel (2019) - Data Scientist, Citadel\nSimon Walter (2019) - Data Scientist, Bridgewater\nSujayam Saha (2018) - Data Scientist, Google\nReza Abbasi-Asl (2018) - Scientist, Allen Institute for Brain Science\nChristine Kuang (2017) - Data Scientist, Facebook\nHanzhong Liu (2016) - Assistant Professor, Department of Statistics, Tsinghua University\nSumanta Basu (2016) - Assistant Professor, Department of Statistics, Cornell University\nSivaraman Balakrishnan (2016) - Assistant Professor, Department of Statistics, Carnegie Mellon University\nAdam Bloniarz (2016) - Data Scientist, Google\nSiqi Wu (2016) - Data Scientist, Citadel\nHongwei Li (2015) - Senior Data Scientist, UBER\nAntony Joseph (2014) - Staff Data Scientist, Walmart eCommerce\nTaesup Moon (2013) - Research Staff Member, Samsung Advanced Institute of Technology\nYuval Benjamini (2013) - Stein fellow, Statistics department, Stanford University\nJulien Mairal (2012) - Research Scientist, INRIA\nYueqing Wang (2012) - Research Scientist, Google\nHau-tieng Wu (2012) - Assistant Professor, Mathematics Department, University of Toronto\nGarvesh Raskutti (2012) - Assistant Professor, Department of Statistics, Univ. Wisconsin-Madison\nLuke W. Miratrix (2012) - Assistant Professor, Department of Statistics, Harvard University\nHarry Kim (2011) - Product Marketing Manager, Google\nChinghway Lim (2011) - Assistant Professor, Department of Statistics and Applied Probability, National University of Singapore\nKarl Rohe (2011) - Assistant Professor, Department, University of Wisconsin-Madison.\nKyle Jia (2010) - Assistant Professor, Peking University, China.\nDavid Purdy (2009) - Data Scientist, Uber\nPradeep Ravikumar (2009) - Associate Professor, Department of Computer Sciences, University of Texas, Austin.\nVince Vu (2009) - Assistant Professor, Department of Statistics, The Ohio State University\nEthan Anderes (2008) - Assistant Professor, Department of Statistics, University of California, Davis\nGuilherme Rocha (2008) - Assistant Professor, Department of Statistics, Indiana University, Bloomington\nPeng Zhao (2006) - CEO of Citadel Securities, Citadel Investment Group in Chicago, IL.\nNicolai Meinshausen (2006) - Professor, Department of Statistics, ETH Zurich\nTao Shi (2005) - Quantitative Researcher, Citadel Investment Group in Chicago, IL.\nGang Liang (2004) - Quantitative Operations Associate, Bank of America\nRebecka J√É¬∂rnsten (2000) - Associate Professor, Mathematical Statistics, Chalmers University of Technology\nGrace Chang (2000) - Senior Manager of Product Development, Marketing Dept, MTN Irancell"
  },
  {
    "objectID": "people.html#former-post-docs-visitors-and-associates-starting-year",
    "href": "people.html#former-post-docs-visitors-and-associates-starting-year",
    "title": "People",
    "section": "Former Post-docs, Visitors, and Associates (Starting Year)",
    "text": "Former Post-docs, Visitors, and Associates (Starting Year)\n\nArman Sabbaghi (2022)\nYanjun Qi (2020)\nEzequiel Smucler (2020)\nLaura Reiger (2019)\nYanyan Lan (2018)\nYahong Han (2016)\nThibault Vatter\nGuoqiang Cai\nTrine Abrahamsen\nXiaoling Lu\nXiusheng Lu (2014)\nXiangyu Chang (2012)\nLing Hong (2012)\nToshiyasu Matsushima (2012)\nYanfeng Gu (2012) - Professor of Electronics Engineering, Harbin Institute of Technology\nHai Zhang (2012)\nRodolphe Jenatton (2010)\nKenichi Hayashi (2010)\nYangbo He (2010)\nFei Wu (2010)\nKei Kobayashi (2008)\nXing Wang (2007)"
  }
]