[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Course Description: Applied statistics and machine learning, focusing on answering scientific questions using data, the data science life cycle, critical thinking, reasoning, methodology, and trustworthy and reproducible computational practice. Hands-on experience in open-ended data labs, using programming languages such as R and Python. Emphasis on understanding and examining the assumptions behind standard statistical models and methods and the match between the assumptions and the scientific question. Exploratory data analysis. Model formulation, fitting, model testing and validation, interpretation, and communication of results. Methods include linear regression and generalizations, decision trees, random forests, simulation, and randomization methods.\nPrerequisites: Linear algebra, calculus, upper division probability and statistics, and familiarity with high-level programming languages. Statistics 133, 134, and 135 recommended.\nCourse Website: [STAT 215A]\n\n\n\nCourse Description: Students will be engaged in open-ended data projects for decision making to solve domain problems. It mirrors the entire data science life cycle in practice, including problem formulation, data cleaning, exploratory data analysis, statistical and machine learning modeling and computational techniques, and interpretation of results in context. It is guided by the Predictability-Computability-Stability (PCS) framework for veridical data science and emphasizes critical thinking and documenting human judgment calls and code. It coaches not only the technical but also communication and teamwork skills in order to obtain responsible and reliable data-driven conclusions for solving complex real world problems.\nPrerequisites: Linear algebra, calculus, upper division probability and statistics, and familiarity with high-level programming languages.\nCourse Website: [STAT 214]\n\n\n\nCourse Description: Theory and practice of statistical prediction. Contemporary methods as extensions of classical methods. Topics: optimal prediction rules, the curse of dimensionality, empirical risk, linear regression and classification, basis expansions, regularization, splines, the bootstrap, model selection, classification and regression trees, boosting, support vector machines. Computational efficiency versus predictive performance. Emphasis on experience with real data and assessing statistical assumptions. This course uses Python as its primary computing language; details are determined by the instructor.\nThis is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear regression, model assessment, model selection, regularization methods (PCR, PLSR, ridge and lasso); logistic regression and discriminant analysis; cross-validation and the bootstrap; tree-based methods, random forests and boosting; support-vector machines. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).\nIn this course, students explore the predictive modeling lifecycle, including question formulation, data preprocessing, exploratory data analysis and visualization, model building, model assessment/validation, model selection, and decision-making. The course focuses on quantitative critical thinking and key principles needed to carry out this cycle: 1) Foundational principles for building predictive models; 2) Intuitive explanations of many commonly used predictive modeling techniques for both classification and regression problems; 3) Principles and steps for validating a predictive model; and 4) write and use computer code to perform the necessary foundational work to build and validate predictive models.\nPrerequisites: Mathematics 53 or equivalent; Mathematics 54, Electrical Engineering 16A, Statistics 89A, Mathematics 110 or equivalent linear algebra; Statistics 135, the combination of Data/Stat C140 and Data/Stat/Compsci C100, or equivalent; experience with some programming language. Recommended prerequisite: Mathematics 55 or equivalent exposure to counting arguments.\nCourse Website: [STAT 154]"
  },
  {
    "objectID": "teaching/index.html#stat-215a---applied-statistics-and-machine-learning",
    "href": "teaching/index.html#stat-215a---applied-statistics-and-machine-learning",
    "title": "Teaching",
    "section": "",
    "text": "Course Description: Applied statistics and machine learning, focusing on answering scientific questions using data, the data science life cycle, critical thinking, reasoning, methodology, and trustworthy and reproducible computational practice. Hands-on experience in open-ended data labs, using programming languages such as R and Python. Emphasis on understanding and examining the assumptions behind standard statistical models and methods and the match between the assumptions and the scientific question. Exploratory data analysis. Model formulation, fitting, model testing and validation, interpretation, and communication of results. Methods include linear regression and generalizations, decision trees, random forests, simulation, and randomization methods.\nPrerequisites: Linear algebra, calculus, upper division probability and statistics, and familiarity with high-level programming languages. Statistics 133, 134, and 135 recommended.\nCourse Website: [STAT 215A]"
  },
  {
    "objectID": "teaching/index.html#stat-214---data-analysis-and-machine-learning-for-real-world-decision-making",
    "href": "teaching/index.html#stat-214---data-analysis-and-machine-learning-for-real-world-decision-making",
    "title": "Teaching",
    "section": "",
    "text": "Course Description: Students will be engaged in open-ended data projects for decision making to solve domain problems. It mirrors the entire data science life cycle in practice, including problem formulation, data cleaning, exploratory data analysis, statistical and machine learning modeling and computational techniques, and interpretation of results in context. It is guided by the Predictability-Computability-Stability (PCS) framework for veridical data science and emphasizes critical thinking and documenting human judgment calls and code. It coaches not only the technical but also communication and teamwork skills in order to obtain responsible and reliable data-driven conclusions for solving complex real world problems.\nPrerequisites: Linear algebra, calculus, upper division probability and statistics, and familiarity with high-level programming languages.\nCourse Website: [STAT 214]"
  },
  {
    "objectID": "teaching/index.html#stat-154---modern-statistical-prediction-and-machine-learning",
    "href": "teaching/index.html#stat-154---modern-statistical-prediction-and-machine-learning",
    "title": "Teaching",
    "section": "",
    "text": "Course Description: Theory and practice of statistical prediction. Contemporary methods as extensions of classical methods. Topics: optimal prediction rules, the curse of dimensionality, empirical risk, linear regression and classification, basis expansions, regularization, splines, the bootstrap, model selection, classification and regression trees, boosting, support vector machines. Computational efficiency versus predictive performance. Emphasis on experience with real data and assessing statistical assumptions. This course uses Python as its primary computing language; details are determined by the instructor.\nThis is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear regression, model assessment, model selection, regularization methods (PCR, PLSR, ridge and lasso); logistic regression and discriminant analysis; cross-validation and the bootstrap; tree-based methods, random forests and boosting; support-vector machines. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).\nIn this course, students explore the predictive modeling lifecycle, including question formulation, data preprocessing, exploratory data analysis and visualization, model building, model assessment/validation, model selection, and decision-making. The course focuses on quantitative critical thinking and key principles needed to carry out this cycle: 1) Foundational principles for building predictive models; 2) Intuitive explanations of many commonly used predictive modeling techniques for both classification and regression problems; 3) Principles and steps for validating a predictive model; and 4) write and use computer code to perform the necessary foundational work to build and validate predictive models.\nPrerequisites: Mathematics 53 or equivalent; Mathematics 54, Electrical Engineering 16A, Statistics 89A, Mathematics 110 or equivalent linear algebra; Statistics 135, the combination of Data/Stat C140 and Data/Stat/Compsci C100, or equivalent; experience with some programming language. Recommended prerequisite: Mathematics 55 or equivalent exposure to counting arguments.\nCourse Website: [STAT 154]"
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "People",
    "section": "",
    "text": "Abhineet Agarwal (Ph.D. Student, Statistics)\nDavid Deriso (Ph.D. student, CS)\nJakob Heiss (Post-doc)\nYaxuan Huang (Ph.D. Student, Statistics)\nNathan Benjamin McNaughton (MEng)\nRobin Netzorg (Ph.D. Student, CS)\nAnthony Ozerov (Ph.D. Student, Statistics)\nZachary Rewolinski (Ph.D. Student, Statistics)\nOmer Ronen (Ph.D. Student, Statistics)\nJingfeng Wu (Post-doc)\nChengzhong Ye (Ph.D. Student, Statistics)\nZeyu Yun (Ph.D. Student, CS)\nAustin Zane (Ph.D. Student, Statistics)\n\n\n\n\n\nSoufiane Hayou (2025) - Assistant Professor, Johns Hopkins University\nAliyah Hsu (2025) - Software Engineer, Salesforce\nCorine Elliot (2024) - Statistical Scientist, Berry Consultants\nJames Duncan (2024) - Climate ML Engineering, Allen Institute for AI (AI2)\nTiffany Tang (2023) - Clare Boothe Luce Assistant Professor, University of Notre Dame\nDennis Shen (2023) - Assistant Professor, USC\nSpencer Frei (2023) - Assistant Professor, UC Davis\nRebecca L. Barter (2022) - Research Assistant Professor, University of Utah\nLuiz Chamon (2022) - Research Group Leader, University of Stuttgart\nAngela Zhou (2022) - Assistant Professor, University of Southern California\nBriton Park (2022) - Citadel Securities\nChandan Singh (2022) - Microsoft Research\nKeyan Nasseri (2022) - Google\nWooseok Ha (2022) - Assistant Professor, KAIST, Korea\nYan Shuo Tan (2022) - Assistant Profressor, Department of Statistics and Data Science, National University of Singapore\nXiao Li (2021) - Voleon\nRaaz Dwivedi (2021) - Assistant Professor, Cornell Tech\nYu Wang (2020) - Two Sigma\nMerle Behr (2020) - Professor of Informatics, Regensburg University, Germany\nNicholas Altieri (2020) - Genentech\nYuansi Chen (2019) - Associate Professor, ETH, Zurich\nKarl Kumbier (2019) - Postdoctoral Researcher, UC San Francisco\nJamie Murdoch (2019) - Co-founder / President, clientelligent.ai\nSoren Kunzel (2019) - Data Scientist, Citadel\nSimon Walter (2019) - Data Scientist, Google\nSujayam Saha (2018) - Data Scientist, Google\nReza Abbasi-Asl (2018) - Associate Professor of Neuroscience, UCSF\nChristine Kuang (2017) - Data Scientist, Facebook\nHanzhong Liu (2016) - Associate Professor, Department of Statistics, Tsinghua University\nSumanta Basu (2016) - Associate Professor, Department of Statistics, Cornell University\nSivaraman Balakrishnan (2016) - Professor, Department of Statistics, Carnegie Mellon University\nAdam Bloniarz (2016) - Data Scientist, Google\nSiqi Wu (2016) - Data Scientist, Citadel\nHongwei Li (2015) - Senior Data Scientist, UBER\nAntony Joseph (2014) - Staff Data Scientist, Walmart eCommerce\nTaesup Moon (2013) - Research Staff Member, Samsung Advanced Institute of Technology\nYuval Benjamini (2013) - Professor, Department of Statistics, Hebrew University\nJulien Mairal (2012) - Senior Research Scientist, INRIA\nYueqing Wang (2012) - Research Scientist, Google\nHau-tieng Wu (2012) - Professor, Mathematics Department, Courant Institute, NYU\nGarvesh Raskutti (2012) - Associate Professor, Department of Statistics, Univ. Wisconsin-Madison\nLuke W. Miratrix (2012) - Associate Professor, School of Education, Harvard University\nHarry Kim (2011) - Product Marketing Manager, Google\nChinghway Lim (2011) - Assistant Professor, Department of Statistics and Applied Probability, National University of Singapore\nKarl Rohe (2011) - Professor, Department, University of Wisconsin-Madison.\nKyle Jia (2010) - Associate Professor, Peking University, China.\nDavid Purdy (2009) - Data Scientist, Uber\nPradeep Ravikumar (2009) - Professor, Department of Computer Sciences, University of Texas, Austin.\nVince Vu (2009) - Associate Professor, Department of Statistics, The Ohio State University\nEthan Anderes (2008) - Professor, Department of Statistics, University of California, Davis\nGuilherme Rocha (2008) - Assistant Professor, Department of Statistics, Indiana University, Bloomington\nPeng Zhao (2006) - CEO of Citadel Securities, Citadel Investment Group in Chicago, IL.\nNicolai Meinshausen (2006) - Professor, Department of Statistics, ETH Zurich\nTao Shi (2005) - Quantitative Researcher, Citadel Investment Group in Chicago, IL.\nGang Liang (2004) - Quantitative Operations Associate, Wells Fargo\nRebecka JÃ¶rnsten (2000) - Professor, Biostatistics and Applied Statistics, Chalmers University of Technology, Sweden\nGrace Chang (2000) - Senior Manager of Product Development, Marketing Dept, MTN Irancell\n\n\n\n\n\nYeshwanth Cherapanamjeri (2023)\nArman Sabbaghi (2022)\nYanjun Qi (2020)\nEzequiel Smucler (2020)\nLaura Reiger (2019)\nYanyan Lan (2018)\nYahong Han (2016)\nThibault Vatter\nGuoqiang Cai\nTrine Abrahamsen\nXiaoling Lu\nXiusheng Lu (2014)\nXiangyu Chang (2012)\nLing Hong (2012)\nToshiyasu Matsushima (2012)\nYanfeng Gu (2012) - Professor of Electronics Engineering, Harbin Institute of Technology\nHai Zhang (2012)\nRodolphe Jenatton (2010)\nKenichi Hayashi (2010)\nYangbo He (2010)\nFei Wu (2010)\nKei Kobayashi (2008)\nXing Wang (2007)"
  },
  {
    "objectID": "people/index.html#current-members-role-program",
    "href": "people/index.html#current-members-role-program",
    "title": "People",
    "section": "",
    "text": "Abhineet Agarwal (Ph.D. Student, Statistics)\nDavid Deriso (Ph.D. student, CS)\nJakob Heiss (Post-doc)\nYaxuan Huang (Ph.D. Student, Statistics)\nNathan Benjamin McNaughton (MEng)\nRobin Netzorg (Ph.D. Student, CS)\nAnthony Ozerov (Ph.D. Student, Statistics)\nZachary Rewolinski (Ph.D. Student, Statistics)\nOmer Ronen (Ph.D. Student, Statistics)\nJingfeng Wu (Post-doc)\nChengzhong Ye (Ph.D. Student, Statistics)\nZeyu Yun (Ph.D. Student, CS)\nAustin Zane (Ph.D. Student, Statistics)"
  },
  {
    "objectID": "people/index.html#former-ph.d.-students-and-postdocs-graduation-year",
    "href": "people/index.html#former-ph.d.-students-and-postdocs-graduation-year",
    "title": "People",
    "section": "",
    "text": "Soufiane Hayou (2025) - Assistant Professor, Johns Hopkins University\nAliyah Hsu (2025) - Software Engineer, Salesforce\nCorine Elliot (2024) - Statistical Scientist, Berry Consultants\nJames Duncan (2024) - Climate ML Engineering, Allen Institute for AI (AI2)\nTiffany Tang (2023) - Clare Boothe Luce Assistant Professor, University of Notre Dame\nDennis Shen (2023) - Assistant Professor, USC\nSpencer Frei (2023) - Assistant Professor, UC Davis\nRebecca L. Barter (2022) - Research Assistant Professor, University of Utah\nLuiz Chamon (2022) - Research Group Leader, University of Stuttgart\nAngela Zhou (2022) - Assistant Professor, University of Southern California\nBriton Park (2022) - Citadel Securities\nChandan Singh (2022) - Microsoft Research\nKeyan Nasseri (2022) - Google\nWooseok Ha (2022) - Assistant Professor, KAIST, Korea\nYan Shuo Tan (2022) - Assistant Profressor, Department of Statistics and Data Science, National University of Singapore\nXiao Li (2021) - Voleon\nRaaz Dwivedi (2021) - Assistant Professor, Cornell Tech\nYu Wang (2020) - Two Sigma\nMerle Behr (2020) - Professor of Informatics, Regensburg University, Germany\nNicholas Altieri (2020) - Genentech\nYuansi Chen (2019) - Associate Professor, ETH, Zurich\nKarl Kumbier (2019) - Postdoctoral Researcher, UC San Francisco\nJamie Murdoch (2019) - Co-founder / President, clientelligent.ai\nSoren Kunzel (2019) - Data Scientist, Citadel\nSimon Walter (2019) - Data Scientist, Google\nSujayam Saha (2018) - Data Scientist, Google\nReza Abbasi-Asl (2018) - Associate Professor of Neuroscience, UCSF\nChristine Kuang (2017) - Data Scientist, Facebook\nHanzhong Liu (2016) - Associate Professor, Department of Statistics, Tsinghua University\nSumanta Basu (2016) - Associate Professor, Department of Statistics, Cornell University\nSivaraman Balakrishnan (2016) - Professor, Department of Statistics, Carnegie Mellon University\nAdam Bloniarz (2016) - Data Scientist, Google\nSiqi Wu (2016) - Data Scientist, Citadel\nHongwei Li (2015) - Senior Data Scientist, UBER\nAntony Joseph (2014) - Staff Data Scientist, Walmart eCommerce\nTaesup Moon (2013) - Research Staff Member, Samsung Advanced Institute of Technology\nYuval Benjamini (2013) - Professor, Department of Statistics, Hebrew University\nJulien Mairal (2012) - Senior Research Scientist, INRIA\nYueqing Wang (2012) - Research Scientist, Google\nHau-tieng Wu (2012) - Professor, Mathematics Department, Courant Institute, NYU\nGarvesh Raskutti (2012) - Associate Professor, Department of Statistics, Univ. Wisconsin-Madison\nLuke W. Miratrix (2012) - Associate Professor, School of Education, Harvard University\nHarry Kim (2011) - Product Marketing Manager, Google\nChinghway Lim (2011) - Assistant Professor, Department of Statistics and Applied Probability, National University of Singapore\nKarl Rohe (2011) - Professor, Department, University of Wisconsin-Madison.\nKyle Jia (2010) - Associate Professor, Peking University, China.\nDavid Purdy (2009) - Data Scientist, Uber\nPradeep Ravikumar (2009) - Professor, Department of Computer Sciences, University of Texas, Austin.\nVince Vu (2009) - Associate Professor, Department of Statistics, The Ohio State University\nEthan Anderes (2008) - Professor, Department of Statistics, University of California, Davis\nGuilherme Rocha (2008) - Assistant Professor, Department of Statistics, Indiana University, Bloomington\nPeng Zhao (2006) - CEO of Citadel Securities, Citadel Investment Group in Chicago, IL.\nNicolai Meinshausen (2006) - Professor, Department of Statistics, ETH Zurich\nTao Shi (2005) - Quantitative Researcher, Citadel Investment Group in Chicago, IL.\nGang Liang (2004) - Quantitative Operations Associate, Wells Fargo\nRebecka JÃ¶rnsten (2000) - Professor, Biostatistics and Applied Statistics, Chalmers University of Technology, Sweden\nGrace Chang (2000) - Senior Manager of Product Development, Marketing Dept, MTN Irancell"
  },
  {
    "objectID": "people/index.html#former-visitors-and-associates-starting-year",
    "href": "people/index.html#former-visitors-and-associates-starting-year",
    "title": "People",
    "section": "",
    "text": "Yeshwanth Cherapanamjeri (2023)\nArman Sabbaghi (2022)\nYanjun Qi (2020)\nEzequiel Smucler (2020)\nLaura Reiger (2019)\nYanyan Lan (2018)\nYahong Han (2016)\nThibault Vatter\nGuoqiang Cai\nTrine Abrahamsen\nXiaoling Lu\nXiusheng Lu (2014)\nXiangyu Chang (2012)\nLing Hong (2012)\nToshiyasu Matsushima (2012)\nYanfeng Gu (2012) - Professor of Electronics Engineering, Harbin Institute of Technology\nHai Zhang (2012)\nRodolphe Jenatton (2010)\nKenichi Hayashi (2010)\nYangbo He (2010)\nFei Wu (2010)\nKei Kobayashi (2008)\nXing Wang (2007)"
  },
  {
    "objectID": "papers/veridical-data-science-pcs.html",
    "href": "papers/veridical-data-science-pcs.html",
    "title": "Veridical Data Science (PCS)",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/veridical-data-science-pcs.html#selected-recent-papers-on-veridical-data-science-pcs",
    "href": "papers/veridical-data-science-pcs.html#selected-recent-papers-on-veridical-data-science-pcs",
    "title": "Veridical Data Science (PCS)",
    "section": "Selected Recent Papers on Veridical Data Science (PCS)",
    "text": "Selected Recent Papers on Veridical Data Science (PCS)\n\nB. Yu (2024). After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI (discussion of Donoho’s paper “Data Science at the Singularity”) Harvard Data Science Review (HDSR).\nB. Yu (2023). What is uncertainty in today’s practice of data science? J. Econometrics. 237, 105519.\nQ. Wang, T. M. Tang, N. Youlton, C. S. Weldy, A. M. Kenney, O. Ronen, J. W. Hughes, E. T. Chin, S. C. Sutton. A. Agarwal, X. Li, M. Behr, K. Kumbier, C. S. Moravec, W. H. W. Tang, K. B. Margulies, T. P. Cappola, A. J. Buitte, R. Arnaout, J. B. Brown, J. R. Priest, V. N. Parikh, B. Yu, E. Ashley (2023). Epistasis regulates genetic control of cardiac hypertrophy. (Code) (PCS documentation)\nR. Cahill, Y. Wang, R. P. Xian, A. J. Lee, H. Zeng, B. Yu, B. Tasic, R. Abbasi-Asl (2023). Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology. (Code)\nA. Agarwal, A. M. Kenny, Y. S. Tan, T. M. Tang, B. Yu (2023). MDI+: a flexible random forest-based feature importance framework. (PCS related)\nM. Behr, K. Kumbier, A. Cordova-Palomera, M. Aguirre, E. Ashley, A. Butte, R. Arnaout, J. B. Brown, J. Preist, B. Yu (2020). Learning epistatic polygenic phenotypes with Boolean interactions. (code) (PCS inference case study)\nB. Norgeot, G. Quer, B. K. Beaulieu-Jones, A. Torkamani, R. Dias, M. Gianfrancesco, R. Arnaout, I. S. Kohane, S. Saria, E. Topol, Z. Obermeyer, B. Yu & A. Butte (2020). Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist, Nature Medicine, 26, 1320–1324.\nB. Yu (2020). Stability expanded, in reality. Harvard Data Science Review (HDSR). (PCS related)\nB. Yu and R. Barter (2020). Data science process: one culture. JASA. (PCS related)\nR. Dwivedi, Y. Tan, B. Park, M. Wei, K. Horgan, D. Madigan, B. Yu (2020). Stable discovery of interpretable subgroups via calibration in causal studies (staDISC). International Statistical Review (arXiv) (code) (PCS case study for causal inference)\nX. Li, T. M. Tang, X. Wang, J. A. Kocher, B. Yu (2020). A stability-driven protocol for drug response interpretable prediction (staDRIP). NeurISP workshop on ML4H (Machine learning for Health) Extended Abstract. drive link\nB. Yu and K. Kumbier (2020) Veridical data science (PCS framework), PNAS. 117 (8), 3920-3929. QnAs with Bin Yu.\nY. Chen, R. Abbasi-Asl, A. Bloniarz, M. Oliver, B. Willmore, J. Gallant, and B. Yu (2018) The DeepTune framework for modeling and characterizing neurons in visual cortex area V4.\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. (an enhanced version of iRF, PCS related)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (code) (PCS related)\nSiqi Wu, Antony Joseph, Ann S. Hammonds, Susan E. Celniker, Bin Yu, and Erwin Frise (2016). Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks (with support information). PNAS, pp. 4290 - 4295. (code) (PCS related)"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Papers",
    "section": "",
    "text": "Also see Google Scholar and arXiv\n\n\nA. R. Hsu, Y. Cherapanamjeri, A. Y. Odisho, P. R. Carroll, B. Yu (2024). Mechanistic Interpretation through Contextual Decomposition in Transformers. (paper)\nY. S. Tan, O. Ronen, T. Saarinen, B. Yu (2024). The Computational Curse of Big Data for Bayesian Additive Regression Trees: a Hitting Time Analysis. (paper)\nO. Ronen, A. I. Humayun, R. Balestriero, R. Baraniuk, B. Yu (2024). ScaLES: Scalable Latent Exploration Score for Pre-Trained Generative Networks. (paper)\nS. Hayou, N. Ghosh, B. Yu (2024). The Impact of Initialization on LoRA Fineuning Dynamics. (paper)\nB. Yu (2024). After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI. (Harvard Data Science Review) (paper)\nS. Hayou, N. Ghosh, B. Yu (2024). LoRA+: Efficient Low Rank Adaptation of Large Models. (Proc. ICML) (paper)\nC. F. Elliott, J. Duncan, T. M. Tang, M. Behr, K. Kumbier, B. Yu (2024). Designing a data science simulation with MERITS: a primer. (paper)\nY. Chen, C. Singh, X. Liu, S. Zuo, B. Yu, H. He, J. Gao (2024). Towards consistent natural-language explanations via explanation-consistent finetuning. (paper)\nN. R. Mallinar, A. Zane, S. Frei, B. Yu (2024). Minimum-Norm Interpolation Under Covariate Shift. (Proc. ICML) (paper)\nL. Sun, A. Agarwal, A. Kornblith, B. Yu, C. Xiong (2024). ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance. (Proc. ICML) (paper)\n\n\n\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu, J. Gao (2023). Explaining black box text modules in natural language with language models. (paper)\nQ. Zhang, C. Singh, L. Liu, X. Liu, B. Yu, J. Gao, T. Zhao (2023). Tell your model where to attend: post-hoc attention steering for LLMs. (paper)\nQ. Wang, T. M. Tang, N. Youlton, C. S. Weldy, A. M. Kenney, O. Ronen, J. W. Hughes, E. T. Chin, S. C. Sutton, A. Agarwal, X. Li, M. Behr, K. Kumbier, C. S. Moravec, W. H. W. Tang, K. B. Margulies, T. P. Cappola, A. J. Buitte, R. Arnaout, J. B. Brown, J. R. Priest, V. N. Parikh, B. Yu, E. Ashley (2023). Epistasis regulates genetic control of cardiac hypertrophy. (paper) (code) (PCS documentation)\nR. Cahill, Y. Wang, R. P. Xian, A. J. Lee, H. Zeng, B. Yu, B. Tasic, R. Abbasi-Asl (2023). Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology. (paper) (code)\nE. Irajizad, A. Kenney, T. Tang, J. Vykoukal, R. Wu, E. Murage, J. B. Dennison, M. Sans, J. P. Long, M. Loftus, J. A. Chabot, M. D. Kluger, F. Kastrinos, L. Brais, A. Babic, K. Jajoo, L. S. Lee, T. E. Clancy, K. Ng, A. Bullock, J. M. Genkinger, A. Maitra, K. A. Do, B. Yu, B. W. Wolpin, S. Hanash, J. F. Fahrmann. (2023). A blood-based metabolomic signature predictive of risk for pancreatic cancer. Cell Reports Medicine 4(9): 101194. doi: 10.1016/j.xcrm.2023.101194. (PCS related) (paper) (Editorial)\nR. Dwivedi, C. Singh, B. Yu, M. Wainwright (2023). Revisiting miniumu description length complexity in overparametrized models. JMLR, 24(268): 1-59. (paper)\nK. Wu, Y. Chen, W. Ha, B. Yu (2023). Prominent roles of conditionally invariant components in domain adaptation: theory and algorithms. JMLR (accepted). (paper)\nN. Ghosh, S. Frei, W. Ha, B. Yu (2023). The effect of SGD batch size on autoencoder learning: sparsity, sharpness and feature learning. (paper)\nR. Netzorg, J. Li, B. Yu (2024). Improving prototypical part networks with reward reweighting, reselection, and retraining. Proc. ICML. (paper)\nA. Agarwal, A. M. Kenny, Y. S. Tan, T. M. Tang, B. Yu (2023). MDI+: a flexible random forest-based feature importance framework. (PCS related) (paper)\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. Proc. ICLR. (paper)\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu and J. Gao (2023). Explaining black box text modules in natural language with language models. (paper)\n\n\n\nD. Shen, P. Ding, J. Sekhon, B. Yu (2022). Same root different leaves: time series and cross-sectional methods in panel data. Econometrica (accepted). (paper)\nB. Park, X. Wu, B. Yu, A. Zhou (2022). Offline evaluation in RL: soft stability weighting to combine fitted Q-learning and model-based methods. NeurIPS 3rd Offline Reinforcement Learning Workshop. (paper)\nY. S. Tan, C. Singh, K. Nasseri, A. Agarwal, J. Duncan, O. Ronen, M. Epland, A. Kornblith, B. Yu (2022). Fast interpretable greedy-tree sums (FIGS). (paper) (code)\nA. Agarwal, Y. S. Tan, O. Ronen, C. Singh, B. Yu (2022). Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods. Proc. ICML (paper) (code).\nB. Yu and C. Singh (2022). Seven principles for rapid-response data science: lessons learned from covid-19 forecasting. Statistical Science, 36(2):266-269. (paper)\nN. Ghosh, S. Mei, and B. Yu (2022). The three stages of dynamics in high-dimensional kernel methods. Proc. ICLR, 2022. (paper)\n\n\n\nY. Tan, A. Agarwal, and B. Yu (2021). A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds. Proc. AISTATS. (paper)\nN. Altieri, B. Park, J. DeNero, A. Odisho, B. Yu. (2021). Improving natural language information extraction from cancer pathology reports using transfer learning and zero-shot string similarity. JAMIA Open. 2021 Sept. 30 4(3). (paper)\nC. Singh, W. Ha and B. Yu (2021). Interpreting and Improving Deep-Learning Models with Reality Checks. To appear in “xxAI - Beyond Explainable AI” (eds. Holzinger et al.). (paper)\nW. Ha, C. Singh, F. Lanusse, S. Upadhyayula, and B. Yu (2021). Adaptive Wavelet Distillation from Neural Networks through Interpretation. Proc. NeurIPS 2021. (paper) (code)\nM. Behr, Y. Wang, X. Li, B. Yu (2022). Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests. PNAS. (PCS-related) (paper) (theory for iRF)\nN. Altieri, B. Park, M. Olson, J. DeNero, A. Odisho, B. Yu. (2021). Supervised line attention for tumor attribute classification from pathology reports: Higher performance with less data. Journal of Biomedical Informatics. 122 (2021) 103872. (paper)\n\n\n\nM. Behr, K. Kumbier, A. Cordova-Palomera, M. Aguirre, E. Ashley, A. Butte, R. Arnaout, J. B. Brown, J. Preist, B. Yu (2020). Learning epistatic polygenic phenotypes with Boolean interactions. (PCS inference case study) (paper) (code)\nB. Norgeot, G. Quer, B. K. Beaulieu-Jones, A. Torkamani, R. Dias, M. Gianfrancesco, R. Arnaout, I. S. Kohane, S. Saria, E. Topol, Z. Obermeyer, B. Yu & A. Butte (2020). Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist, Nature Medicine, 26, 1320–1324. (paper)\nB. Yu (2020). Stability expanded, in reality. Harvard Data Science Review (PCS related) (paper)\nB. Yu and R. Barter (2020). Data science process: one culture. JASA. (PCS related) (paper)\nR. Dwivedi, Y. Tan, B. Park, M. Wei, K. Horgan, D. Madigan, B. Yu (2020). Stable discovery of interpretable subgroups via calibration in causal studies (staDISC). International Statistical Review (PCS case study for causal inference) (paper) (code)\nX. Li, T. M. Tang, X. Wang, J. A. Kocher, B. Yu (2020). A stability-driven protocol for drug response interpretable prediction (staDRIP). NeurISP workshop on ML4H (Machine learning for Health) Extended Abstract. (paper)\nA. Y. Odisho, B. Park, N. Altieri, J. DeNero, M. R Cooperberg, P. R .Carroll, B. Yu (2020). Natural language processing systems for pathology parsing in limited data environments with uncertainty estimation. JAMIA Open. (paper)\nL. Reiger, J. W. Murdoch, S. Singh, B. Yu (2020). Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge. ICML Proceedings. (paper) (code)\nC. Singh, W. Ha, F. Lanusse, V. Boehm, J. Liu, B. Yu (2020). Transformation Importance with Applications to Cosmology. ICLR Workshop paper. (paper) (code)\nN. Altieri, R. Barter, J. Duncan, R. Dwivedi, K. Kumbier, X. Li, R. Netzorg, B. Park, C. Singh, Y. Tan, T.Tang, Y. Wang, C. Zhang, B. Yu. (2020) Curating a COVID-19 data repository and forecasting county-level death counts in the United States. Harvard Data Science Review (paper) (code) (7-day prediction results) (Short talk video)\nB. Yu and K. Kumbier (2020) Veridical data science (PCS framework), PNAS. 117 (8), 3920-3929. (paper) (QnAs with Bin Yu)\nR. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan and B. Yu (2020) Sharp Analysis of Expectation-Maximization for Weakly Identifiable Mixture Models AISTATS. (paper)\nR. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan and B. Yu (2020) Singularity, Misspecification and the Convergence Rate of EM Annals of Statistics. (paper)\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Fast Mixing of Metropolized Hamiltonian Monte Carlo: Benefits of Multi-Step Gradients. JMLR, (paper) (arXiv)\n\n\n\nR. Dwivedi, Y. Chen, M. J. Wainwright and B. Yu (2019) Log-concave Sampling: Metropolis Hastings Algorithms are Fast JMLR. (paper)\nD. Rothenhäusler and B. Yu (2019). Incremental causal effects. (paper)\n\n\n\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2018) Fast MCMC Algorithms on Polytopes. JMLR.\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Vaidya Walk: A Sampling Algorithm Based on Volumetric-Logarithmic Barrier. Allerton Conference 2017. (paper)\nW. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu* (2019) Definitions, methods, and applications in interpretable machine learning. PNAS, 116 (44) 22071-22080. (paper)\nW. J. Murdoch, C. Sign, and B. Yu (2019). Hierarchical interpretations for neural network predictions. ICLR. (paper) (code)\nY. Wang, S. Wu and B. Yu (2020) Unique Sharp Local Minimum in l1-minimization Complete Dictionary Learning. JMLR. 21(63), pp. 1-52. (paper) (arXiv)\nY. Chen, R. Abbasi-Asl, A. Bloniarz, M. Oliver, B. Willmore, J. Gallant, and B. Yu (2018) The DeepTune framework for modeling and characterizing neurons in visual cortex area V4. (paper)\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. (an enhanced version of iRF, PCS related) (paper) (iRF)\nY. Chen C. Jin, and B. Yu (2018) Stability and Convergence Trade-off of Iterative Optimization Algorithms. (paper)\nJ. Murdoch, P. Liu, and B. Yu (2018) Beyond word importance: contextual decomposition to extract interactions from LSTMs. Proc. ICLR 2018. (paper) (code)\nR. Diwivedi, Y. Chen, M. J. Wainwright, and B. Yu (2018) Log-concave sampling: Metropolis-Hastings algorithms are fast! (paper)\n\n\n\nY. Chen, R. Dwivedi, M. J. Wainwright, and B. Yu (2017) Fast MCMC sampling algorithms on polytopes. (paper)\nB. Yu and K. Kumbier (2018) Artificial Intelligence and Statistics. Frontiers of Information Technology and Electronic Engineering. 19(1), 6-9. (paper)\nR. Abbasi-Asl and B. Yu (2017) Structural Compression of Convolutional Neural Networks Based on Greedy Filter Pruning. (paper)\nR. Abbasi-Asl and B. Yu (2017) Interpreting Convolutional Neural Networks Through Compression. NIPS 2017. Symposium on Interpretable Machine Learning. (paper)\nS. Kunzel, J. Sekhon, P. Bickel, and B. Yu* (2019) Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning. PNAS. 116 (10) 4156-4165. (paper) (arXiv) (code)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (PCS related) (paper) (code)\nS. Balakrishnan, M. Wainwright, B. Yu (2017) Statistical Guarantees for the EM algorithm: from population to sample-based analysis. Annals of Statistics, 45(1), 77 - 120. (paper)\nR. Barter and B. Yu (2017) Superheat: An R package for creating beautiful and extendable heatmaps for visualizing complex data. JCGS (revised). (paper) (code)\nH. Liu and B. Yu (2017) Comments on: High-dimensional simultaneous inference with the bootstrap by Dezeure et al Test. 26: 740-750. (paper)\nC. Carson et al (2016). UC Berkeley Data Science Planning Initiative Faculty Advisory Board (FAB) Report. (paper) (FAB Report Executive Summary)\nS. Wu and B. Yu (2018). Local identifiability of l1-minimization dictionary learning: a sufficient and almost necessary condition. JMLR. 18, 1 - 56. (paper)\n\n\n\nK. Rohe, T. Qin and B. Yu* (2016). Co-clustering directed graphs to discover asymmetries and directional communities. Proc. National Academy of Sciences (PNAS), 113(45), 12679 - 12684. (paper)\nR. E. Kass, B. S. Caffo, M. Davidian, X. Meng, B. Yu, Nancy Reid* (2016). Ten simple rules for effective statistical practice. PLoS Comput. Biol., 12(6): e1004961. doi:10.1371/journal.pcbi.1004961 (paper)\nSiqi Wu, Antony Joseph, Ann S. Hammonds, Susan E. Celniker, Bin Yu, and Erwin Frise (2016). Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks (with support information). PNAS, pp. 4290 - 4295. (PCS related) (paper) (code)\nA. Bloniarz, C. Wu, B. Yu, A. Talwalkar (2016). Supervised neighborhoods for distributed nonparametric regression. Proc. of AISTATS, Barcelona, Spain. (paper)\n\n\n\nB. Yu (2015). Data wisdom for data science. Operational Database Management Systems (ODBMS.ORG). (paper)\nA. Bloniarz, H. Liu, C. Zhang, J. Sekhon, and B. Yu* (2015). Lasso adjustments of treatment effect estimates in randomized experiments. PNAS. 113, 7383 - 7390. (paper)\nP. Ma, M. W. Mahoney and B. Yu (2015). A Statistical Perspective on Algorithmic Leveraging. Journal of Machine Learning Research, 16, (2015), 861-911. (paper)\nT. Moon, Y. Wang, Y. Liu, and B. Yu (2015). Evaluation of a MISR-based high-resolution aerosol retrieval method using AERONET DRAGON campaign data. IEEE Transactions on Geoscience and Remote Sensing, 53, 4328-4339. (paper)\n\n\n\nB. Yu (2014). Let us own data science. Institute of Mathematical Statistics (IMS) Presidental Address, ASC-IMS Joint Conference, Sydney, July, 2014. (paper) (video) (IMS Bulletin)\nG. Schiebinger, M. J. Wainwright and B. Yu (2014). The geometry of kernelized spectral clustering. Annals of Statistics, 43, 819-846. (paper)\nL. Miratrix, J. Jia, B. Yu, B. Gawalt, L. El Ghaoui, L. Barnesmoore, S. Clavier (2014). Concise comparative summaries (CCS) of large text corpora with a human experiment. Ann. Applied Statist., 8, 499-529. (paper)\nY. Benjamini and B. Yu (2014). The shuttle estimator for explainable variance in fMRI experiments. Annals of Applied Statistics, 7, 2007-2033. (paper)\nD. Bean, P. Bickel, N. El Karoui and B. Yu (2014). Optimal M-estimation in high-dimensional regression. Proceedings of National Academy of Sciences, 110, 1456314568. (paper)\nN. El Karoui, D. Bean, P. Bickel, C. Lim, and B. Yu (2014). On robust regression with high-dimensional predictors. Proceedings of National Academy of Sciences, 110, 1455714562. (paper)\nP. Ma, M. W. Mahoney, B. Yu (2014). A Statistical Perspective on Algorithmic Leveraging. Proc. of International Conference on Machine Learning (ICML) (This conference paper contains some of preliminary results of the journal submission Ma et al. (2015)) (paper)\nA. Bloniarz, A. Talwalkar, J. Terhorst, M. Jordan, D. Patterson, B. Yu and Y. Song (2014). Changepoint Analysis for Efficient Variant Calling. Proc. of RECOMB 2014 (to appear). (paper)\nTao Shi (2013), A conversation with Professor Bin Yu International Chinese Statistical Association (ICSA) Bulletin, Vol 25, Issue 2, pp 85-98. (paper) (Selected Parts in Statblogs)\nA. Joseph and B. Yu (2016). The impact of regularization on spectral clustering. Annals of Statistics. 4, 1765 - 1791. (paper)\nC. Lim and B. Yu (2016). Estimation Stability with Cross Validation (ESCV) Journal of Computational and Graphical Statistics. 25, 464 - 492. (First paper towards PCS) (paper)\n\n\n\nA. S. Hammonds, C. A. Bristow, W. W. Fisher, R. Weiszmann, S. Wu, V. Hartenstein, M. Kellis, B. Yu, E. Frise, and S. E. Celniker (2013). Spatial expression of transcription factors in Drosophila embryonic organ development. Genome Biology, 14(12), R140. (paper)\nH. Liu and B. Yu (2013). Asymptotic properties of Lasso+mLS and Lasso+Ridge in sparse high-dimensional linear regression. Electron. J. Statist., 7, 312-3169. (paper)\nJ. Mairal and B. Yu (2013). Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows. Journal of Machine Learning Research, 14, 2449-2485. (paper)\nY. Wang, X. Jiang, B. Yu, M. Jiang (2013). A Hierarchical Bayesian Approach for Aerosol Retrieval Using MISR Data. J. American Statistical Association, 108, 483-493. (paper)\nY. He, J. Jia and B. Yu (2013). Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs. Annals of Statistics, 41(4), 1742-1779. (paper)\nB. Yu (2013). Stability. Bernoulli, 19 (4), 1484-1500. (Invited paper for the Special Issue commemorating the 300th anniversary of the publication of Jakob Bernoullis Ars Conjectandi in 1712) (Begining of PCS) (paper)\nJ. Mairal and B. Yu (2013). Discussion on Grouping Strategies and Thresholding for High Dimensional Linear Models Journal of Statistical Planning and Inference, 143, 1451-1453.\nC. Uhler, G. Raskutti, and P. Buhlmann and B. Yu (2013). Geometry of faithfulness assumption in causal inference. Annals of Statistics, 41, 436-463. (paper)\nL. Miratrix, J. Sehkon, and B. Yu (2013). Adjusting Treatment Effect Estimates by Post-Stratification in Randomized Experiments. Journal of Royal Statistical Society, Series B, 75 (part 2), 369-396. (paper)\nJ. Jia, K. Rohe and B. Yu (2013) The Lasso under Poisson-like Heteroscadecity. Statistica Sinica, 23, 99-118. (paper)\nS. Negahban, P. Ravikumar, M. Wainwrigt, and B. Yu (2012) A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. Statistical Science, 27, 538-557. (paper)\nG. Raskutti, M. Wainwrigt, and B. Yu (2012) Minimax-optimal rates for sparse additive models over kernel classes via convex programming. J. Machine Learning Research, 13, 389-427. (paper)\nJ. Mairal and B. Yu (2012). Complexity analysis of the Lasso regularization path. Proc. of International Conference on Machine Learning (ICML). (paper)\n\n\n\nYanfeng Gu, Shizhe Wang, Tao Shi, Yinghui Lu, Eugene E. Clothiaux, and Bin Yu (2012). Multiple-kernel learning-based unmixing algorithm for estimation of cloud fractions with MODIS and CLOUDSAT data. Proc. of IEEE International Geoscience and Remote Sensing Symposium (IGRSS). (paper)\n\n\n\nS. Nishimoto, A. T. Vu, T. Naselaris, Y. Benjamini, B. Yu, J. L. Gallant (2011). Reconstructing visual experiences from brain activity evoked by natural movies. Current Biology, 21(19), 1641-1646. (paper) (related videos)\nP. Ravikumar, M. Wainwright, G. Raskutti, B. Yu (2011). High-dimensional covariance estimation by minimizing l1-penalized log-determinant divergence. Electronic Journal of Statistics, 5, 935-980. (paper)\nG. Raskutti, M. Wainwright, B. Yu (2011). Minimax rates of estimation for high-dimensional linear regression over lq-balls. IEEE Trans. Inform. Th., 57(10), 6976-6994. (paper)\nK. Rohe, S. Chatterjee, and B. Yu (2011). Spectral clustering and the high-dimensional Stochastic Block Model. Annals of Statistics, 39 (4), 1878-1915 (paper)\nV. Q. Vu, P. Ravikumar, T. Naselaris, K. N. Kay, J. L. Gallant, B. Yu* (2011). Encoding and decoding V1 fMRI responses to natural images with sparse nonparametric models. Annals of Applied Statistics, 5, 1150-1182. (*First senior author as last author in biology tradition) (paper)\nS. N. Pakzad, G. Rocha, and B. Yu (2011). Distributed modal identification by regularized auto regressive models. International Journal of Systems Science, 42, 1473-1489.\nJ. Yousafzai, P. Sollich, Z. Cvetkovic, and B. Yu (2011). Combined Features and Kernel Design for Robust Phoneme Classification Using Support Vector Machines. IEEE Trans. Audio, Speech and Language Processing (to appear). 64. (paper)\nX. Dai, J. Jia, B. Yu, El Ghaoui (2011) SBA-term: Sparse Bilingual Association for terms. Proc. International Conference on Semantic Computing. (paper)\nB. Yu (2011). Asymptotics and Coding Theory: One of the n - 1 Dimensions of Terry. In Selected Works of Terry Speed (ed. S. Duoit), pp. 33-36, Springer. (paper)\n\n\n\nB. Yu (2010). Remembering Leo. Annals of Applied Statistics, 4(4), 1657-1659. (paper)\nJ. Jia, Y. Benjamini, C. Lim, G. Raskutti, B. Yu (2010). Comment on “Envelope models for parsimonious and efficient multivariate linear regression” by R. D. Cook, B. Li, and F. Chiaromonte. Statistica Sinica, 20, 961-967. (paper)\nG. Raskutti, M. Wainwrigt, and B. Yu (2010) Restricted Eigenvalue Properties for Correlated Gaussian Designs. Journal of Machine Learning Research, 11, 2241-2259. (paper)\nJ. Jia and B. Yu (2010). On model selection consistency of elastic net when p &gt;&gt;n. Statistica Sinica, 10, 595-611. (paper)\nP. Buhlmann and B. Yu (2010). Boosting. Wiley Interdisciplinary Reviews: Computational Statistics, 2, 69-74. (paper)\nL. Huang, J. Jia, B. Yu, B. Chun, P. Maniatis, M. Naik (2010). Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression. Proc. NIPS 2010. (paper)\nY. Han, F. Wu, J. Jia, Y. Zhuang and B. Yu (2010). Multi-task Sparse Discriminant Analysis (MtSDA) with Overlapping Categories. Proc. of The 24th AAAI Conference on Artificial Intelligence, July 11-15, Atlanta, GA. (paper)\nB. Gawalt, J. Jia, L. Miratrix, L. El Ghaoui, B. Yu, and S. Clavier (2010). Discovering Word Associations in News Media via Feature Selection and Sparse Classification. Proc. 11th ACM SIGMM International Confernece on Multimedia Information Retrieval (MIR). (paper)\n\n\n\nE. Anderes, B. Yu, V. Jovanovic, C. Moroney, M. Garay, A. Braverman, E. Clothiaux (2009) Maximum Likelihood Estimation of Cloud Height from Multi-Angle Satellite Imagery. Annals of Applied Statistics, 3, 902-921 (paper)\nT. Shi, M. Belkin, and B. Yu, (2009) Data Spectroscopy: Eigenspace of Convolution Operator and Clustering Annals of Statistics, 37 (6B), 3960-3984. (paper)\nVincent Q. Vu, Bin Yu, Robert E. Kass (2009) Information In The Non-Stationary Case Neural computation 21, 688-703. (paper)\nN. Meinshausen and B. Yu (2009). Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics 37, 246-270. (paper)\nP. Zhao, G. Rocha, and B. Yu (2009). The composite absolute penalties family for grouped and hierarchical variable selection Annals of Statistics 37, 3468-3497. (An earlier version ‘appeared as Grouped and hierarchical model selection through composite absolute penalties’ by P. Zhao, G. Rocha and B. Yu, Department of Statistics, UC Berkeley, Tech. Rep 703.) (paper)\nS. Negahban, P. Ravikumar, M. Wainwright, and B. Yu (2009). A unified framework for high-dimensional analysis of \\(M\\)-estimators with decomposable regularizers Proc. NIPS, 2009. (This conference paper contains preliminary results of the journal submission Negahban et al. 2012). (paper)\nG. Raskutti, M. Wainwright, B. Yu (2009) High-dimensional regression under lq-ball sparsity: Optimal rates of convergence. Proc. of Allerton Conference on Communication, Control, and Computing. (This conference paper contains some of preliminary results of the journal submission Ravikumar et al. 2011). (paper)\nG. Raskutti, M. Wainwrigt, and B. Yu (2009) Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness. Proc. NIPS, 2009. (This conference paper contains some of preliminary results of the journal submission Ravikumar et al. 2011). (paper)\n\n\n\nT. Shi, B. Yu, E. Clothiaux, and A. Braverman (2008). Daytime Arctic Cloud Detection based on Multi-angle Satellite Data with Case Studies. Journal of American Statistical Association. 103( 482), 584-593. (paper)\nPeter Buhlmann and Bin Yu (2008) Invited discussion on “Evidence contrary to the statistical view of boosting (D. Mease and A. Wyner)”. Journal of Machine Learning Research 9, 187-194. (paper with discussion)\nP. Ravikumer, V. Vu, B. Yu, T. Naselaris, K. Kay, J. Gallant (2008). Nonparametric sparse hiearchical models describe V1 fMRI responses to natural images In Adavances in Neural Information Processing Systems (NIPS) 21, (2008). (This conference paper contains some preliminary results of journal paper Vu et al. (2011) on encoding models, but also contains an encoding model that is not in Vu et al. (2011). It does not contain decoding results.) (paper)\nP. Ravikumar, G. Raskutti, M. Wainwright, B. Yu (2008) Model selection in Gaussian graphical models: high-dimensional consistency of l1-regularized MLE. In Adavances in Neural Information Processing Systems (NIPS) 21, (2008). (paper)\nT. Shi, M. Belkin, and B. Yu (2008). Data spectroscopy: learning mixture models using eigenspaces of convolution operators. Proc. of ICML 2008. (paper)\nM. Ager, Z. Cvetkovic, P. Pollich, and B. Yu (2008). Towards Robust Phoneme Classification Augmentation of PLP Models with Acoustic Waveforms. Proceedings of EUSIPCO. (paper)\nJ. Yousafzai, Z. Cvetković, P. Pollich, and B. Yu (2008). Combined PLP-Acoustic Waveform Classification for Robust Phoneme Recognition using Support Vector Machines. Proceedings of EUSIPCO. (paper)\n\n\n\nN. Meinshausen, G. Rocha, and B. Yu (2007). A tale of three cousins: Lasso, L2Boosting, and Danzig Annals of Statistics (invited discussion on Candes and Tao’s Danzig Selector paper) (paper)\nV. Vu, B. Yu, and R. Kass (2007). Coverage Adjusted Entropy Estimation. Statistics and Medicine, 26(21), 4039-4060. (paper)\nB. Yu (2007). Embracing Statistical Challenges in the Information Technology Age. Technometrics (special issue on statistics and information technologies). vol. 49 (3), 237-248. (paper)\nX. Jiang, Y. Liu, B. Yu and M. Jiang (2007). Comparison of MISR aerosol optical thickness with AERONET measurements in Beijing metropolitan area. Remote Sensing of Environment (Special Issue on Multi-angle Imaging SpectroRadiometer), vol. 107, pp. 45-53. (paper)\nT. Shi, E. E. Clothiaux, B. Yu, A. J. Braverman, and G. N. Groff (2007). Detection of Daytime Arctic Clouds using MISR and MODIS Data. Remote Sensing of Environment (Special Issue on Multi-angle Imaging SpectroRadiometer), vol. 107, pp. 172-184. (paper)\n\n\n\nPeng Zhao and Bin Yu (2006). On Model Selection Consistency of Lasso. J. Machine Learning Research, 7 (nov), 2541-2567. (paper)\nB. Yu (2006). Comments on: Monitoring networked applications with incremental quantile estimation by Chambers et al. Statist. Sci., 21, 483-485. (paper)\nB. Yu (2006). Comments on: Regularization in Statistics, by P. J. Bickel and B. Li. Test, vol. 15 (2), pages 314-316. (paper)\nP. Buhlmann and B. Yu (2006). Sparse Boosing Journal of Machine Learning Research ( 7 (June), 1001-1024). This is a shortened and more focused version of Buhlmann and Yu “Boosting, Model Selection, Lasso and Nonnegative Garotte” given below. (paper)\nJ. Gao, H. Suzuki, and B. Yu (2006). Approximation Lasso Methods for Language Modeling. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp. 225-232, Sydney. (paper)\n\n\n\nT. Shi and B. Yu (2005). Binning in Gaussian Kernel Regularization. Statistica Sinica (special issue on machine learning), 16, 541-567. (paper)\nG. Liang, N. Taft, and B. Yu (2005). A fast lightweight approach to origin-destination IP traffic estimation using partial measurements. Tech Report 687, Statistics Department, UCB (accepted for Special Issue of IEEE-IT and ACM Networks on data networks, Jan. 2006) (paper)\nTong Zhang and B. Yu (2005). Boosting with early stopping: convergence and consistency. The Annals of Statistics. Vol. 33, 1538-1579. (paper)\nCastro, M. Coates, G. Liang, R. Nowak, and B. Yu (2005) Network tomography: recent developments. Statistical Science, 19, 499-517. (paper)\nC. D. Giurcaneanu and B. Yu (2005). Efficient algorithms for discrete universal denoising for channels with memeory. Proceedings of International Symposium on Information Theory, Australia. (Also as Tech. Report 686, Statistics Department, UCB (Proc. ISIT, Sept. 2005)) (paper)\n\n\n\nP. Zhao and B. Yu (2004). Stagewise Lasso (old title: Boosted Lasso) Journal of Machine Learning Research, 8, 2701-2726. (An earlier version appeared as Tech. Report #678, Statistics Department, UC Berkeley (December, 2004; revised in April, 2005) (paper)\nD. J. Diner et al (2004). PARAGON: A Systematic, Integrated Approach to Aerosol Observation and Modeling. American Meterological Society, Oct., 1491-1501. (paper)\nP. Buhlmann and B. Yu (2004). Discussion on three boosting papers by Jiang, Lugosi and Vayatis, and Zhang Annals of Statistics. 32 (1): 96-101. (paper)\nR. Jorsten and B. Yu (2004). Compressing genomic and proteomic array images for statistical analyses. Invited chapter in a book on Genomic signal processing and statistics, edited by E. R. Dougherty, I. Shmulevich, J. Chen, and Z. J. Wang, pp. 341 - 366. (paper)\nG. Liang, B. Yu, and N. Taft (2004). Maximum entropy models: convergence rates and application in dynamic system monitoring. International Symposium on Information Theory, Chicago. (paper)\n\n\n\nR. Castro, M. Coates, G. Liang, R. Nowak, and B. Yu (2003). Internet Tomography: Recent Developments Statistical Science. Vol. 19(3), 499-517. (paper)\nG. Liang and B. Yu (2003). Maximum Pseudo Likelihood Estimation in Network Tomography. IEEE Trans. on Signal Processing (Special Issue on Data Networks). 51(8), 2043-2053 (paper)\nRebecka Jornsten and Bin Yu (2003). Simultaneous Gene Clustering and Subset Selection for Classification via MDL. Bioinformatics. 19(9): 1100-1109. (paper)\nPeter Buhlmann and Bin Yu (2003). Boosting with the L2 Loss: Regression and Classification. J. Amer. Statist. Assoc. 98, 324-340. (paper)\nR. Jornsten, W. Wang, B. Yu, and K. Ramchandran (2003). Microarray image compression: SLOCO and the effects of information loss. Signal Processing Journal (Special Issue on Genomic Signal Processing). 83, 859-869. (paper)\nG. Liang and B. Yu (2003). Pseudo Likelihood Estimation in Network Tomography. Proceedings of of Infocom, San Francisco. (paper)\n\n\n\nPeter Buhlmann and Bin Yu (2002). Analyzing Bagging. Annals of Statistics vol. 30, 927-961. (paper)\nR. Jornsten, M. Hansen, and B. Yu (2002). Adaptive Minimum Description Length (MDL) criteria with applications to microarray data. In Advances in Minimum Description Length: Theory and Applications, edited by P. Grunwald, I.J. Myung and M.A. Pitt. The MIT Press, pp. 295-321.\nMark Hansen and Bin Yu (2002). Minimum Description Length Model Selection Criteria for Generalized Linear Models.{}, IMS Lecture Notes – Monograph Series, Vol. 40. (paper)\nRebecka Jornsten, and Bin Yu (2002). Multiterminal Estimation: Extensions and a Geometric interpretation. Proceedings of International Symposium on Information Theory (ISIT), June, 2002. (paper)\nGerald Schuller, Bin Yu, Dawei Huang, and Bern Edler (2002). Perceptual Audio Coding using Pre- and Poster- Filters and Lossless Compression. IEEE Trans. Speech and Audio Processing. Vol. 10 (6), 379-390 (paper)\nMark Coates, Alfred Hero, Robert Nowak, and Bin Yu (2002). Internet Tomography. Signal Processing Magazine. vol. 19, No. 3 (May issue), 47-65. (paper)\n\n\n\nM. Hansen and B. Yu (2001). Model selection and the principle of Minimum Description Length. Journal of American Statistical Association. 96, 746-774. (paper)\n\n\n\nJin Cao, Drew Davis, Scott Vander Wiel and Bin Yu (2000). Time-varying network tomography: router link data. J. Amer. Statist. Assoc. vol. 95, 1063-1075. (PDF) (paper)\nPeter Buhlmann and Bin Yu (2000). Discussion. Additive logistic regression: a statistical view of boosting, by Friedman, J., Hastie, T. and Tibshirani, R. Annals of Statistics. Vol. 28, 377-386 (paper)\nMark Hansen and Bin Yu (2000). Wavelet thresholding via MDL for natural images. IEEE Trans. Inform. Theory (Special Issue on Information Theoretic Imaging). vol. 46, 1778-1788. (paper)\nJorma Rissanen and Bin Yu (2000). Coding and compression: a happy union of theory and practice. J. Amer. Statist. Assoc. (Year 2000 Commemorative Vignette on Engineering and Physical Sciences). vol. 95, 986-988. (paper)\nLei Li and Bin Yu (2000). Iterated logarithm expansions of the pathwise code lengths for exponential families. IEEE Trans. Inform. Theory. vol. 46, 2683-2689. (paper)\nG. Chang, B. Yu and M. Vetterli (2000). Adaptive wavelet thresholding for image denoising and compression. IEEE Trans. Image Processing, vol. 9, 1532-1546. (paper)\nG. Chang, B. Yu and M. Vetterli (2000). Spatially adaptive wavelet thresholding based on context modeling for image denoising. IEEE Trans. Image Processing, vol. 9, 1522-1531. (paper)\nG. Chang, B. Yu and M. Vetterli (2000). Wavelet thresholding for multiple noisy image copies. IEEE Trans. Image Processing, vol. 9, 1631-1635. (paper)\n\n\n\nY. Yoo, A. Ortega, and B. Yu (1999). Image subband coding using context-based classification and adaptive quantization. IEEE Trans. Image Processing, vol. 8, 1702-1215. (paper)\nB. Yu, M. Ostland, P. Gong and R. Pu (1999). Penalized discriminant analysis of in situ hyperspectral data for conifer species recognition. IEEE Trans. Geoscience and Remote Sensing, in press.\n\n\n\nA. Barron, J. Rissanen, and B. Yu (1998). The Minimum Description Length principle in coding and modeling. (Special Commemorative Issue: Information Theory: 1948-1998) IEEE. Trans. Inform. Th., 44, 2743-2760. Reprinted in Information 50 Years of Discovery, Theory: S. Verdu and S. McLaughlin (eds), IEEE Press , 1999.\nB. Yu and P. Mykland (1998). Looking at Markov samplers through cusum path plots: a simple diagnostic idea. Statistics and Computing , 8, 275-286.\nP. Gong, R. Pu and B. Yu (1998) Conifer species recognition: effects of data transformation and band width (in Chinese) Journal of Remote Sensing, 2(3), 211-217.\nG. Chang, B. Yu and M. Vetterli (1998). Spatially adaptive wavelet thresholding for image denoising. Proceedings of IEEE International Conference on Image Processing, October, Chicago. (paper)\nS. G. Chang, B. Yu, and M. Vetterli (1998). Image denoising via lossy compression and wavelet thresholding. Proceedings of International Conference on Image Processing. Santa Barbara, California, vol. 1, pp. 604-607. (paper)\nM. Ostland and B. Yu (1997). Exploring quasi Monte Carlo for marginal density approximation. Statistics and Computing, 7, 217-228. (paper)\nP. Gong, R. Pu, and B. Yu (1997). Conifer species recognition with in Situ hyperspectral data. Remote Sensing of Environment, 62, 189-200.\nB. Yu and T. P. Speed (1997). Information and the clone mapping of chromosomes. Ann. Statist. 25, 169-185. (paper)\n\n\n\nD. Nelson, T. Speed, and B. Yu (1997). The limits of random fingerprinting. Genomics, 40, 1-12.\nB. Yu (1997). Assouad, Fano, and Le Cam. Festschrift for Lucien Le Cam . D. Pollard, E. Torgersen, and G. Yang (eds), pp. 423-435, Springer-Verlag. (paper)\n\n\n\nB. Yu (1996). Lower bounds on expected redundancy for nonparametric classes. IEEE Trans. on Information Theory, 42, 272-275.\nY. Yoo, A. Ortega, and B. Yu (1996). Adaptive quantization of image subbands with efficient overhead rate selection. In Proceedings of IEEE International Conference on Image Processing, Lausanne, Switzerland.\nB. Yu (1996). A Statistical analysis of adaptive scalar quantization based on quantized past data. In Proceedings of International Symposium on Information Theory and its Applications (ISITA96), Victoria, Canada. (paper)\n\n\n\nB. Yu (1995). Comment: Extracting more diagnostic information from a single run using cusum path plot. Statist. Sci., 10, 54-58.\nJ. Rissanen and B. Yu (1995). MDL learning. In Learning and Geometry: Computational Approaches, Progress in Computer Science and Applied Logic, 14, David Kueker and Carl Smith (eds), Birkhäuser, Boston, pp. 3-19.\nP. Mykland, L. Tierney, and B. Yu (1995). Regeneration in Markov Chain samplers. J. Amer. Statist. Assoc., 90, 233-241.\n\n\n\nB. Yu (1994). Rates of convergence for empirical processes of stationary mixing sequences. Ann. Probab. 22, 94-116.\nM. Arcones and B. Yu (1994). Central limit theorems for empirical and U-processes of stationary mixing sequences. J. Theor. Probab. 7, 47-71.\nB. Yu (1994). Lower bound on the expected redundancy for classes of continuous Markov sources. In Statistical Decision Theory and Related Topics V, S. S. Gupta and J. O. Berger (eds), 453-466.\nM. Arcones and B. Yu (1994). Limit theorems for empirical processes under dependence. In Proceedings in Chaos expansions, multiple Wiener integrals and their applications. 205-221.\nA. R. Barron, Y. Yang and B. Yu (1994). Asymptotically optimal function estimation by minimum complexity criteria. In Proceedings of 1994 International Symposium on Information Theory, pp. 38, Trondheim, Norway.\n\n\n\nB. Yu and T. Speed (1993). A rate of convergence result for a universal D-semifaithful code. IEEE Trans. on Information Theory 39, 8813-820.\nB. Yu (1993). Density estimation in the L∞ norm for dependent data with applications to the Gibbs sampler. Ann. Statist. 21, 711-735.\nT. Speed and B. Yu (1993). Model selection and prediction: normal regression. J. Inst. Statist. Math. 45, 35-54.\n\n\n\nJ. Rissanen, T. Speed and B. Yu (1992). Density estimation by stochastic complexity. IEEE Trans. on Information Theory, 38, 315-323.\nB. Yu and T. Speed (1992) Data compression and histograms. Probability Theory and Related Fields, 92, 195-229."
  },
  {
    "objectID": "papers/index.html#section",
    "href": "papers/index.html#section",
    "title": "Papers",
    "section": "",
    "text": "A. R. Hsu, Y. Cherapanamjeri, A. Y. Odisho, P. R. Carroll, B. Yu (2024). Mechanistic Interpretation through Contextual Decomposition in Transformers. (paper)\nY. S. Tan, O. Ronen, T. Saarinen, B. Yu (2024). The Computational Curse of Big Data for Bayesian Additive Regression Trees: a Hitting Time Analysis. (paper)\nO. Ronen, A. I. Humayun, R. Balestriero, R. Baraniuk, B. Yu (2024). ScaLES: Scalable Latent Exploration Score for Pre-Trained Generative Networks. (paper)\nS. Hayou, N. Ghosh, B. Yu (2024). The Impact of Initialization on LoRA Fineuning Dynamics. (paper)\nB. Yu (2024). After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI. (Harvard Data Science Review) (paper)\nS. Hayou, N. Ghosh, B. Yu (2024). LoRA+: Efficient Low Rank Adaptation of Large Models. (Proc. ICML) (paper)\nC. F. Elliott, J. Duncan, T. M. Tang, M. Behr, K. Kumbier, B. Yu (2024). Designing a data science simulation with MERITS: a primer. (paper)\nY. Chen, C. Singh, X. Liu, S. Zuo, B. Yu, H. He, J. Gao (2024). Towards consistent natural-language explanations via explanation-consistent finetuning. (paper)\nN. R. Mallinar, A. Zane, S. Frei, B. Yu (2024). Minimum-Norm Interpolation Under Covariate Shift. (Proc. ICML) (paper)\nL. Sun, A. Agarwal, A. Kornblith, B. Yu, C. Xiong (2024). ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance. (Proc. ICML) (paper)"
  },
  {
    "objectID": "papers/index.html#section-1",
    "href": "papers/index.html#section-1",
    "title": "Papers",
    "section": "",
    "text": "C. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu, J. Gao (2023). Explaining black box text modules in natural language with language models. (paper)\nQ. Zhang, C. Singh, L. Liu, X. Liu, B. Yu, J. Gao, T. Zhao (2023). Tell your model where to attend: post-hoc attention steering for LLMs. (paper)\nQ. Wang, T. M. Tang, N. Youlton, C. S. Weldy, A. M. Kenney, O. Ronen, J. W. Hughes, E. T. Chin, S. C. Sutton, A. Agarwal, X. Li, M. Behr, K. Kumbier, C. S. Moravec, W. H. W. Tang, K. B. Margulies, T. P. Cappola, A. J. Buitte, R. Arnaout, J. B. Brown, J. R. Priest, V. N. Parikh, B. Yu, E. Ashley (2023). Epistasis regulates genetic control of cardiac hypertrophy. (paper) (code) (PCS documentation)\nR. Cahill, Y. Wang, R. P. Xian, A. J. Lee, H. Zeng, B. Yu, B. Tasic, R. Abbasi-Asl (2023). Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology. (paper) (code)\nE. Irajizad, A. Kenney, T. Tang, J. Vykoukal, R. Wu, E. Murage, J. B. Dennison, M. Sans, J. P. Long, M. Loftus, J. A. Chabot, M. D. Kluger, F. Kastrinos, L. Brais, A. Babic, K. Jajoo, L. S. Lee, T. E. Clancy, K. Ng, A. Bullock, J. M. Genkinger, A. Maitra, K. A. Do, B. Yu, B. W. Wolpin, S. Hanash, J. F. Fahrmann. (2023). A blood-based metabolomic signature predictive of risk for pancreatic cancer. Cell Reports Medicine 4(9): 101194. doi: 10.1016/j.xcrm.2023.101194. (PCS related) (paper) (Editorial)\nR. Dwivedi, C. Singh, B. Yu, M. Wainwright (2023). Revisiting miniumu description length complexity in overparametrized models. JMLR, 24(268): 1-59. (paper)\nK. Wu, Y. Chen, W. Ha, B. Yu (2023). Prominent roles of conditionally invariant components in domain adaptation: theory and algorithms. JMLR (accepted). (paper)\nN. Ghosh, S. Frei, W. Ha, B. Yu (2023). The effect of SGD batch size on autoencoder learning: sparsity, sharpness and feature learning. (paper)\nR. Netzorg, J. Li, B. Yu (2024). Improving prototypical part networks with reward reweighting, reselection, and retraining. Proc. ICML. (paper)\nA. Agarwal, A. M. Kenny, Y. S. Tan, T. M. Tang, B. Yu (2023). MDI+: a flexible random forest-based feature importance framework. (PCS related) (paper)\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. Proc. ICLR. (paper)\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu and J. Gao (2023). Explaining black box text modules in natural language with language models. (paper)"
  },
  {
    "objectID": "papers/index.html#section-2",
    "href": "papers/index.html#section-2",
    "title": "Papers",
    "section": "",
    "text": "D. Shen, P. Ding, J. Sekhon, B. Yu (2022). Same root different leaves: time series and cross-sectional methods in panel data. Econometrica (accepted). (paper)\nB. Park, X. Wu, B. Yu, A. Zhou (2022). Offline evaluation in RL: soft stability weighting to combine fitted Q-learning and model-based methods. NeurIPS 3rd Offline Reinforcement Learning Workshop. (paper)\nY. S. Tan, C. Singh, K. Nasseri, A. Agarwal, J. Duncan, O. Ronen, M. Epland, A. Kornblith, B. Yu (2022). Fast interpretable greedy-tree sums (FIGS). (paper) (code)\nA. Agarwal, Y. S. Tan, O. Ronen, C. Singh, B. Yu (2022). Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods. Proc. ICML (paper) (code).\nB. Yu and C. Singh (2022). Seven principles for rapid-response data science: lessons learned from covid-19 forecasting. Statistical Science, 36(2):266-269. (paper)\nN. Ghosh, S. Mei, and B. Yu (2022). The three stages of dynamics in high-dimensional kernel methods. Proc. ICLR, 2022. (paper)"
  },
  {
    "objectID": "papers/index.html#section-3",
    "href": "papers/index.html#section-3",
    "title": "Papers",
    "section": "",
    "text": "Y. Tan, A. Agarwal, and B. Yu (2021). A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds. Proc. AISTATS. (paper)\nN. Altieri, B. Park, J. DeNero, A. Odisho, B. Yu. (2021). Improving natural language information extraction from cancer pathology reports using transfer learning and zero-shot string similarity. JAMIA Open. 2021 Sept. 30 4(3). (paper)\nC. Singh, W. Ha and B. Yu (2021). Interpreting and Improving Deep-Learning Models with Reality Checks. To appear in “xxAI - Beyond Explainable AI” (eds. Holzinger et al.). (paper)\nW. Ha, C. Singh, F. Lanusse, S. Upadhyayula, and B. Yu (2021). Adaptive Wavelet Distillation from Neural Networks through Interpretation. Proc. NeurIPS 2021. (paper) (code)\nM. Behr, Y. Wang, X. Li, B. Yu (2022). Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests. PNAS. (PCS-related) (paper) (theory for iRF)\nN. Altieri, B. Park, M. Olson, J. DeNero, A. Odisho, B. Yu. (2021). Supervised line attention for tumor attribute classification from pathology reports: Higher performance with less data. Journal of Biomedical Informatics. 122 (2021) 103872. (paper)"
  },
  {
    "objectID": "papers/index.html#section-4",
    "href": "papers/index.html#section-4",
    "title": "Papers",
    "section": "",
    "text": "M. Behr, K. Kumbier, A. Cordova-Palomera, M. Aguirre, E. Ashley, A. Butte, R. Arnaout, J. B. Brown, J. Preist, B. Yu (2020). Learning epistatic polygenic phenotypes with Boolean interactions. (PCS inference case study) (paper) (code)\nB. Norgeot, G. Quer, B. K. Beaulieu-Jones, A. Torkamani, R. Dias, M. Gianfrancesco, R. Arnaout, I. S. Kohane, S. Saria, E. Topol, Z. Obermeyer, B. Yu & A. Butte (2020). Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist, Nature Medicine, 26, 1320–1324. (paper)\nB. Yu (2020). Stability expanded, in reality. Harvard Data Science Review (PCS related) (paper)\nB. Yu and R. Barter (2020). Data science process: one culture. JASA. (PCS related) (paper)\nR. Dwivedi, Y. Tan, B. Park, M. Wei, K. Horgan, D. Madigan, B. Yu (2020). Stable discovery of interpretable subgroups via calibration in causal studies (staDISC). International Statistical Review (PCS case study for causal inference) (paper) (code)\nX. Li, T. M. Tang, X. Wang, J. A. Kocher, B. Yu (2020). A stability-driven protocol for drug response interpretable prediction (staDRIP). NeurISP workshop on ML4H (Machine learning for Health) Extended Abstract. (paper)\nA. Y. Odisho, B. Park, N. Altieri, J. DeNero, M. R Cooperberg, P. R .Carroll, B. Yu (2020). Natural language processing systems for pathology parsing in limited data environments with uncertainty estimation. JAMIA Open. (paper)\nL. Reiger, J. W. Murdoch, S. Singh, B. Yu (2020). Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge. ICML Proceedings. (paper) (code)\nC. Singh, W. Ha, F. Lanusse, V. Boehm, J. Liu, B. Yu (2020). Transformation Importance with Applications to Cosmology. ICLR Workshop paper. (paper) (code)\nN. Altieri, R. Barter, J. Duncan, R. Dwivedi, K. Kumbier, X. Li, R. Netzorg, B. Park, C. Singh, Y. Tan, T.Tang, Y. Wang, C. Zhang, B. Yu. (2020) Curating a COVID-19 data repository and forecasting county-level death counts in the United States. Harvard Data Science Review (paper) (code) (7-day prediction results) (Short talk video)\nB. Yu and K. Kumbier (2020) Veridical data science (PCS framework), PNAS. 117 (8), 3920-3929. (paper) (QnAs with Bin Yu)\nR. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan and B. Yu (2020) Sharp Analysis of Expectation-Maximization for Weakly Identifiable Mixture Models AISTATS. (paper)\nR. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan and B. Yu (2020) Singularity, Misspecification and the Convergence Rate of EM Annals of Statistics. (paper)\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Fast Mixing of Metropolized Hamiltonian Monte Carlo: Benefits of Multi-Step Gradients. JMLR, (paper) (arXiv)"
  },
  {
    "objectID": "papers/index.html#section-5",
    "href": "papers/index.html#section-5",
    "title": "Papers",
    "section": "",
    "text": "R. Dwivedi, Y. Chen, M. J. Wainwright and B. Yu (2019) Log-concave Sampling: Metropolis Hastings Algorithms are Fast JMLR. (paper)\nD. Rothenhäusler and B. Yu (2019). Incremental causal effects. (paper)"
  },
  {
    "objectID": "papers/index.html#section-6",
    "href": "papers/index.html#section-6",
    "title": "Papers",
    "section": "",
    "text": "Y. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2018) Fast MCMC Algorithms on Polytopes. JMLR.\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Vaidya Walk: A Sampling Algorithm Based on Volumetric-Logarithmic Barrier. Allerton Conference 2017. (paper)\nW. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu* (2019) Definitions, methods, and applications in interpretable machine learning. PNAS, 116 (44) 22071-22080. (paper)\nW. J. Murdoch, C. Sign, and B. Yu (2019). Hierarchical interpretations for neural network predictions. ICLR. (paper) (code)\nY. Wang, S. Wu and B. Yu (2020) Unique Sharp Local Minimum in l1-minimization Complete Dictionary Learning. JMLR. 21(63), pp. 1-52. (paper) (arXiv)\nY. Chen, R. Abbasi-Asl, A. Bloniarz, M. Oliver, B. Willmore, J. Gallant, and B. Yu (2018) The DeepTune framework for modeling and characterizing neurons in visual cortex area V4. (paper)\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. (an enhanced version of iRF, PCS related) (paper) (iRF)\nY. Chen C. Jin, and B. Yu (2018) Stability and Convergence Trade-off of Iterative Optimization Algorithms. (paper)\nJ. Murdoch, P. Liu, and B. Yu (2018) Beyond word importance: contextual decomposition to extract interactions from LSTMs. Proc. ICLR 2018. (paper) (code)\nR. Diwivedi, Y. Chen, M. J. Wainwright, and B. Yu (2018) Log-concave sampling: Metropolis-Hastings algorithms are fast! (paper)"
  },
  {
    "objectID": "papers/index.html#section-7",
    "href": "papers/index.html#section-7",
    "title": "Papers",
    "section": "",
    "text": "Y. Chen, R. Dwivedi, M. J. Wainwright, and B. Yu (2017) Fast MCMC sampling algorithms on polytopes. (paper)\nB. Yu and K. Kumbier (2018) Artificial Intelligence and Statistics. Frontiers of Information Technology and Electronic Engineering. 19(1), 6-9. (paper)\nR. Abbasi-Asl and B. Yu (2017) Structural Compression of Convolutional Neural Networks Based on Greedy Filter Pruning. (paper)\nR. Abbasi-Asl and B. Yu (2017) Interpreting Convolutional Neural Networks Through Compression. NIPS 2017. Symposium on Interpretable Machine Learning. (paper)\nS. Kunzel, J. Sekhon, P. Bickel, and B. Yu* (2019) Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning. PNAS. 116 (10) 4156-4165. (paper) (arXiv) (code)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (PCS related) (paper) (code)\nS. Balakrishnan, M. Wainwright, B. Yu (2017) Statistical Guarantees for the EM algorithm: from population to sample-based analysis. Annals of Statistics, 45(1), 77 - 120. (paper)\nR. Barter and B. Yu (2017) Superheat: An R package for creating beautiful and extendable heatmaps for visualizing complex data. JCGS (revised). (paper) (code)\nH. Liu and B. Yu (2017) Comments on: High-dimensional simultaneous inference with the bootstrap by Dezeure et al Test. 26: 740-750. (paper)\nC. Carson et al (2016). UC Berkeley Data Science Planning Initiative Faculty Advisory Board (FAB) Report. (paper) (FAB Report Executive Summary)\nS. Wu and B. Yu (2018). Local identifiability of l1-minimization dictionary learning: a sufficient and almost necessary condition. JMLR. 18, 1 - 56. (paper)"
  },
  {
    "objectID": "papers/index.html#section-8",
    "href": "papers/index.html#section-8",
    "title": "Papers",
    "section": "",
    "text": "K. Rohe, T. Qin and B. Yu* (2016). Co-clustering directed graphs to discover asymmetries and directional communities. Proc. National Academy of Sciences (PNAS), 113(45), 12679 - 12684. (paper)\nR. E. Kass, B. S. Caffo, M. Davidian, X. Meng, B. Yu, Nancy Reid* (2016). Ten simple rules for effective statistical practice. PLoS Comput. Biol., 12(6): e1004961. doi:10.1371/journal.pcbi.1004961 (paper)\nSiqi Wu, Antony Joseph, Ann S. Hammonds, Susan E. Celniker, Bin Yu, and Erwin Frise (2016). Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks (with support information). PNAS, pp. 4290 - 4295. (PCS related) (paper) (code)\nA. Bloniarz, C. Wu, B. Yu, A. Talwalkar (2016). Supervised neighborhoods for distributed nonparametric regression. Proc. of AISTATS, Barcelona, Spain. (paper)"
  },
  {
    "objectID": "papers/index.html#section-9",
    "href": "papers/index.html#section-9",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (2015). Data wisdom for data science. Operational Database Management Systems (ODBMS.ORG). (paper)\nA. Bloniarz, H. Liu, C. Zhang, J. Sekhon, and B. Yu* (2015). Lasso adjustments of treatment effect estimates in randomized experiments. PNAS. 113, 7383 - 7390. (paper)\nP. Ma, M. W. Mahoney and B. Yu (2015). A Statistical Perspective on Algorithmic Leveraging. Journal of Machine Learning Research, 16, (2015), 861-911. (paper)\nT. Moon, Y. Wang, Y. Liu, and B. Yu (2015). Evaluation of a MISR-based high-resolution aerosol retrieval method using AERONET DRAGON campaign data. IEEE Transactions on Geoscience and Remote Sensing, 53, 4328-4339. (paper)"
  },
  {
    "objectID": "papers/index.html#section-10",
    "href": "papers/index.html#section-10",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (2014). Let us own data science. Institute of Mathematical Statistics (IMS) Presidental Address, ASC-IMS Joint Conference, Sydney, July, 2014. (paper) (video) (IMS Bulletin)\nG. Schiebinger, M. J. Wainwright and B. Yu (2014). The geometry of kernelized spectral clustering. Annals of Statistics, 43, 819-846. (paper)\nL. Miratrix, J. Jia, B. Yu, B. Gawalt, L. El Ghaoui, L. Barnesmoore, S. Clavier (2014). Concise comparative summaries (CCS) of large text corpora with a human experiment. Ann. Applied Statist., 8, 499-529. (paper)\nY. Benjamini and B. Yu (2014). The shuttle estimator for explainable variance in fMRI experiments. Annals of Applied Statistics, 7, 2007-2033. (paper)\nD. Bean, P. Bickel, N. El Karoui and B. Yu (2014). Optimal M-estimation in high-dimensional regression. Proceedings of National Academy of Sciences, 110, 1456314568. (paper)\nN. El Karoui, D. Bean, P. Bickel, C. Lim, and B. Yu (2014). On robust regression with high-dimensional predictors. Proceedings of National Academy of Sciences, 110, 1455714562. (paper)\nP. Ma, M. W. Mahoney, B. Yu (2014). A Statistical Perspective on Algorithmic Leveraging. Proc. of International Conference on Machine Learning (ICML) (This conference paper contains some of preliminary results of the journal submission Ma et al. (2015)) (paper)\nA. Bloniarz, A. Talwalkar, J. Terhorst, M. Jordan, D. Patterson, B. Yu and Y. Song (2014). Changepoint Analysis for Efficient Variant Calling. Proc. of RECOMB 2014 (to appear). (paper)\nTao Shi (2013), A conversation with Professor Bin Yu International Chinese Statistical Association (ICSA) Bulletin, Vol 25, Issue 2, pp 85-98. (paper) (Selected Parts in Statblogs)\nA. Joseph and B. Yu (2016). The impact of regularization on spectral clustering. Annals of Statistics. 4, 1765 - 1791. (paper)\nC. Lim and B. Yu (2016). Estimation Stability with Cross Validation (ESCV) Journal of Computational and Graphical Statistics. 25, 464 - 492. (First paper towards PCS) (paper)"
  },
  {
    "objectID": "papers/index.html#section-11",
    "href": "papers/index.html#section-11",
    "title": "Papers",
    "section": "",
    "text": "A. S. Hammonds, C. A. Bristow, W. W. Fisher, R. Weiszmann, S. Wu, V. Hartenstein, M. Kellis, B. Yu, E. Frise, and S. E. Celniker (2013). Spatial expression of transcription factors in Drosophila embryonic organ development. Genome Biology, 14(12), R140. (paper)\nH. Liu and B. Yu (2013). Asymptotic properties of Lasso+mLS and Lasso+Ridge in sparse high-dimensional linear regression. Electron. J. Statist., 7, 312-3169. (paper)\nJ. Mairal and B. Yu (2013). Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows. Journal of Machine Learning Research, 14, 2449-2485. (paper)\nY. Wang, X. Jiang, B. Yu, M. Jiang (2013). A Hierarchical Bayesian Approach for Aerosol Retrieval Using MISR Data. J. American Statistical Association, 108, 483-493. (paper)\nY. He, J. Jia and B. Yu (2013). Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs. Annals of Statistics, 41(4), 1742-1779. (paper)\nB. Yu (2013). Stability. Bernoulli, 19 (4), 1484-1500. (Invited paper for the Special Issue commemorating the 300th anniversary of the publication of Jakob Bernoullis Ars Conjectandi in 1712) (Begining of PCS) (paper)\nJ. Mairal and B. Yu (2013). Discussion on Grouping Strategies and Thresholding for High Dimensional Linear Models Journal of Statistical Planning and Inference, 143, 1451-1453.\nC. Uhler, G. Raskutti, and P. Buhlmann and B. Yu (2013). Geometry of faithfulness assumption in causal inference. Annals of Statistics, 41, 436-463. (paper)\nL. Miratrix, J. Sehkon, and B. Yu (2013). Adjusting Treatment Effect Estimates by Post-Stratification in Randomized Experiments. Journal of Royal Statistical Society, Series B, 75 (part 2), 369-396. (paper)\nJ. Jia, K. Rohe and B. Yu (2013) The Lasso under Poisson-like Heteroscadecity. Statistica Sinica, 23, 99-118. (paper)\nS. Negahban, P. Ravikumar, M. Wainwrigt, and B. Yu (2012) A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. Statistical Science, 27, 538-557. (paper)\nG. Raskutti, M. Wainwrigt, and B. Yu (2012) Minimax-optimal rates for sparse additive models over kernel classes via convex programming. J. Machine Learning Research, 13, 389-427. (paper)\nJ. Mairal and B. Yu (2012). Complexity analysis of the Lasso regularization path. Proc. of International Conference on Machine Learning (ICML). (paper)"
  },
  {
    "objectID": "papers/index.html#section-12",
    "href": "papers/index.html#section-12",
    "title": "Papers",
    "section": "",
    "text": "Yanfeng Gu, Shizhe Wang, Tao Shi, Yinghui Lu, Eugene E. Clothiaux, and Bin Yu (2012). Multiple-kernel learning-based unmixing algorithm for estimation of cloud fractions with MODIS and CLOUDSAT data. Proc. of IEEE International Geoscience and Remote Sensing Symposium (IGRSS). (paper)"
  },
  {
    "objectID": "papers/index.html#section-13",
    "href": "papers/index.html#section-13",
    "title": "Papers",
    "section": "",
    "text": "S. Nishimoto, A. T. Vu, T. Naselaris, Y. Benjamini, B. Yu, J. L. Gallant (2011). Reconstructing visual experiences from brain activity evoked by natural movies. Current Biology, 21(19), 1641-1646. (paper) (related videos)\nP. Ravikumar, M. Wainwright, G. Raskutti, B. Yu (2011). High-dimensional covariance estimation by minimizing l1-penalized log-determinant divergence. Electronic Journal of Statistics, 5, 935-980. (paper)\nG. Raskutti, M. Wainwright, B. Yu (2011). Minimax rates of estimation for high-dimensional linear regression over lq-balls. IEEE Trans. Inform. Th., 57(10), 6976-6994. (paper)\nK. Rohe, S. Chatterjee, and B. Yu (2011). Spectral clustering and the high-dimensional Stochastic Block Model. Annals of Statistics, 39 (4), 1878-1915 (paper)\nV. Q. Vu, P. Ravikumar, T. Naselaris, K. N. Kay, J. L. Gallant, B. Yu* (2011). Encoding and decoding V1 fMRI responses to natural images with sparse nonparametric models. Annals of Applied Statistics, 5, 1150-1182. (*First senior author as last author in biology tradition) (paper)\nS. N. Pakzad, G. Rocha, and B. Yu (2011). Distributed modal identification by regularized auto regressive models. International Journal of Systems Science, 42, 1473-1489.\nJ. Yousafzai, P. Sollich, Z. Cvetkovic, and B. Yu (2011). Combined Features and Kernel Design for Robust Phoneme Classification Using Support Vector Machines. IEEE Trans. Audio, Speech and Language Processing (to appear). 64. (paper)\nX. Dai, J. Jia, B. Yu, El Ghaoui (2011) SBA-term: Sparse Bilingual Association for terms. Proc. International Conference on Semantic Computing. (paper)\nB. Yu (2011). Asymptotics and Coding Theory: One of the n - 1 Dimensions of Terry. In Selected Works of Terry Speed (ed. S. Duoit), pp. 33-36, Springer. (paper)"
  },
  {
    "objectID": "papers/index.html#section-14",
    "href": "papers/index.html#section-14",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (2010). Remembering Leo. Annals of Applied Statistics, 4(4), 1657-1659. (paper)\nJ. Jia, Y. Benjamini, C. Lim, G. Raskutti, B. Yu (2010). Comment on “Envelope models for parsimonious and efficient multivariate linear regression” by R. D. Cook, B. Li, and F. Chiaromonte. Statistica Sinica, 20, 961-967. (paper)\nG. Raskutti, M. Wainwrigt, and B. Yu (2010) Restricted Eigenvalue Properties for Correlated Gaussian Designs. Journal of Machine Learning Research, 11, 2241-2259. (paper)\nJ. Jia and B. Yu (2010). On model selection consistency of elastic net when p &gt;&gt;n. Statistica Sinica, 10, 595-611. (paper)\nP. Buhlmann and B. Yu (2010). Boosting. Wiley Interdisciplinary Reviews: Computational Statistics, 2, 69-74. (paper)\nL. Huang, J. Jia, B. Yu, B. Chun, P. Maniatis, M. Naik (2010). Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression. Proc. NIPS 2010. (paper)\nY. Han, F. Wu, J. Jia, Y. Zhuang and B. Yu (2010). Multi-task Sparse Discriminant Analysis (MtSDA) with Overlapping Categories. Proc. of The 24th AAAI Conference on Artificial Intelligence, July 11-15, Atlanta, GA. (paper)\nB. Gawalt, J. Jia, L. Miratrix, L. El Ghaoui, B. Yu, and S. Clavier (2010). Discovering Word Associations in News Media via Feature Selection and Sparse Classification. Proc. 11th ACM SIGMM International Confernece on Multimedia Information Retrieval (MIR). (paper)"
  },
  {
    "objectID": "papers/index.html#section-15",
    "href": "papers/index.html#section-15",
    "title": "Papers",
    "section": "",
    "text": "E. Anderes, B. Yu, V. Jovanovic, C. Moroney, M. Garay, A. Braverman, E. Clothiaux (2009) Maximum Likelihood Estimation of Cloud Height from Multi-Angle Satellite Imagery. Annals of Applied Statistics, 3, 902-921 (paper)\nT. Shi, M. Belkin, and B. Yu, (2009) Data Spectroscopy: Eigenspace of Convolution Operator and Clustering Annals of Statistics, 37 (6B), 3960-3984. (paper)\nVincent Q. Vu, Bin Yu, Robert E. Kass (2009) Information In The Non-Stationary Case Neural computation 21, 688-703. (paper)\nN. Meinshausen and B. Yu (2009). Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics 37, 246-270. (paper)\nP. Zhao, G. Rocha, and B. Yu (2009). The composite absolute penalties family for grouped and hierarchical variable selection Annals of Statistics 37, 3468-3497. (An earlier version ‘appeared as Grouped and hierarchical model selection through composite absolute penalties’ by P. Zhao, G. Rocha and B. Yu, Department of Statistics, UC Berkeley, Tech. Rep 703.) (paper)\nS. Negahban, P. Ravikumar, M. Wainwright, and B. Yu (2009). A unified framework for high-dimensional analysis of \\(M\\)-estimators with decomposable regularizers Proc. NIPS, 2009. (This conference paper contains preliminary results of the journal submission Negahban et al. 2012). (paper)\nG. Raskutti, M. Wainwright, B. Yu (2009) High-dimensional regression under lq-ball sparsity: Optimal rates of convergence. Proc. of Allerton Conference on Communication, Control, and Computing. (This conference paper contains some of preliminary results of the journal submission Ravikumar et al. 2011). (paper)\nG. Raskutti, M. Wainwrigt, and B. Yu (2009) Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness. Proc. NIPS, 2009. (This conference paper contains some of preliminary results of the journal submission Ravikumar et al. 2011). (paper)"
  },
  {
    "objectID": "papers/index.html#section-16",
    "href": "papers/index.html#section-16",
    "title": "Papers",
    "section": "",
    "text": "T. Shi, B. Yu, E. Clothiaux, and A. Braverman (2008). Daytime Arctic Cloud Detection based on Multi-angle Satellite Data with Case Studies. Journal of American Statistical Association. 103( 482), 584-593. (paper)\nPeter Buhlmann and Bin Yu (2008) Invited discussion on “Evidence contrary to the statistical view of boosting (D. Mease and A. Wyner)”. Journal of Machine Learning Research 9, 187-194. (paper with discussion)\nP. Ravikumer, V. Vu, B. Yu, T. Naselaris, K. Kay, J. Gallant (2008). Nonparametric sparse hiearchical models describe V1 fMRI responses to natural images In Adavances in Neural Information Processing Systems (NIPS) 21, (2008). (This conference paper contains some preliminary results of journal paper Vu et al. (2011) on encoding models, but also contains an encoding model that is not in Vu et al. (2011). It does not contain decoding results.) (paper)\nP. Ravikumar, G. Raskutti, M. Wainwright, B. Yu (2008) Model selection in Gaussian graphical models: high-dimensional consistency of l1-regularized MLE. In Adavances in Neural Information Processing Systems (NIPS) 21, (2008). (paper)\nT. Shi, M. Belkin, and B. Yu (2008). Data spectroscopy: learning mixture models using eigenspaces of convolution operators. Proc. of ICML 2008. (paper)\nM. Ager, Z. Cvetkovic, P. Pollich, and B. Yu (2008). Towards Robust Phoneme Classification Augmentation of PLP Models with Acoustic Waveforms. Proceedings of EUSIPCO. (paper)\nJ. Yousafzai, Z. Cvetković, P. Pollich, and B. Yu (2008). Combined PLP-Acoustic Waveform Classification for Robust Phoneme Recognition using Support Vector Machines. Proceedings of EUSIPCO. (paper)"
  },
  {
    "objectID": "papers/index.html#section-17",
    "href": "papers/index.html#section-17",
    "title": "Papers",
    "section": "",
    "text": "N. Meinshausen, G. Rocha, and B. Yu (2007). A tale of three cousins: Lasso, L2Boosting, and Danzig Annals of Statistics (invited discussion on Candes and Tao’s Danzig Selector paper) (paper)\nV. Vu, B. Yu, and R. Kass (2007). Coverage Adjusted Entropy Estimation. Statistics and Medicine, 26(21), 4039-4060. (paper)\nB. Yu (2007). Embracing Statistical Challenges in the Information Technology Age. Technometrics (special issue on statistics and information technologies). vol. 49 (3), 237-248. (paper)\nX. Jiang, Y. Liu, B. Yu and M. Jiang (2007). Comparison of MISR aerosol optical thickness with AERONET measurements in Beijing metropolitan area. Remote Sensing of Environment (Special Issue on Multi-angle Imaging SpectroRadiometer), vol. 107, pp. 45-53. (paper)\nT. Shi, E. E. Clothiaux, B. Yu, A. J. Braverman, and G. N. Groff (2007). Detection of Daytime Arctic Clouds using MISR and MODIS Data. Remote Sensing of Environment (Special Issue on Multi-angle Imaging SpectroRadiometer), vol. 107, pp. 172-184. (paper)"
  },
  {
    "objectID": "papers/index.html#section-18",
    "href": "papers/index.html#section-18",
    "title": "Papers",
    "section": "",
    "text": "Peng Zhao and Bin Yu (2006). On Model Selection Consistency of Lasso. J. Machine Learning Research, 7 (nov), 2541-2567. (paper)\nB. Yu (2006). Comments on: Monitoring networked applications with incremental quantile estimation by Chambers et al. Statist. Sci., 21, 483-485. (paper)\nB. Yu (2006). Comments on: Regularization in Statistics, by P. J. Bickel and B. Li. Test, vol. 15 (2), pages 314-316. (paper)\nP. Buhlmann and B. Yu (2006). Sparse Boosing Journal of Machine Learning Research ( 7 (June), 1001-1024). This is a shortened and more focused version of Buhlmann and Yu “Boosting, Model Selection, Lasso and Nonnegative Garotte” given below. (paper)\nJ. Gao, H. Suzuki, and B. Yu (2006). Approximation Lasso Methods for Language Modeling. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp. 225-232, Sydney. (paper)"
  },
  {
    "objectID": "papers/index.html#section-19",
    "href": "papers/index.html#section-19",
    "title": "Papers",
    "section": "",
    "text": "T. Shi and B. Yu (2005). Binning in Gaussian Kernel Regularization. Statistica Sinica (special issue on machine learning), 16, 541-567. (paper)\nG. Liang, N. Taft, and B. Yu (2005). A fast lightweight approach to origin-destination IP traffic estimation using partial measurements. Tech Report 687, Statistics Department, UCB (accepted for Special Issue of IEEE-IT and ACM Networks on data networks, Jan. 2006) (paper)\nTong Zhang and B. Yu (2005). Boosting with early stopping: convergence and consistency. The Annals of Statistics. Vol. 33, 1538-1579. (paper)\nCastro, M. Coates, G. Liang, R. Nowak, and B. Yu (2005) Network tomography: recent developments. Statistical Science, 19, 499-517. (paper)\nC. D. Giurcaneanu and B. Yu (2005). Efficient algorithms for discrete universal denoising for channels with memeory. Proceedings of International Symposium on Information Theory, Australia. (Also as Tech. Report 686, Statistics Department, UCB (Proc. ISIT, Sept. 2005)) (paper)"
  },
  {
    "objectID": "papers/index.html#section-20",
    "href": "papers/index.html#section-20",
    "title": "Papers",
    "section": "",
    "text": "P. Zhao and B. Yu (2004). Stagewise Lasso (old title: Boosted Lasso) Journal of Machine Learning Research, 8, 2701-2726. (An earlier version appeared as Tech. Report #678, Statistics Department, UC Berkeley (December, 2004; revised in April, 2005) (paper)\nD. J. Diner et al (2004). PARAGON: A Systematic, Integrated Approach to Aerosol Observation and Modeling. American Meterological Society, Oct., 1491-1501. (paper)\nP. Buhlmann and B. Yu (2004). Discussion on three boosting papers by Jiang, Lugosi and Vayatis, and Zhang Annals of Statistics. 32 (1): 96-101. (paper)\nR. Jorsten and B. Yu (2004). Compressing genomic and proteomic array images for statistical analyses. Invited chapter in a book on Genomic signal processing and statistics, edited by E. R. Dougherty, I. Shmulevich, J. Chen, and Z. J. Wang, pp. 341 - 366. (paper)\nG. Liang, B. Yu, and N. Taft (2004). Maximum entropy models: convergence rates and application in dynamic system monitoring. International Symposium on Information Theory, Chicago. (paper)"
  },
  {
    "objectID": "papers/index.html#section-21",
    "href": "papers/index.html#section-21",
    "title": "Papers",
    "section": "",
    "text": "R. Castro, M. Coates, G. Liang, R. Nowak, and B. Yu (2003). Internet Tomography: Recent Developments Statistical Science. Vol. 19(3), 499-517. (paper)\nG. Liang and B. Yu (2003). Maximum Pseudo Likelihood Estimation in Network Tomography. IEEE Trans. on Signal Processing (Special Issue on Data Networks). 51(8), 2043-2053 (paper)\nRebecka Jornsten and Bin Yu (2003). Simultaneous Gene Clustering and Subset Selection for Classification via MDL. Bioinformatics. 19(9): 1100-1109. (paper)\nPeter Buhlmann and Bin Yu (2003). Boosting with the L2 Loss: Regression and Classification. J. Amer. Statist. Assoc. 98, 324-340. (paper)\nR. Jornsten, W. Wang, B. Yu, and K. Ramchandran (2003). Microarray image compression: SLOCO and the effects of information loss. Signal Processing Journal (Special Issue on Genomic Signal Processing). 83, 859-869. (paper)\nG. Liang and B. Yu (2003). Pseudo Likelihood Estimation in Network Tomography. Proceedings of of Infocom, San Francisco. (paper)"
  },
  {
    "objectID": "papers/index.html#section-22",
    "href": "papers/index.html#section-22",
    "title": "Papers",
    "section": "",
    "text": "Peter Buhlmann and Bin Yu (2002). Analyzing Bagging. Annals of Statistics vol. 30, 927-961. (paper)\nR. Jornsten, M. Hansen, and B. Yu (2002). Adaptive Minimum Description Length (MDL) criteria with applications to microarray data. In Advances in Minimum Description Length: Theory and Applications, edited by P. Grunwald, I.J. Myung and M.A. Pitt. The MIT Press, pp. 295-321.\nMark Hansen and Bin Yu (2002). Minimum Description Length Model Selection Criteria for Generalized Linear Models.{}, IMS Lecture Notes – Monograph Series, Vol. 40. (paper)\nRebecka Jornsten, and Bin Yu (2002). Multiterminal Estimation: Extensions and a Geometric interpretation. Proceedings of International Symposium on Information Theory (ISIT), June, 2002. (paper)\nGerald Schuller, Bin Yu, Dawei Huang, and Bern Edler (2002). Perceptual Audio Coding using Pre- and Poster- Filters and Lossless Compression. IEEE Trans. Speech and Audio Processing. Vol. 10 (6), 379-390 (paper)\nMark Coates, Alfred Hero, Robert Nowak, and Bin Yu (2002). Internet Tomography. Signal Processing Magazine. vol. 19, No. 3 (May issue), 47-65. (paper)"
  },
  {
    "objectID": "papers/index.html#section-23",
    "href": "papers/index.html#section-23",
    "title": "Papers",
    "section": "",
    "text": "M. Hansen and B. Yu (2001). Model selection and the principle of Minimum Description Length. Journal of American Statistical Association. 96, 746-774. (paper)"
  },
  {
    "objectID": "papers/index.html#section-24",
    "href": "papers/index.html#section-24",
    "title": "Papers",
    "section": "",
    "text": "Jin Cao, Drew Davis, Scott Vander Wiel and Bin Yu (2000). Time-varying network tomography: router link data. J. Amer. Statist. Assoc. vol. 95, 1063-1075. (PDF) (paper)\nPeter Buhlmann and Bin Yu (2000). Discussion. Additive logistic regression: a statistical view of boosting, by Friedman, J., Hastie, T. and Tibshirani, R. Annals of Statistics. Vol. 28, 377-386 (paper)\nMark Hansen and Bin Yu (2000). Wavelet thresholding via MDL for natural images. IEEE Trans. Inform. Theory (Special Issue on Information Theoretic Imaging). vol. 46, 1778-1788. (paper)\nJorma Rissanen and Bin Yu (2000). Coding and compression: a happy union of theory and practice. J. Amer. Statist. Assoc. (Year 2000 Commemorative Vignette on Engineering and Physical Sciences). vol. 95, 986-988. (paper)\nLei Li and Bin Yu (2000). Iterated logarithm expansions of the pathwise code lengths for exponential families. IEEE Trans. Inform. Theory. vol. 46, 2683-2689. (paper)\nG. Chang, B. Yu and M. Vetterli (2000). Adaptive wavelet thresholding for image denoising and compression. IEEE Trans. Image Processing, vol. 9, 1532-1546. (paper)\nG. Chang, B. Yu and M. Vetterli (2000). Spatially adaptive wavelet thresholding based on context modeling for image denoising. IEEE Trans. Image Processing, vol. 9, 1522-1531. (paper)\nG. Chang, B. Yu and M. Vetterli (2000). Wavelet thresholding for multiple noisy image copies. IEEE Trans. Image Processing, vol. 9, 1631-1635. (paper)"
  },
  {
    "objectID": "papers/index.html#section-25",
    "href": "papers/index.html#section-25",
    "title": "Papers",
    "section": "",
    "text": "Y. Yoo, A. Ortega, and B. Yu (1999). Image subband coding using context-based classification and adaptive quantization. IEEE Trans. Image Processing, vol. 8, 1702-1215. (paper)\nB. Yu, M. Ostland, P. Gong and R. Pu (1999). Penalized discriminant analysis of in situ hyperspectral data for conifer species recognition. IEEE Trans. Geoscience and Remote Sensing, in press."
  },
  {
    "objectID": "papers/index.html#section-26",
    "href": "papers/index.html#section-26",
    "title": "Papers",
    "section": "",
    "text": "A. Barron, J. Rissanen, and B. Yu (1998). The Minimum Description Length principle in coding and modeling. (Special Commemorative Issue: Information Theory: 1948-1998) IEEE. Trans. Inform. Th., 44, 2743-2760. Reprinted in Information 50 Years of Discovery, Theory: S. Verdu and S. McLaughlin (eds), IEEE Press , 1999.\nB. Yu and P. Mykland (1998). Looking at Markov samplers through cusum path plots: a simple diagnostic idea. Statistics and Computing , 8, 275-286.\nP. Gong, R. Pu and B. Yu (1998) Conifer species recognition: effects of data transformation and band width (in Chinese) Journal of Remote Sensing, 2(3), 211-217.\nG. Chang, B. Yu and M. Vetterli (1998). Spatially adaptive wavelet thresholding for image denoising. Proceedings of IEEE International Conference on Image Processing, October, Chicago. (paper)\nS. G. Chang, B. Yu, and M. Vetterli (1998). Image denoising via lossy compression and wavelet thresholding. Proceedings of International Conference on Image Processing. Santa Barbara, California, vol. 1, pp. 604-607. (paper)\nM. Ostland and B. Yu (1997). Exploring quasi Monte Carlo for marginal density approximation. Statistics and Computing, 7, 217-228. (paper)\nP. Gong, R. Pu, and B. Yu (1997). Conifer species recognition with in Situ hyperspectral data. Remote Sensing of Environment, 62, 189-200.\nB. Yu and T. P. Speed (1997). Information and the clone mapping of chromosomes. Ann. Statist. 25, 169-185. (paper)"
  },
  {
    "objectID": "papers/index.html#section-27",
    "href": "papers/index.html#section-27",
    "title": "Papers",
    "section": "",
    "text": "D. Nelson, T. Speed, and B. Yu (1997). The limits of random fingerprinting. Genomics, 40, 1-12.\nB. Yu (1997). Assouad, Fano, and Le Cam. Festschrift for Lucien Le Cam . D. Pollard, E. Torgersen, and G. Yang (eds), pp. 423-435, Springer-Verlag. (paper)"
  },
  {
    "objectID": "papers/index.html#section-28",
    "href": "papers/index.html#section-28",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (1996). Lower bounds on expected redundancy for nonparametric classes. IEEE Trans. on Information Theory, 42, 272-275.\nY. Yoo, A. Ortega, and B. Yu (1996). Adaptive quantization of image subbands with efficient overhead rate selection. In Proceedings of IEEE International Conference on Image Processing, Lausanne, Switzerland.\nB. Yu (1996). A Statistical analysis of adaptive scalar quantization based on quantized past data. In Proceedings of International Symposium on Information Theory and its Applications (ISITA96), Victoria, Canada. (paper)"
  },
  {
    "objectID": "papers/index.html#section-29",
    "href": "papers/index.html#section-29",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (1995). Comment: Extracting more diagnostic information from a single run using cusum path plot. Statist. Sci., 10, 54-58.\nJ. Rissanen and B. Yu (1995). MDL learning. In Learning and Geometry: Computational Approaches, Progress in Computer Science and Applied Logic, 14, David Kueker and Carl Smith (eds), Birkhäuser, Boston, pp. 3-19.\nP. Mykland, L. Tierney, and B. Yu (1995). Regeneration in Markov Chain samplers. J. Amer. Statist. Assoc., 90, 233-241."
  },
  {
    "objectID": "papers/index.html#section-30",
    "href": "papers/index.html#section-30",
    "title": "Papers",
    "section": "",
    "text": "B. Yu (1994). Rates of convergence for empirical processes of stationary mixing sequences. Ann. Probab. 22, 94-116.\nM. Arcones and B. Yu (1994). Central limit theorems for empirical and U-processes of stationary mixing sequences. J. Theor. Probab. 7, 47-71.\nB. Yu (1994). Lower bound on the expected redundancy for classes of continuous Markov sources. In Statistical Decision Theory and Related Topics V, S. S. Gupta and J. O. Berger (eds), 453-466.\nM. Arcones and B. Yu (1994). Limit theorems for empirical processes under dependence. In Proceedings in Chaos expansions, multiple Wiener integrals and their applications. 205-221.\nA. R. Barron, Y. Yang and B. Yu (1994). Asymptotically optimal function estimation by minimum complexity criteria. In Proceedings of 1994 International Symposium on Information Theory, pp. 38, Trondheim, Norway."
  },
  {
    "objectID": "papers/index.html#section-31",
    "href": "papers/index.html#section-31",
    "title": "Papers",
    "section": "",
    "text": "B. Yu and T. Speed (1993). A rate of convergence result for a universal D-semifaithful code. IEEE Trans. on Information Theory 39, 8813-820.\nB. Yu (1993). Density estimation in the L∞ norm for dependent data with applications to the Gibbs sampler. Ann. Statist. 21, 711-735.\nT. Speed and B. Yu (1993). Model selection and prediction: normal regression. J. Inst. Statist. Math. 45, 35-54."
  },
  {
    "objectID": "papers/index.html#section-32",
    "href": "papers/index.html#section-32",
    "title": "Papers",
    "section": "",
    "text": "J. Rissanen, T. Speed and B. Yu (1992). Density estimation by stochastic complexity. IEEE Trans. on Information Theory, 38, 315-323.\nB. Yu and T. Speed (1992) Data compression and histograms. Probability Theory and Related Fields, 92, 195-229."
  },
  {
    "objectID": "papers/interdisciplinary-research-in-biomedicine.html",
    "href": "papers/interdisciplinary-research-in-biomedicine.html",
    "title": "Interdisciplinary Research in Biomedicine",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/interdisciplinary-research-in-biomedicine.html#selected-recent-papers-in-biomedicine",
    "href": "papers/interdisciplinary-research-in-biomedicine.html#selected-recent-papers-in-biomedicine",
    "title": "Interdisciplinary Research in Biomedicine",
    "section": "Selected Recent Papers in Biomedicine",
    "text": "Selected Recent Papers in Biomedicine\n\nQ. Wang,* T. M. Tang, N. Youlton, C. S. Weldy, A. M. Kenney, O. Ronen, J. W. Hughes, E. T. Chin, S. C. Sutton. A. Agarwal, X. Li, M. Behr, K. Kumbier, C. S. Moravec, W. H. W. Tang, K. B. Margulies, T. P. Cappola, A. J. Buitte, R. Arnaout, J. B. Brown, J. R. Priest, V. N. Parikh, B. Yu, E. Ashley* (2023). Epistasis regulates genetic control of cardiac hypertrophy. https://www.medrxiv.org/content/10.1101/2023.11.06.23297858v1 (Code) (PCS documentation)\nR. Cahill, Y. Wang, R. P. Xian, A. J. Lee, H. Zeng, B. Yu, B. Tasic, R. Abbasi-Asl (2023). Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology. https://www.biorxiv.org/content/10.1101/2023.03.10.531984v2 (Code)\nE. Irajizad, A. Kenney, T. Tang, J. Vykoukal, R. Wu, E. Murage, J. B. Dennison, M. Sans, J. P. Long, M. Loftus, J. A. Chabot, M. D. Kluger, F. Kastrinos, L. Brais, A. Babic, K. Jajoo, L. S. Lee, T. E. Clancy, K. Ng, A. Bullock, J. M. Genkinger, A. Maitra, K. A. Do, B. Yu, B. W. Wolpin, S. Hanash, J. F. Fahrmann. (2023). A blood-based metabolomic signature predictive of risk for pancreatic cancer. Cell Reports Medicine 4(9): 101194. doi: 10.1016/j.xcrm.2023.101194. (PCS related) (Editorial in Cell Reports Medicine by L. Oldfield and E. Costello on Erajizad et al. (2023))\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. https://arxiv.org/abs/2305.17588\nN. Altieri, B. Park, J. DeNero, A. Odisho, B. Yu. (2021). Improving natural language information extraction from cancer pathology reports using transfer learning and zero-shot string similarity. JAMIA Open. 2021 Sept. 30 4(3).\nB. Norgeot, G. Quer, B. K. Beaulieu-Jones, A. Torkamani, R. Dias, M. Gianfrancesco, R. Arnaout, I. S. Kohane, S. Saria, E. Topol, Z. Obermeyer, B. Yu & A. Butte (2020). Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist, Nature Medicine, 26, 1320–1324."
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Software",
    "section": "",
    "text": "Software\nAlso see GitHub\n\nVeridicalFlow: Library for building stable, trustworthy data-science pipelines, PCS framework, paper, docs\nsimChef: R package to facilitate PCS simulation studies, data science workflows\nimodels: Python package for interpretable machine learning models, FIGS, hierarchical shrinkage, MDI+, FIGS paper, hierarchical shrinkage paper, MDI+ paper\niRF: Iterative Random Forests, stable high-order interactions, PCS-guided, paper\nAdaptive wavelets: Adaptive interpretable wavelets and wavelet distillation, domain-agnostic, paper\nCOVID-19 severity prediction: Extensive COVID-19 data and forecasting for counties and hospitals, pandemic severity index, paper, website\nEpistasis cardiac hypertrophy: Code for epistasis in genetic control of cardiac hypertrophy, PCS-guided, preprint, PCS docs\nMolecular partner prediction: Predicting successful CME events using clathrin markers, cell biology\nsMPS2: Simplified MyProstateScore2.0 for high-grade prostate cancer detection, clinical decision rules\nMS-analysis-NMF: Non-negative matrix factorization for deconvolving complex mass spectra, simulation study\nVDS book supplementary: Supplementary materials for Veridical Data Science book, educational resources\nimodels-experiments: Experimental interpretable models to accompany imodels package, rule-based models\nYu Group website: Source code for the Yu Group website, web development"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "CDSS Chancellor’s Distinguished Professor of Statistics\nElectrical Engineering and Computer Sciences, and Center for Computational Biology\nStatistics Department Chair, 2009 - 2012\n\n\n\nAddress: 367 Evans Hall #3860, Berkeley, CA 94720\nPhone: 510-642-2781\nFax: 510-642-7892\nEmail: binyu@berkeley.edu"
  },
  {
    "objectID": "index.html#professor-bin-yu",
    "href": "index.html#professor-bin-yu",
    "title": "Home",
    "section": "",
    "text": "CDSS Chancellor’s Distinguished Professor of Statistics\nElectrical Engineering and Computer Sciences, and Center for Computational Biology\nStatistics Department Chair, 2009 - 2012"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Home",
    "section": "",
    "text": "Address: 367 Evans Hall #3860, Berkeley, CA 94720\nPhone: 510-642-2781\nFax: 510-642-7892\nEmail: binyu@berkeley.edu"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Home",
    "section": "Welcome",
    "text": "Welcome\nProfessor Bin Yu is the head of the Yu Group at Berkeley, which consists of 12-15 students and postdocs from Statistics and EECS. She was formally trained as a statistician, but her research interests and achievements extend beyond the realm of statistics. Together with her group, Bin Yu has leveraged new computational developments to solve important scientific problems by combining novel and often interpretable statistical machine learning approaches with the domain expertise of my many collaborators in neuroscience, genomics and precision medicine. She also develops relevant theory to understand random forests and deep learning for insight into and guidance for practice. Her work has been recongized by many awards. In particular, she was inducted to the National Academy of Sciences in 2014 and to the American Academy of Arts and Sciences in 2013. She delivered the IMS Wald Lectures and COPSS Distinguished Achievement and Award Lecture (DAAL, formerly Fisher Award and Lecture) in 2023.\nShe and her team have developed the PCS framework for veridical data science (or responsible, reliable, and transparent data analysis and decision-making). PCS stands for predictability, computability and stability, and it unifies, streamlines, and expands on ideas and best practices of machine learning and statistics.\nIn order to augment empirical evidence for decision-making, they are investigating statistical machine learning methods/algorithms (and associated statistical inference problems) such as dictionary learning, non-negative matrix factorization (NMF), EM and deep learning (CNNs and LSTMs), and heterogeneous effect estimation in randomized experiments (X-learner). Their recent algorithms include staNMF for unsupervised learning, iterative Random Forests (iRF) and signed iRF (s-iRF) for discovering predictive and stable high-order interactions in supervised learning, next generation tree-based methods (e.g. fast and interpretable greedy-tree sums (FIGS) and hierarchical shrinked (HS) trees, and RF+), contextual decomposition (CD), aggregated contextual decomposition (ACD), and adaptive wavelet distillation (AWD) for interpretation of Deep Neural Networks (DNNs)."
  },
  {
    "objectID": "index.html#recent-publications-talks-news",
    "href": "index.html#recent-publications-talks-news",
    "title": "Home",
    "section": "Recent Publications, Talks & News",
    "text": "Recent Publications, Talks & News\n\n2025PCS workflow for veridical data science in the age of AI - Framework for responsible AI development and deployment\n2025Veridical Data Science in Biology workshop, UC Berkeley, July 11 - Applying trustworthy data science to biological research\n2025Rome Workshop on Veridical Data Science, June 20 - International workshop on responsible data analysis\n2024Veridical data science and medical foundation models - Guidelines for trustworthy medical AI systems\n2024Mechanistic Interpretation through Contextual Decomposition in Transformers - Method for understanding transformer decision-making processes\n2024LoRA+: Efficient Low Rank Adaptation of Large Models - Improved technique for fine-tuning large language models\n2024VDS book review in Harvard Data Science Review - Praise for pedagogical approach to responsible data science\n2024Berkeley-Stanford Workshop on Veridical Data Science (videos) - Inaugural academic collaboration on trustworthy AI\n2024Veridical Data Science book released (online) - Comprehensive guide to responsible data analysis and decision making\n2023Explaining black box text modules in natural language with language models - Natural language explanations for neural network text processing\n2023Diagnosing transformers: illuminating feature space for clinical decision-making - Interpretability methods for medical transformer applications\n2023COPSS Distinguished Award and Lecture, JSM Toronto - Recognition for contributions to statistical science and AI\n\nView all papers →"
  },
  {
    "objectID": "books/index.html",
    "href": "books/index.html",
    "title": "Books",
    "section": "",
    "text": "Published by MIT Press, 2024\nVeridical Data Science presents the PCS (Predictability, Computability, and Stability) framework that prioritizes reproducibility, interpretability, and reliability in data analysis. The book covers essential topics including statistical modeling, machine learning, causal inference, and ethical considerations in data science. Through real-world case studies and practical examples, readers learn how to apply the PCS principles to ensure their analyses are not only accurate but also robust and interpretable.\n\nWeb (free online access)\nPrinted"
  },
  {
    "objectID": "books/index.html#veridical-data-science",
    "href": "books/index.html#veridical-data-science",
    "title": "Books",
    "section": "",
    "text": "Published by MIT Press, 2024\nVeridical Data Science presents the PCS (Predictability, Computability, and Stability) framework that prioritizes reproducibility, interpretability, and reliability in data analysis. The book covers essential topics including statistical modeling, machine learning, causal inference, and ethical considerations in data science. Through real-world case studies and practical examples, readers learn how to apply the PCS principles to ensure their analyses are not only accurate but also robust and interpretable.\n\nWeb (free online access)\nPrinted"
  },
  {
    "objectID": "papers/deep-learning-and-machine-learning.html",
    "href": "papers/deep-learning-and-machine-learning.html",
    "title": "Deep Learning and Machine Learning",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/deep-learning-and-machine-learning.html#selected-recent-papers-in-deep-learning-and-machine-learning",
    "href": "papers/deep-learning-and-machine-learning.html#selected-recent-papers-in-deep-learning-and-machine-learning",
    "title": "Deep Learning and Machine Learning",
    "section": "Selected Recent Papers in Deep Learning and Machine Learning",
    "text": "Selected Recent Papers in Deep Learning and Machine Learning\n\nA. R. Hsu, Y. Cherapanamjeri, A. Y. Odisho, P. R. Carroll, B. Yu (2024). Mechanistic Interpretation through Contexual Decomposition in Transformers. https://arxiv.org/pdf/2407.00886.\nO. Ronen, A. I. Humayun, R. Balestriero, R. Baraniuk, B. Yu (2024). ScaLES: Scalable Latent Exploration Score for Pre-Trained Generative Networks. https://arxiv.org/pdf/2406.09657\nS. Hayou, N. Ghosh, B. Yu (2024). The Impact of Initialization on LoRA Fineuning Dynamics. https://arxiv.org/pdf/2406.08447\nB. Yu (2024). After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI (discussion of Donoho’s paper “Data Science at the Singularity”) Harvard Data Science Review (HDSR).\nN. R. Mallinar, A. Zane, S. Frei, B. Yu (2024). Minimum-Norm Interpolation Under Covariate Shift. Proc. ICML. https://arxiv.org/pdf/2404.00522\nL. Sun, A. Agarwal, A. Kornblith, B. Yu, C. Xiong (2024). ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance. Proc. ICML. https://arxiv.org/abs/2402.13448\nS. Hayou, N. Ghosh, B. Yu (2024). LoRA+: Efficient Low Rank Adaptation of Large Models. Proc. ICML. https://arxiv.org/abs/2402.12354\nN. Ghosh, S. Frei, W. Ha, B. Yu (2023). The effect of SGD batch size on autoencoder learning: sparsity, sharpness and feature learning. https://arxiv.org/abs/2308.03215\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. ICLR 2023. https://arxiv.org/abs/2305.17588\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu and J. Gao (2023). Explaining black box text modules in natural language with language models.\nN. Ghosh, S. Mei, and B. Yu (2022). The three stages of dynamics in high-dimensional kernel methods. Proc. ICLR, 2022. https://arxiv.org/abs/2111.07167\nW. Ha, C. Singh, F. Lanusse, S. Upadhyayula, and B. Yu (2021). Adaptive Wavelet Distillation from Neural Networks through Interpretation. Proc. NeurIPS 2021. (code)\nL. Reiger, J. W. Murdoch, S. Singh, B. Yu (2020). Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge. ICML Proceedings. (code)\nC. Singh, W. Ha, F. Lanusse, V. Boehm , J. Liu, B. Yu (2020). Transformation Importance with Applications to Cosmology ICLR Workshop paper. (code)\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2020) Fast Mixing of Metropolized Hamiltonian Monte Carlo: Benefits of Multi-Step Gradients, JMLR, https://arxiv.org/abs/1905.12247\nR. Dwivedi, Y. Chen, M. J. Wainwright and B. Yu (2019) Log-concave Sampling: Metropolis Hastings Algorithms are Fast JMLR. http://jmlr.org/papers/v20/19-306.html\nY. Chen, R. Dwivedi, M. J. Wainwright and B. Yu (2018) Fast MCMC Algorithms on Polytopes. JMLR. http://jmlr.org/papers/v19/18-158.html\nY. Chen, R. Abbasi-Asl, A. Bloniarz, M. Oliver, B. Willmore, J. Gallant, and B. Yu (2018) The DeepTune framework for modeling and characterizing neurons in visual cortex area V4 https://www.biorxiv.org/content/10.1101/465534v1\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. https://arxiv.org/abs/1810.0728 (an enhanced version of iRF, PCS related)\nJ. Murdoch, P. Liu, and B. Yu (2018) Beyond word importance: contextual decomposition to extract interactions from LSTMs. Proc. ICLR 2018. https://arxiv.org/abs/1705.07356 (code)\nS. Kunzel, J. Sekhon, P. Bickel, and B. Yu* (2019) Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning, PNAS. 116 (10) 4156-4165. https://arxiv.org/abs/1706.03461 (code)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (code) (PCS related)\nS. Balakrishnan, M. Wainwright, B. Yu (2017) Statistical Guarantees for the EM algorithm: from population to sample-based analysis. Annals of Statistics, 45(1), 77 - 120.\nS. Wu and B. Yu (2018). Local identifiability of l1-minimization dictionary learning: a sufficient and almost necessary condition. JMLR. 18, 1 - 56.\nK. Rohe, T. Qin and B. Yu* (2016). Co-clustering directed graphs to discover asymmetries and directional communities. Proc. National Academy of Sciences (PNAS), 113(45), 12679 - 12684.\nSiqi Wu, Antony Joseph, Ann S. Hammonds, Susan E. Celniker, Bin Yu, and Erwin Frise (2016). Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks (with support information). PNAS, pp. 4290 - 4295. (code) (PCS related)\nA. Bloniarz, H. Liu, C. Zhang, J. Sekhon, and B. Yu* (2015). Lasso adjustments of treatment effect estimates in randomized experiments. PNAS. 113, 7383 - 7390."
  },
  {
    "objectID": "papers/interpretable-machine-learning.html",
    "href": "papers/interpretable-machine-learning.html",
    "title": "Interpretable Machine Learning",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/interpretable-machine-learning.html#selected-recent-papers-on-interpretable-machine-learning",
    "href": "papers/interpretable-machine-learning.html#selected-recent-papers-on-interpretable-machine-learning",
    "title": "Interpretable Machine Learning",
    "section": "Selected Recent Papers on Interpretable Machine Learning",
    "text": "Selected Recent Papers on Interpretable Machine Learning\n\nA. R. Hsu, Y. Cherapanamjeri, A. Y. Odisho, P. R. Carroll, B. Yu (2024). Mechanistic Interpretation through Contextual Decomposition in Transformers.\nY. Chen, C. Singh, X. Liu, S. Zuo, B. Yu, H. He, J. Gao (2024). Towards consistent natural-language explanations via explanation-consistent finetuning.\nQ. Zhang, C. Singh, L. Liu, X. Liu, B. Yu, J. Gao, T. Zhao (2023). Tell your model where to attend: post-hoc attention steering for LLMs. ICLR 2024.\nA. Agarwal, A. M. Kenny, Y. S. Tan, T. M. Tang, B. Yu (2023). MDI+: a flexible random forest-based feature importance framework. (PCS related)\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decision-making. ICLR (2024)\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu and J. Gao (2023). Explaining black box text modules in natural language with language models.\nC. Singh, W. Ha and B. Yu (2021). Interpreting and Improving Deep-Learning Models with Reality Checks. To appear in the book entitled “xxAI - Beyond Explainable AI” (eds. Andreas Holzinger, Randy Goebel, Ruth Fong, Taesup Moon, Klaus-Robert Müller, and Wojciech Samek).\nW. Ha, C. Singh, F. Lanusse, S. Upadhyayula, and B. Yu (2021). Adaptive Wavelet Distillation from Neural Networks through Interpretation. Proc. NeurIPS 2021. (code)\nL. Reiger, J. W. Murdoch, S. Singh, B. Yu (2020). Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge. ICML Proceedings. (code)\nC. Singh, W. Ha, F. Lanusse, V. Boehm , J. Liu, B. Yu (2020). Transformation Importance with Applications to Cosmology ICLR Workshop paper. (code)\nW. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu* (2019) Definitions, methods, and applications in interpretable machine learning. PNAS, 116 (44) 22071-22080.\nW. J. Murdoch, C. Sign, and B. Yu (2019). Hierarchical interpretations for neural network predictions. ICLR. (code)\nJ. Murdoch, P. Liu, and B. Yu (2018) Beyond word importance: contextual decomposition to extract interactions from LSTMs. Proc. ICLR 2018. (code)"
  },
  {
    "objectID": "papers/tree-based-methods.html",
    "href": "papers/tree-based-methods.html",
    "title": "Tree-based Methods",
    "section": "",
    "text": "Complete Paper List in Reverse Chronological Order"
  },
  {
    "objectID": "papers/tree-based-methods.html#selected-recent-papers-on-tree-based-methods",
    "href": "papers/tree-based-methods.html#selected-recent-papers-on-tree-based-methods",
    "title": "Tree-based Methods",
    "section": "Selected Recent Papers on Tree-based Methods",
    "text": "Selected Recent Papers on Tree-based Methods\n\nY. S. Tan, O. Ronen, T. Saarinen, B. Yu (2024). The Computational Curse of Big Data for Bayesian Additive Regression Trees: a Hitting Time Analysis.\nY. S. Tan, C. Singh, K. Nasseri, A. Agarwal, J. Duncan, O. Ronen, M. Epland, A. Kornblith, B. Yu (2022). Fast interpretable greedy-tree sums (FIGS). (imodels 🔎: a python package for fitting interpretable models contains code for FIGS).\nM. Behr, Y. Wang, X. Li, B. Yu (2022). Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests. PNAS, (theory for a tractable version of iRF, PCS-related)\nA. Agarwal, Y. S. Tan, O. Ronen, C. Singh, B. Yu (2022). Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods. Proc. ICML (imodels 🔎: a python package for fitting interpretable models contains code for hierarchical shrinkage (HS))\nY. Tan, A. Agarwal, and B. Yu (2021). A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds. Proc. AISTATS.\nM. Behr, K. Kumbier, A. Cordova-Palomera, M. Aguirre, E. Ashley, A. Butte, R. Arnaout, J. B. Brown, J. Preist, B. Yu (2020). Learning epistatic polygenic phenotypes with Boolean interactions. (code) (PCS inference case study)\nK. Kumbier, S. Sumanta, J. B. Brown, S. Celniker, and B. Yu* (2018) Refining interaction search through signed iterative Random Forests. (an enhanced version of iRF, PCS related)\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948. (code) (PCS related)"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Bin Yu is the Chancellor’s Distinguished Professor in the UC Berkeley Departments of Statistics and EECS. She was Chair of the Department of Statistics at UC Berkeley from 2009 to 2012. She is a member of National Academy of Sciences and currently serves on the editorial board of Proceedings of National Academy of Sciences (PNAS).\nProfessor Bin Yu recieved a BS Degree in Mathematics from Peking University, and MS and Ph.D. in Statistics from UC Berkeley. She was a Member of Technical Staff at Lucent Bell-Labs, Distinguished Researcher in the Deep Learning Group of Microsoft Research, Assistant Professor at UW-Madison, and Miller Research Professor at Berkeley. She was a Visiting Faculty at MIT, Peking University, Newton Institute at Cambridge University, ETH, Yale University, Flatiron Institute, Poincare Institute, INRIA-Paris, and Fields Institute at University of Toronto.\nProfessor Bin Yu has published many research papers and one book, Veridical Data Science (MIT Press, 2024). Her research focuses on developing trustworthy and interpretable machine learning methods, with particular emphasis on the Predictability-Computability-Stability (PCS) framework for veridical data science. She has made fundamental contributions to statistical theory, including pioneering work on Vapnik-Chervonenkis (VC) theory for time series analysis, minimum description length (MDL) and entropy estimation, sparse modeling, boosting, spectral clustering, and MCMC convergence analysis. Her applied work spans neuroscience, genomics, remote sensing, and precision medicine, always emphasizing interdisciplinary collaboration with domain experts. Currently, she leads research in interpretable machine learning (including tree-based methods and deep learning interpretability), causal inference, and the development of stable, reproducible methods for scientific discovery. Her group has developed influential algorithms such as iterative random forests (iRF), contextual decomposition for transformers, and adaptive wavelet distillation for interpreting neural networks.\nProfessor Bin Yu has received many awards and honors throughout her career. She has been elected to the National Academy of Sciences and the American Academy of Arts and Sciences. Her major awards include the Guggenheim Fellowship, COPSS E. L. Scott Prize, and most recently, the COPSS Distinguished Achievement Award and Lecture (DAAL) (formerly Fisher Award and Lecture) at JSM in 2023. She has delivered several distinguished lectures, including the Wald Memorial Lectures of the Institute of Mathematical Statistics (IMS), the Tukey Memorial Lecture of the Bernoulli Society, and the Rietz Lecture of IMS. She holds an Honorary Doctorate from the University of Lausanne in Switzerland.\nProfessor Bin Yu has held many leadership positions in the statistical and data science communities. She served as President of the Institute of Mathematical Statistics (IMS) and was Chair of the Department of Statistics at UC Berkeley from 2009 to 2012. She served on the Inaugural Scientific Committee of the UK Turing Institute for Data Science and AI. Her editorial leadership includes current service on the Editorial Board of Proceedings of National Academy of Sciences (PNAS), and previous service on editorial boards of Annals of Statistics, Journal of American Statistical Association, and Journal of Machine Learning Research. Her committee and advisory work includes co-chairing the National Scientific Committee of the Statistical and Applied Mathematical Sciences Institute (SAMSI), serving on scientific advisory committees of SAMSI and IPAM, and on the board of trustees of ICERM and the Board of Governors of IEEE-IT Society. She recently served on the scientific advisory committee for the IAS Special Year on optimization, statistics and theoretical machine learning, and the Scientific Advisory Boards of Canadian Statistical Sciences Institute (CANSSI). Currently, she serves on the advisory board of the AI Policy Hub at UC Berkeley, the Scientific Advisory Committee of the Department of Quantitative and Computational Biology at USC, and on the External Advisory Committee for Learning the Earth with Artificial Intelligence and Physics (LEAP), an NSF Science and Technology Center (STC), at Columbia University. She is a Chan-Zuckerberg Biohub Investigator and Weill Neurohub Investigator. She is a member of the UC Berkeley Center for Computational Biology and serves as a scientific advisor at the Simons Institute for the Theory of Computing."
  },
  {
    "objectID": "about/index.html#deep-learning-and-machine-learning",
    "href": "about/index.html#deep-learning-and-machine-learning",
    "title": "About",
    "section": "Deep Learning and Machine Learning",
    "text": "Deep Learning and Machine Learning\n\nA. R. Hsu, Y. Cherapanamjeri, A. Y. Odisho, P. R. Carroll, B. Yu (2024). Mechanistic Interpretation through Contexual Decomposition in Transformers.\nS. Hayou, N. Ghosh, B. Yu (2024). LoRA+: Efficient Low Rank Adaptation of Large Models. Proc. ICML.\nA. R. Hsu, Y. Cherapanamjeri, B. Park, T. Naumann, A. Odisho, and B. Yu (2023). Diagnosing transformers: illuminating feature space for clinical decison-making. ICLR 2023.\nJ. Murdoch, P. Liu, and B. Yu (2018) Beyond word importance: contextual decomposition to extract interactions from LSTMs. Proc. ICLR 2018."
  },
  {
    "objectID": "about/index.html#interdisciplinary-research-in-biomedicine",
    "href": "about/index.html#interdisciplinary-research-in-biomedicine",
    "title": "About",
    "section": "Interdisciplinary Research in Biomedicine",
    "text": "Interdisciplinary Research in Biomedicine\n\nQ. Wang,* T. M. Tang, N. Youlton, C. S. Weldy, A. M. Kenney, O. Ronen, J. W. Hughes, E. T. Chin, S. C. Sutton. A. Agarwal, X. Li, M. Behr, K. Kumbier, C. S. Moravec, W. H. W. Tang, K. B. Margulies, T. P. Cappola, A. J. Buitte, R. Arnaout, J. B. Brown, J. R. Priest, V. N. Parikh, B. Yu, E. Ashley* (2023). Epistasis regulates genetic control of cardiac hypertrophy.\nE. Irajizad, A. Kenney, T. Tang, J. Vykoukal, R. Wu, E. Murage, J. B. Dennison, M. Sans, J. P. Long, M. Loftus, J. A. Chabot, M. D. Kluger, F. Kastrinos, L. Brais, A. Babic, K. Jajoo, L. S. Lee, T. E. Clancy, K. Ng, A. Bullock, J. M. Genkinger, A. Maitra, K. A. Do, B. Yu, B. W. Wolpin, S. Hanash, J. F. Fahrmann. (2023). A blood-based metabolomic signature predictive of risk for pancreatic cancer. Cell Reports Medicine 4(9): 101194.\nB. Norgeot, G. Quer, B. K. Beaulieu-Jones, A. Torkamani, R. Dias, M. Gianfrancesco, R. Arnaout, I. S. Kohane, S. Saria, E. Topol, Z. Obermeyer, B. Yu & A. Butte (2020). Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist, Nature Medicine, 26, 1320–1324."
  },
  {
    "objectID": "about/index.html#interpretable-machine-learning",
    "href": "about/index.html#interpretable-machine-learning",
    "title": "About",
    "section": "Interpretable Machine Learning",
    "text": "Interpretable Machine Learning\n\nQ. Zhang, C. Singh, L. Liu, X. Liu, B. Yu, J. Gao, T. Zhao (2023). Tell your model where to attend: post-hoc attention steering for LLMs. ICLR 2024.\nA. Agarwal, A. M. Kenny, Y. S. Tan, T. M. Tang, B. Yu (2023). MDI+: a flexible random forest-based feature importance framework.\nC. Singh, A. R. Hsu, R. Antonello, S. Jain, A. G. Huth, B. Yu and J. Gao (2023). Explaining black box text modules in natural language with language models.\nW. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu* (2019) Definitions, methods, and applications in interpretable machine learning. PNAS, 116 (44) 22071-22080."
  },
  {
    "objectID": "about/index.html#tree-based-methods",
    "href": "about/index.html#tree-based-methods",
    "title": "About",
    "section": "Tree-based Methods",
    "text": "Tree-based Methods\n\nY. S. Tan, C. Singh, K. Nasseri, A. Agarwal, J. Duncan, O. Ronen, M. Epland, A. Kornblith, B. Yu (2022). Fast interpretable greedy-tree sums (FIGS).\nA. Agarwal, Y. S. Tan, O. Ronen, C. Singh, B. Yu (2022). Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods. Proc. ICML\nM. Behr, Y. Wang, X. Li, B. Yu (2022). Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests. PNAS.\nS. Basu, K. Kumbier, J. B. Brown, and B. Yu (2018) iterative Random Forests to discover predictive and stable high-order interactions PNAS, 115 (8), 1943-1948."
  },
  {
    "objectID": "about/index.html#veridical-data-science-pcs",
    "href": "about/index.html#veridical-data-science-pcs",
    "title": "About",
    "section": "Veridical Data Science (PCS)",
    "text": "Veridical Data Science (PCS)\n\nB. Yu (2024). After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI Harvard Data Science Review (HDSR).\nB. Yu and K. Kumbier (2020) Veridical data science PNAS. 117 (8), 3920-3929.\nB. Yu (2023). What is uncertainty in today’s practice of data science? J. Econometrics. 237, 105519."
  }
]