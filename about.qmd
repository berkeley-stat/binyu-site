---
title: About
---

# About Bin Yu

Bin Yu is the Chancellor's Distinguished Professor in the UC Berkeley Departments of statistics and EECS. She was Chair of the Department of Statistics at UC Berkeley from 2009 to 2012. She has been elected to the National Academy of Sciences and Academy of Arts and Sciences. She was President of the Institute of Mathematical Statistics (IMS), Guggenheim Fellow, Tukey Memorial Lecturer of the Bernoulli Society, Rietz Lecturer of IMS, and a COPSS E. L. Scott Prize winner.  She is a member of the UC Berkeley Center for Computational Biology and serves as a scientific advisor at the Simons Institute for the Theory of Computing. She is a Chan-Zuckerberg Biohub Investigator and Weill Neurohub Investigator.

She obtained her BS Degree in Mathematics from Peking University, and MS and PhD Degrees in Statistics from UC Berkeley. She was Assistant Professor at UW-Madison, Visiting Assistant Professor at Yale University, Member of Technical Staff at Lucent Bell-Labs, and Miller Research Professor at Berkeley. She was a Visiting Faculty at MIT, ETH, Poincare Institute, Peking University, INRIA-Paris, Fields Institute at University of Toronto, Newton Institute at Cambridge University, and the Flatiron Institute in NYC.  Recently, she was a 50% consultant researcher in the deep learning group of MSR at Redmond (2022-2023).

Recently, she delivered the Wald Memorial Lectures of IMS and COPSS Distinguished Achievement Award and Lecture (DAAL) (formerly Fisher Award and Lecture) at JSM in 2023. She served on the Inaugural Scientific Committee of the UK Turing Institute for Data Science and AI. She holds an Honorary Doctorate from the University of Lausanne in Switzerland. She is serving on the Editorial Board of Proceedings of National Academy of Sciences (PNAS).

She has published more than 170 papers in premier venues on statistical machine learning including deep learning and the Predictability-Computability-Stability (PCS) framework and documentation for [veridical (truthful) data science](https://vdsbook.com/) and these papers not only investigate a wide range of research topics from practice to algorithms and to theory, but also seek deep insights. The breadth and depth of her research experience enabled unique and novel solutions to interdisciplinary data problems in audio and image compression, network tomography, remote sensing, climate science, neuroscience, genomics, and precision medicine.

Professor Bin Yu was formally trained as a statistician, but her research interests and achievements extend beyond the realm of statistics. Together with her group, Bin Yu has leveraged new computational developments to solve important scientific problems by combining novel and often interpretable statistical machine learning approaches with the domain expertise of her many collaborators in neuroscience, genomics and precision medicine. She also develops relevant theory to understand random forests and deep learning for insight into and guidance for practice. Her work has been recognized by many awards. In particular, she was inducted to the National Academy of Sciences in 2014 and to the American Academy of Arts and Sciences in 2013. She delivered the IMS Wald Lectures and COPSS Distinguished Achievement and Award Lecture (DAAL, formerly Fisher Award and Lecture) in 2023.

Currently, she is championing "in-context" research with experts in the subject knowledge and leading research in interpretable machine learning (e.g. tree-based methods and deep learning) and causal inference to design algorithms such as [iterative random forests (iRF)](https://www.pnas.org/doi/10.1073/pnas.1711236115), [Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition (CD-T)](https://openreview.net/forum?id=41HlN8XYM5) and [adaptive wavelet distillation (AWD)](https://proceedings.neurips.cc/paper/2021/file/acaa23f71f963e96c8847585e71352d6-Paper.pdf) for interpreting deep neural networks and [X-learner for heterogeneous treatment effect estimation in causal inference](https://www.pnas.org/doi/10.1073/pnas.1804597116). Recently, she and her collaborators developed PCS-guided [low-signal signed iterative random forests (lo-siRF)](https://www.medrxiv.org/content/10.1101/2023.11.06.23297858v2) that recommends genetic targets for high-yield follow-up experiments with a case study showing epistasis regulation controls cardiac hypertrophy through gene-silencing experiment (4 out of 5 sets of experiments found causal genes or gene-gene interactions). She is also working on relevant deep learning theory and practical algorithms with her team (e.g. the development of [LoRA+ algorithm](https://github.com/nikhil-ghosh-berkeley/loraplus) for fine-tuning large language models).

Previously, she pioneered Vapnik-Chervonenkis (VC) type theory needed for asymptotic analysis of time series and spatio-temporal processes, and made fundamental contributions to information theory and statistics through work on minimum description length (MDL) and entropy estimation, and through theory on sparse modeling, boosting and spectral clustering, EM algorithm, and MCMC convergence analysis. With her students and collaborators, she developed a highly cited spatially adaptive wavelet image denoising method and a low-complexity low-delay perceptually lossless audio coder that was incorporated in Bose wireless speakers, and developed a fast and well-validated Arctic cloud detection algorithm using NASA's MISR data. With the Jack Gallant Lab and her students, she developed predictive models of fMRI brain activity in vision neuroscience that made "mind-reading" possible (or reconstruction of movies using only fMRI signals).

Moreover, she served on editorial boards including Annals of Statistics, Journal of American Statistical Association, and Journal of Machine Learning Research. Her leadership roles included co-chairing the National Scientific Committee of the Statistical and Applied Mathematical Sciences Institute (SAMSI), and serving on the scientific advisory committee of SAMSI and IPAM, and on the board of trustees of ICERM and the Board of Governors of IEEE-IT Society. She recently served on the scientific advisory committee for the IAS Special Year on optimization, statistics and theoretical machine learning, the Scientific Advisory Boards of Canadian Statistical Sciences Institute (CANSSI). She is serving on the advisory board of the AI Policy Hub at UC Berkeley, the Scientific Advisory Committee of the Department of Quantitative and Computational Biology at USC, and on the External Advisory Committee, Learning the Earth with Artificial Intelligence and Physics (LEAP), an NSF Science and Technology Center (STC), at Columbia University.

# About Our Research Group

The [Yu Group](https://www.stat.berkeley.edu/~yugroup/) at Berkeley consists of 12-15 students and postdocs from Statistics and EECS. In the research group, we cultivate a strongly interdisciplinary and collaborative culture, solving data problems across fields such as neuroscience, genomics, remote sensing, and precision medicine. 

Through these projects we have successfully [mapped a cell's destiny](https://newscenter.lbl.gov/2016/05/04/mapping-cells-destiny/) using spatial gene expression images of Drosophila embryos, we have characterized V4 neurons through [DeepTune](https://www.biorxiv.org/content/10.1101/465534v1) images, and we are currently seeking genomic markers of heart disease using UK Biobank data. Recently, my group and I have developed approaches for [predicting county-level COVID-19 death counts](https://covidseverity.com/) in an effort to support the non-profit, [Response4Life](https://response4life.org/), who are working towards distributing PPE across the country to those who need it most.

The Yu Group and I have also developed an array of statistical and machine learning methods inspired by our interdisciplinary projects, including [stability-driven nonnegative matrix factorization (staNMF)](https://github.com/Yu-Group/staNMF) for unsupervised learning, [iterative Random Forests (iRF)](https://github.com/Yu-Group/iterative-Random-Forest) and [signed iRF (s-iRF)](https://github.com/karlkumbier/iRF2.0) for discovering predictive and stable high-order (Boolean) interactions in supervised learning, [contextual decomposition (CD)](https://github.com/jamie-murdoch/ContextualDecomposition) and [aggregated contextual decomposition (ACD)](https://github.com/csinva/hierarchical-dnn-interpretations) for phrase or patch importance extraction from Deep Neural Networks (DNNs).

Our research covers several key areas:

## Deep Learning and Machine Learning

Our work in deep learning spans from theoretical understanding to practical applications. Key contributions include mechanistic interpretation through contextual decomposition in transformers, efficient fine-tuning methods like [LoRA+](https://github.com/nikhil-ghosh-berkeley/loraplus), and frameworks for understanding neural network dynamics. We've developed methods like [Adaptive Wavelet Distillation (AWD)](https://github.com/Yu-Group/adaptive-wavelets) for interpreting deep neural networks, and theoretical work on the three stages of dynamics in high-dimensional kernel methods. Our applications range from clinical decision-making in healthcare to cosmological analysis.

Selected recent papers:
- [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) (ICML 2024)
- [Mechanistic Interpretation through Contextual Decomposition in Transformers](https://arxiv.org/pdf/2407.00886) (2024)
- [Diagnosing transformers: illuminating feature space for clinical decision-making](https://arxiv.org/abs/2305.17588) (ICLR 2024)
- [Adaptive Wavelet Distillation from Neural Networks through Interpretation](https://nips.cc/Conferences/2021/ScheduleMultitrack?event=28027) (NeurIPS 2021)

## Veridical Data Science (PCS)

The PCS (Predictability, Computability, Stability) framework represents our approach to responsible, reliable, and transparent data analysis. This framework unifies and expands on best practices in machine learning and statistics, ensuring that data science findings are both scientifically sound and practically actionable. Our work includes applications to genomics, clinical research, and epidemiology, always emphasizing the importance of stability and reproducibility in scientific discovery.

Selected recent papers:
- [Veridical data science](https://www.stat.berkeley.edu/~binyu/ps/papers2020/VDS20-YuKumbier.pdf) (PNAS 2020) - foundational framework paper
- [Epistasis regulates genetic control of cardiac hypertrophy](https://www.medrxiv.org/content/10.1101/2023.11.06.23297858v1) (2023) - PCS application to genomics
- [After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI](https://hdsr.mitpress.mit.edu/pub/8qexde24/release/1) (Harvard Data Science Review 2024)
- [Stable discovery of interpretable subgroups via calibration in causal studies (staDISC)](https://onlinelibrary.wiley.com/doi/10.1111/insr.12427)

## Interpretable Machine Learning

We develop methods to make machine learning models interpretable and trustworthy, particularly for high-stakes applications in healthcare and science. Our work spans contextual decomposition for understanding neural networks, feature importance frameworks, and methods for aligning model explanations with prior knowledge. We emphasize the principle that interpretations should be both faithful to the model and useful for human decision-making.

Selected recent papers:
- [Mechanistic Interpretation through Contextual Decomposition in Transformers](https://arxiv.org/pdf/2407.00886) (2024)
- [MDI+: a flexible random forest-based feature importance framework](https://arxiv.org/abs/2307.01932) (2023)
- [Tell your model where to attend: post-hoc attention steering for LLMs](https://arxiv.org/abs/2311.02262) (ICLR 2024)
- [Definitions, methods, and applications in interpretable machine learning](https://www.stat.berkeley.edu/~binyu/ps/papers2020/iML19-Murdochetal.pdf) (PNAS 2019)

## Tree-based Methods

We develop both theoretical understanding and practical improvements for tree-based methods, including random forests and gradient boosting. Our contributions include the iterative Random Forests (iRF) algorithm for discovering high-order interactions, hierarchical shrinkage methods for improving accuracy and interpretability, and fast interpretable greedy-tree sums (FIGS). We also provide theoretical analysis of when and why tree-based methods succeed or fail.

Selected recent papers:
- [Fast interpretable greedy-tree sums (FIGS)](https://arxiv.org/abs/2201.11931) (2022)
- [Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods](https://arxiv.org/abs/2202.00858) (ICML 2022)
- [Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests](https://arxiv.org/abs/2102.11800) (PNAS 2022)
- [iterative Random Forests to discover predictive and stable high-order interactions](https://www.stat.berkeley.edu/~binyu/ps/papers2018/iRF+SI18.pdf) (PNAS 2018)

<!-- ## Interdisciplinary Research in Biomedicine

Our biomedicine work applies statistical machine learning to problems in genomics, clinical prediction, and biomarker discovery. We collaborate closely with domain experts to develop methods that are both statistically rigorous and clinically meaningful. Recent work includes epistasis analysis for cardiac hypertrophy, metabolomic signatures for pancreatic cancer risk, and clinical AI modeling standards.

Selected recent papers:
- [Epistasis regulates genetic control of cardiac hypertrophy](https://www.medrxiv.org/content/10.1101/2023.11.06.23297858v1) (2023)
- [A blood-based metabolomic signature predictive of risk for pancreatic cancer](https://pubmed.ncbi.nlm.nih.gov/37729870/) (Cell Reports Medicine 2023)
- [Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist](https://www.nature.com/articles/s41591-020-1041-y) (Nature Medicine 2020)
- [Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology](https://www.biorxiv.org/content/10.1101/2023.03.10.531984v2) (2023) -->

Research is supported in part by grants from NSF, NIH, the Weill Neurohub, and the Simons Foundation.

- [Yu Group at Berkeley](https://www.stat.berkeley.edu/~yugroup/) for current group members and current projects
- [Papers](https://binyu.stat.berkeley.edu/papers)
- [Code](https://www.stat.berkeley.edu/~yugroup/code.html)
- [Statistical Machine Learning at Berkeley](http://www.stat.berkeley.edu/~statlearning/)

# Vision

My broad vision of data science best described in my article [Veridical Data Science](https://www.pnas.org/content/117/8/3920). In this work, I introduced a framework based on three principles: predictability, computability and stability (abbreviated to PCS). This framework helps guide practitioners who solve domain data problems with data science tools to be creative in their analysis and properly validate their findings. I have written a book on the Veridical Data Science framework together with my former student [Rebecca Barter](https://www.rebeccabarter.com/). The book is being published by the MIT Press in 2024 with a free on-line copy available soon.

She and her team have developed the PCS framework for veridical data science (or responsible, reliable, and transparent data analysis and decision-making). PCS stands for predictability, computability and stability, and it unifies, streamlines, and expands on ideas and best practices of machine learning and statistics.

In order to augment empirical evidence for decision-making, they are investigating statistical machine learning methods/algorithms (and associated statistical inference problems) such as dictionary learning, non-negative matrix factorization (NMF), EM and deep learning (CNNs and LSTMs), and heterogeneous effect estimation in randomized experiments (X-learner). Their recent algorithms include staNMF for unsupervised learning, iterative Random Forests (iRF) and signed iRF (s-iRF) for discovering predictive and stable high-order interactions in supervised learning, next generation tree-based methods (e.g. fast and interpretable greedy-tree sums (FIGS) and hierarchical shrinked (HS) trees, and RF+ ), contextual decomposition (CD), aggregated contextual decomposition (ACD), and adaptive wavelet distillation (AWD) for interpretation of Deep Neural Networks (DNNs).