---
title: About
---

# About Bin Yu

Bin Yu is the Chancellor's Distinguished Professor in the UC Berkeley Departments of Statistics and EECS. She was Chair of the Department of Statistics at UC Berkeley from 2009 to 2012. She is a member of National Academy of Sciences and currently serves on the editorial board of Proceedings of National Academy of Sciences (PNAS).

Professor Bin Yu recieved a BS Degree in Mathematics from Peking University, and MS and Ph.D. in Statistics from UC Berkeley. She was a Member of Technical Staff at Lucent Bell-Labs, Distinguished Researcher in the Deep Learning Group of Microsoft Research, Assistant Professor at UW-Madison, and Miller Research Professor at Berkeley. She was a Visiting Faculty at MIT, Peking University,  Newton Institute at Cambridge University, ETH, Yale University, Flatiron Institute, Poincare Institute, INRIA-Paris, and Fields Institute at University of Toronto.

Professor Bin Yu has published many research papers and one book, *Veridical Data Science* (MIT Press, 2024). Her research focuses on developing trustworthy and interpretable machine learning methods, with particular emphasis on the Predictability-Computability-Stability (PCS) framework for veridical data science. She has made fundamental contributions to statistical theory, including pioneering work on Vapnik-Chervonenkis (VC) theory for time series analysis, minimum description length (MDL) and entropy estimation, sparse modeling, boosting, spectral clustering, and MCMC convergence analysis. Her applied work spans neuroscience, genomics, remote sensing, and precision medicine, always emphasizing interdisciplinary collaboration with domain experts. Currently, she leads research in interpretable machine learning (including tree-based methods and deep learning interpretability), causal inference, and the development of stable, reproducible methods for scientific discovery. Her group has developed influential algorithms such as iterative random forests (iRF), contextual decomposition for transformers, and adaptive wavelet distillation for interpreting neural networks.

Professor Bin Yu has received many awards and honors throughout her career. She has been elected to the National Academy of Sciences and the American Academy of Arts and Sciences. Her major awards include the Guggenheim Fellowship, COPSS E. L. Scott Prize, and most recently, the COPSS Distinguished Achievement Award and Lecture (DAAL) (formerly Fisher Award and Lecture) at JSM in 2023. She has delivered several distinguished lectures, including the Wald Memorial Lectures of the Institute of Mathematical Statistics (IMS), the Tukey Memorial Lecture of the Bernoulli Society, and the Rietz Lecture of IMS. She holds an Honorary Doctorate from the University of Lausanne in Switzerland.

Professor Bin Yu has held many leadership positions in the statistical and data science communities. She served as President of the Institute of Mathematical Statistics (IMS) and was Chair of the Department of Statistics at UC Berkeley from 2009 to 2012. She served on the Inaugural Scientific Committee of the UK Turing Institute for Data Science and AI.
Her editorial leadership includes current service on the Editorial Board of Proceedings of National Academy of Sciences (PNAS), and previous service on editorial boards of Annals of Statistics, Journal of American Statistical Association, and Journal of Machine Learning Research.
Her committee and advisory work includes co-chairing the National Scientific Committee of the Statistical and Applied Mathematical Sciences Institute (SAMSI), serving on scientific advisory committees of SAMSI and IPAM, and on the board of trustees of ICERM and the Board of Governors of IEEE-IT Society. She recently served on the scientific advisory committee for the IAS Special Year on optimization, statistics and theoretical machine learning, and the Scientific Advisory Boards of Canadian Statistical Sciences Institute (CANSSI).
Currently, she serves on the advisory board of the AI Policy Hub at UC Berkeley, the Scientific Advisory Committee of the Department of Quantitative and Computational Biology at USC, and on the External Advisory Committee for Learning the Earth with Artificial Intelligence and Physics (LEAP), an NSF Science and Technology Center (STC), at Columbia University. She is a Chan-Zuckerberg Biohub Investigator and Weill Neurohub Investigator.
She is a member of the UC Berkeley Center for Computational Biology and serves as a scientific advisor at the Simons Institute for the Theory of Computing.
<!-- 
# About Our Research Group

The [Yu Group](https://www.stat.berkeley.edu/~yugroup/) at Berkeley consists of a small group of students and postdocs from Statistics and EECS, developing cutting-edge statistical machine learning methods that have proven highly valuable in theortical and applied diciplines spanning a broad range from genetics to quantitative finance. Our graduates have achieved exceptional success in academia and industry, with notable alumni including Peng Zhao (CEO of Citadel Securities), and senior quantitative researchers at Citadel, Two Sigma, Bridgewater, and Voleon.

Our research focuses on developing stable, interpretable, and scalable methods. We have successfully built **real-time forecasting systems** for complex temporal data (including our [COVID-19 severity prediction](https://covidseverity.com/) system), developed **stability-driven machine learning frameworks** crucial for risk management, and created **interpretable deep learning methods** essential for regulatory compliance in financial applications.

The Yu Group has developed many fundamental statistical and machine learning methods, including [**stability-driven nonnegative matrix factorization (staNMF)**](https://github.com/Yu-Group/staNMF) for regime detection and factor modeling, [**iterative Random Forests (iRF)**](https://github.com/Yu-Group/iterative-Random-Forest) and [**signed iRF (s-iRF)**](https://github.com/karlkumbier/iRF2.0) for discovering complex market interactions and non-linear relationships, [**hierarchical shrinkage methods**](https://arxiv.org/abs/2202.00858) for robust prediction in high-dimensional settings, and [**contextual decomposition (CD)**](https://github.com/jamie-murdoch/ContextualDecomposition) and [**aggregated contextual decomposition (ACD)**](https://github.com/csinva/hierarchical-dnn-interpretations) for interpreting complex deep learning models in trading applications.

Our research covers several key areas:

## Deep Learning and Machine Learning

Our work in deep learning spans from theoretical understanding to practical applications across diverse domains. Key contributions include mechanistic interpretation through contextual decomposition in transformers, efficient fine-tuning methods like [LoRA+](https://github.com/nikhil-ghosh-berkeley/loraplus), and frameworks for understanding neural network dynamics. We've developed methods like [Adaptive Wavelet Distillation (AWD)](https://github.com/Yu-Group/adaptive-wavelets) for interpreting deep neural networks, and theoretical work on the three stages of dynamics in high-dimensional kernel methods. Our applications range from clinical decision-making in healthcare to cosmological analysis and financial modeling.

Selected recent papers:
- [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) (ICML 2024)
- [Mechanistic Interpretation through Contextual Decomposition in Transformers](https://arxiv.org/pdf/2407.00886) (2024)
- [Minimum-Norm Interpolation Under Covariate Shift](https://arxiv.org/pdf/2404.00522) (ICML 2024)
- [The three stages of dynamics in high-dimensional kernel methods](https://arxiv.org/abs/2111.07167) (ICLR 2022)

## Veridical Data Science (PCS)

The PCS (Predictability, Computability, Stability) framework represents our approach to responsible, reliable, and transparent data analysis. This framework unifies and expands on best practices in machine learning and statistics, ensuring that data science findings are both scientifically sound and practically actionable. The **Stability** component addresses model robustness and reproducibility, the **Predictability** component focuses on out-of-sample performance, and the **Computability** component ensures scalable implementation. Our work includes applications to genomics, clinical research, epidemiology, and quantitative finance, always emphasizing the importance of stability and reproducibility in scientific discovery.

Selected recent papers:
- [Veridical data science](https://www.stat.berkeley.edu/~binyu/ps/papers2020/VDS20-YuKumbier.pdf) (PNAS 2020) — foundational framework paper
- [Epistasis regulates genetic control of cardiac hypertrophy](https://www.medrxiv.org/content/10.1101/2023.11.06.23297858v1) (2023) — PCS application to genomics
- [After Computational Reproducibility: Scientific Reproducibility and Trustworthy AI](https://hdsr.mitpress.mit.edu/pub/8qexde24/release/1) (Harvard Data Science Review 2024)
- [Stable discovery of interpretable subgroups via calibration in causal studies (staDISC)](https://onlinelibrary.wiley.com/doi/10.1111/insr.12427)

## Interpretable Machine Learning

We develop methods to make machine learning models interpretable and trustworthy, particularly for high-stakes applications in healthcare, science, and finance. Our work spans contextual decomposition for understanding neural networks, feature importance frameworks, and methods for aligning model explanations with prior knowledge. We emphasize the principle that interpretations should be both faithful to the model and useful for human decision-making across diverse domains.

Selected recent papers:
- [Mechanistic Interpretation through Contextual Decomposition in Transformers](https://arxiv.org/pdf/2407.00886) (2024)
- [MDI+: a flexible random forest-based feature importance framework](https://arxiv.org/abs/2307.01932) (2023)
- [Tell your model where to attend: post-hoc attention steering for LLMs](https://arxiv.org/abs/2311.02262) (ICLR 2024)
- [Definitions, methods, and applications in interpretable machine learning](https://www.stat.berkeley.edu/~binyu/ps/papers2020/iML19-Murdochetal.pdf) (PNAS 2019)

## Tree-based Methods

We develop both theoretical understanding and practical improvements for tree-based methods, including random forests and gradient boosting. Our contributions include the iterative Random Forests (iRF) algorithm for discovering high-order interactions, hierarchical shrinkage methods for improving accuracy and interpretability, and fast interpretable greedy-tree sums (FIGS). We also provide theoretical analysis of when and why tree-based methods succeed or fail, with applications spanning from genomics to finance to clinical medicine.

Selected recent papers:
- [The Computational Curse of Big Data for Bayesian Additive Regression Trees: a Hitting Time Analysis](https://arxiv.org/pdf/2406.19958) (2024)
- [Fast interpretable greedy-tree sums (FIGS)](https://arxiv.org/abs/2201.11931) (2022)
- [Hierarchical shrinkage: improving accuracy and interpretability of tree-based methods](https://arxiv.org/abs/2202.00858) (ICML 2022)
- [Provable Boolean Interaction Recovery from Tree Ensemble obtained via Random Forests](https://arxiv.org/abs/2102.11800) (PNAS 2022)
- [iterative Random Forests to discover predictive and stable high-order interactions](https://www.stat.berkeley.edu/~binyu/ps/papers2018/iRF+SI18.pdf) (PNAS 2018)

## Quantitative Finance Applications

Our methods have proven particularly valuable in quantitative finance, where model stability, interpretability, and out-of-sample performance are crucial. Our graduates have achieved exceptional success at leading hedge funds including Citadel Securities (where Peng Zhao serves as CEO), Two Sigma, Bridgewater, and Voleon. We develop stability-driven approaches for regime detection, portfolio optimization, risk management, and alpha generation, always emphasizing robust performance across changing market conditions.

Key applications and alumni outcomes:
- **Real-time forecasting systems** for complex temporal data and market dynamics
- **Stability-driven frameworks** crucial for live trading environments
- **Interpretable models** essential for regulatory compliance and risk management
- **High-dimensional interaction discovery** for complex market relationship modeling

## Interdisciplinary Research in Biomedicine

Our biomedicine work applies statistical machine learning to problems in genomics, clinical prediction, and biomarker discovery. We collaborate closely with domain experts to develop methods that are both statistically rigorous and clinically meaningful. Recent work includes epistasis analysis for cardiac hypertrophy, metabolomic signatures for pancreatic cancer risk, and clinical AI modeling standards.

Selected recent papers:
- [Epistasis regulates genetic control of cardiac hypertrophy](https://www.medrxiv.org/content/10.1101/2023.11.06.23297858v1) (2023)
- [A blood-based metabolomic signature predictive of risk for pancreatic cancer](https://pubmed.ncbi.nlm.nih.gov/37729870/) (Cell Reports Medicine 2023)
- [Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist](https://www.nature.com/articles/s41591-020-1041-y) (Nature Medicine 2020)
- [Unsupervised pattern discovery in spatial gene expression atlas reveals mouse brain regions beyond established ontology](https://www.biorxiv.org/content/10.1101/2023.03.10.531984v2) (2023)

Our graduates have achieved remarkable success across academia and industry, securing positions at leading institutions including top universities, major technology companies, and premier quantitative trading firms such as Citadel Securities, Two Sigma, Bridgewater, and Voleon. This success reflects the broad applicability of our research and the exceptional training our students receive in robust statistical methods, scalable machine learning, and rigorous mathematical foundations essential for tackling complex real-world problems.

Research is supported in part by grants from NSF, NIH, the Weill Neurohub, and the Simons Foundation.

- [Yu Group at Berkeley](https://www.stat.berkeley.edu/~yugroup/) for current group members and current projects
- [Papers](https://binyu.stat.berkeley.edu/papers)
- [Code](https://www.stat.berkeley.edu/~yugroup/code.html)
- [Statistical Machine Learning at Berkeley](http://www.stat.berkeley.edu/~statlearning/)

# Vision

My broad vision of data science best described in my article [Veridical Data Science](https://www.pnas.org/content/117/8/3920). In this work, I introduced a framework based on three principles: predictability, computability and stability (abbreviated to PCS). This framework helps guide practitioners who solve domain data problems with data science tools to be creative in their analysis and properly validate their findings. I have written a book on the Veridical Data Science framework together with my former student [Rebecca Barter](https://www.rebeccabarter.com/). The book is being published by the MIT Press in 2024 with a free on-line copy available soon.

She and her team have developed the PCS framework for veridical data science (or responsible, reliable, and transparent data analysis and decision-making). PCS stands for predictability, computability and stability, and it unifies, streamlines, and expands on ideas and best practices of machine learning and statistics.

In order to augment empirical evidence for decision-making, they are investigating statistical machine learning methods/algorithms (and associated statistical inference problems) such as dictionary learning, non-negative matrix factorization (NMF), EM and deep learning (CNNs and LSTMs), and heterogeneous effect estimation in randomized experiments (X-learner). Their recent algorithms include staNMF for unsupervised learning, iterative Random Forests (iRF) and signed iRF (s-iRF) for discovering predictive and stable high-order interactions in supervised learning, next generation tree-based methods (e.g. fast and interpretable greedy-tree sums (FIGS) and hierarchical shrinked (HS) trees, and RF+ ), contextual decomposition (CD), aggregated contextual decomposition (ACD), and adaptive wavelet distillation (AWD) for interpretation of Deep Neural Networks (DNNs). -->