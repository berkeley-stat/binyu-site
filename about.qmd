---
title: About
---

# About Bin

<img src="images/bio1.jpg" alt="Bin Yu speaking" style="float:right; margin:0 0 1em 2em; width:200px; border-radius:8px;" />

Bin Yu is CDSS Chancellor's Distinguished Professor in the Departments of statistics and EECS, and Center for Computational Biology, and serves as a scientific advisor at the Simons Institute for the Theory of Computing, all at UC Berkeley.

She obtained her BS Degree in Mathematics from Peking University, and MS and PhD Degrees in Statistics from UC Berkeley. She was Assistant Professor at UW-Madison, Visiting Assistant Professor at Yale University, Member of Technical Staff at Lucent Bell-Labs, and Miller Research Professor at Berkeley. She was a Visiting Faculty at MIT, ETH, Poincare Institute, Peking University, INRIA-Paris, Fields Institute at University of Toronto, Newton Institute at Cambridge University, and the Flatiron Institute in NYC. She was Chair of the Department of Statistics at UC Berkeley from 2009 to 2012. Recently, she was a 50% consultant researcher in the deep learning group of MSR at Redmond (2022-2023).

She is a Member of the U.S. National Academy of Sciences and of the American Academy of Arts and Sciences. She was President of the Institute of Mathematical Statistics (IMS), Guggenheim Fellow, Tukey Memorial Lecturer of the Bernoulli Society, Rietz Lecturer of IMS, and a COPSS E. L. Scott Prize winner. Recently, she delivered the Wald Memorial Lectures of IMS and COPSS Distinguished Achievement Award and Lecture (DAAL) (formerly Fisher Award and Lecture) at JSM in 2023. She served on the Inaugural Scientific Committee of the UK Turing Institute for Data Science and AI. She holds an Honorary Doctorate from the University of Lausanne in Switzerland. She is serving on the Editorial Board of Proceedings of National Academy of Sciences (PNAS).

She has published more than 170 papers in premier venues on statistical machine learning including deep learning and the Predictability-Computability-Stability (PCS) framework and documentation for [veridical (truthful) data science](https://vdsbook.com/) and these papers not only investigate a wide range of research topics from practice to algorithms and to theory, but also seek deep insights. The breadth and depth of her research experience enabled unique and novel solutions to interdisciplinary data problems in audio and image compression, network tomography, remote sensing, climate science, neuroscience, genomics, and precision medicine.

Professor Bin Yu was formally trained as a statistician, but her research interests and achievements extend beyond the realm of statistics. Together with her group, Bin Yu has leveraged new computational developments to solve important scientific problems by combining novel and often interpretable statistical machine learning approaches with the domain expertise of her many collaborators in neuroscience, genomics and precision medicine. She also develops relevant theory to understand random forests and deep learning for insight into and guidance for practice. Her work has been recognized by many awards. In particular, she was inducted to the National Academy of Sciences in 2014 and to the American Academy of Arts and Sciences in 2013. She delivered the IMS Wald Lectures and COPSS Distinguished Achievement and Award Lecture (DAAL, formerly Fisher Award and Lecture) in 2023.

Currently, she is championing "in-context" research with experts in the subject knowledge and leading research in interpretable machine learning (e.g. tree-based methods and deep learning) and causal inference to design algorithms such as [iterative random forests (iRF)](https://www.pnas.org/doi/10.1073/pnas.1711236115), [Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition (CD-T)](https://openreview.net/forum?id=41HlN8XYM5) and [adaptive wavelet distillation (AWD)](https://proceedings.neurips.cc/paper/2021/file/acaa23f71f963e96c8847585e71352d6-Paper.pdf) for interpreting deep neural networks and [X-learner for heterogeneous treatment effect estimation in causal inference](https://www.pnas.org/doi/10.1073/pnas.1804597116). Recently, she and her collaborators developed PCS-guided [low-signal signed iterative random forests (lo-siRF)](https://www.medrxiv.org/content/10.1101/2023.11.06.23297858v2) that recommends genetic targets for high-yield follow-up experiments with a case study showing epistasis regulation controls cardiac hypertrophy through gene-silencing experiment (4 out of 5 sets of experiments found causal genes or gene-gene interactions). She is also working on relevant deep learning theory and practical algorithms with her team (e.g. the development of [LoRA+ algorithm](https://github.com/nikhil-ghosh-berkeley/loraplus) for fine-tuning large language models).

Previously, she pioneered Vapnik-Chervonenkis (VC) type theory needed for asymptotic analysis of time series and spatio-temporal processes, and made fundamental contributions to information theory and statistics through work on minimum description length (MDL) and entropy estimation, and through theory on sparse modeling, boosting and spectral clustering, EM algorithm, and MCMC convergence analysis. With her students and collaborators, she developed a highly cited spatially adaptive wavelet image denoising method and a low-complexity low-delay perceptually lossless audio coder that was incorporated in Bose wireless speakers, and developed a fast and well-validated Arctic cloud detection algorithm using NASA's MISR data. With the Jack Gallant Lab and her students, she developed predictive models of fMRI brain activity in vision neuroscience that made "mind-reading" possible (or reconstruction of movies using only fMRI signals).

Moreover, she served on editorial boards including Annals of Statistics, Journal of American Statistical Association, and Journal of Machine Learning Research. Her leadership roles included co-chairing the National Scientific Committee of the Statistical and Applied Mathematical Sciences Institute (SAMSI), and serving on the scientific advisory committee of SAMSI and IPAM, and on the board of trustees of ICERM and the Board of Governors of IEEE-IT Society. She recently served on the scientific advisory committee for the IAS Special Year on optimization, statistics and theoretical machine learning, the Scientific Advisory Boards of Canadian Statistical Sciences Institute (CANSSI). She is serving on the advisory board of the AI Policy Hub at UC Berkeley, the Scientific Advisory Committee of the Department of Quantitative and Computational Biology at USC, and on the External Advisory Committee, Learning the Earth with Artificial Intelligence and Physics (LEAP), an NSF Science and Technology Center (STC), at Columbia University.

# About the Research Group

The [Yu Group](https://www.stat.berkeley.edu/~yugroup/) at Berkeley consists of 12-15 students and postdocs from Statistics and EECS. In the research group, we cultivate a strongly interdisciplinary and collaborative culture, solving data problems across fields such as neuroscience, genomics, remote sensing, and precision medicine. 

Through these projects we have successfully [mapped a cell's destiny](https://newscenter.lbl.gov/2016/05/04/mapping-cells-destiny/) using spatial gene expression images of Drosophila embryos, we have characterized V4 neurons through [DeepTune](https://www.biorxiv.org/content/10.1101/465534v1) images, and we are currently seeking genomic markers of heart disease using UK Biobank data. Recently, my group and I have developed approaches for [predicting county-level COVID-19 death counts](https://covidseverity.com/) in an effort to support the non-profit, [Response4Life](https://response4life.org/), who are working towards distributing PPE across the country to those who need it most.

The Yu Group and I have also developed an array of statistical and machine learning methods inspired by our interdisciplinary projects, including [stability-driven nonnegative matrix factorization (staNMF)](https://github.com/Yu-Group/staNMF) for unsupervised learning, [iterative Random Forests (iRF)](https://github.com/Yu-Group/iterative-Random-Forest) and [signed iRF (s-iRF)](https://github.com/karlkumbier/iRF2.0) for discovering predictive and stable high-order (Boolean) interactions in supervised learning, [contextual decomposition (CD)](https://github.com/jamie-murdoch/ContextualDecomposition) and [aggregated contextual decomposition (ACD)](https://github.com/csinva/hierarchical-dnn-interpretations) for phrase or patch importance extraction from Deep Neural Networks (DNNs).

Research is supported in part by grants from NSF, NIH, the Weill Neurohub, and the Simons Foundation.

- [Yu Group at Berkeley](https://www.stat.berkeley.edu/~yugroup/) for current group members and current projects
- [Papers](https://binyu.stat.berkeley.edu/papers)
- [Code](https://www.stat.berkeley.edu/~yugroup/code.html)
- [Statistical Machine Learning at Berkeley](http://www.stat.berkeley.edu/~statlearning/)

# Vision

In 2014, I was elected to the National Academy of Sciences based on my statistical and scientific contributions, as well as my broad vision of data science best described in my article [Veridical Data Science](https://www.pnas.org/content/117/8/3920), written together with my former student Karl Kumbier. In this work, I introduced a framework based on three principles: predictability, computability and stability (abbreviated to PCS). This framework helps guide practitioners who solve domain data problems with data science tools to be creative in their analysis and properly validate their findings. I have written a book on the Veridical Data Science framework together with my former student [Rebecca Barter](https://www.rebeccabarter.com/). The book is being published by the MIT Press in 2024 with a free on-line copy available soon.

She and her team have developed the PCS framework for veridical data science (or responsible, reliable, and transparent data analysis and decision-making). PCS stands for predictability, computability and stability, and it unifies, streamlines, and expands on ideas and best practices of machine learning and statistics.

In order to augment empirical evidence for decision-making, they are investigating statistical machine learning methods/algorithms (and associated statistical inference problems) such as dictionary learning, non-negative matrix factorization (NMF), EM and deep learning (CNNs and LSTMs), and heterogeneous effect estimation in randomized experiments (X-learner). Their recent algorithms include staNMF for unsupervised learning, iterative Random Forests (iRF) and signed iRF (s-iRF) for discovering predictive and stable high-order interactions in supervised learning, next generation tree-based methods (e.g. fast and interpretable greedy-tree sums (FIGS) and hierarchical shrinked (HS) trees, and RF+ ), contextual decomposition (CD), aggregated contextual decomposition (ACD), and adaptive wavelet distillation (AWD) for interpretation of Deep Neural Networks (DNNs).